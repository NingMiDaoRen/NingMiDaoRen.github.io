---
title: 操作系统
date: 2020-03-02 07:53:10
toc: true
tags: [操作系统]
categories: [计算机基础]
declare: true
description:

---

<meta name="referrer" content="no-referrer" />  

<!--more-->

# 操作系统(Operation System)

[TOC]



## Part 1 为什么要学操作系统

私以为，无论是从软件开发角度看，还是专业素养角度看，这OS都是必学且要学好的。在学习计算机技术的过程中我认为有这么几道坎，开始时掌握一门编程语言，数据结构与算法【终生课】，操作系统，数据库以及计算机网络。只有将这几块都学好了，才能成为一个合格的软件开发者。

说到操作系统，它到底是个什么东西呢？鄙人以为它和计算机的构成有着密切关系。计算机里都有什么呢？

软件系统？硬件系统？

> 硬件系统：主机 + 外设
>
> > 主机：中央处理器 + 内存(主存)
> >
> > 外设：I/O设备 + 辅存(移动硬盘，U盘)
>
> 软件系统：系统软件，支撑软件，应用软件
>
> > 系统软件：最靠近硬件的那一层软件->操作系统，编译软件
> >
> > 支撑软件：给其他软件提供接口和工具组
> >
> > 应用软件：QQ，微信，淘宝...

![](https://images.gitee.com/uploads/images/2020/0302/151653_ca99d05d_5550632.png)

按照学习计算机网络那样，我觉得把计算机体系按功能分层，更能了解操作系统的定位。

![](https://images.gitee.com/uploads/images/2020/0302/152710_72c346ca_5550632.png)



## Part 2 操作系统是什么

我的定义：操作系统是最接近底层硬件的系统软件，它负责分配调度管理各种电脑资源，在应用程序和底层硬件中起到接口【桥梁】的作用。

那么，操作系统究竟负责调度什么样的资源呢？既然是桥梁，那就先看看它两端都有啥。

> 桥的一头：硬件
>
> > 硬件中的老大哥：
> >
> > > CPU【中央处理器】
> > >
> > > 内存
> >
> > 硬件中的小老弟：
> >
> > > I/O设备
> >
> > 桥的另一头：软件
> >
> > 各种软件【数据】

So，OS负责调度的电脑资源如下图：

![](https://images.gitee.com/uploads/images/2020/0302/154752_30fee504_5550632.png)



## Part 3 操作系统的形成和发展过程

### 3.1 无操作系统时代

#### 3.1.1 人工操作方式

在第一代计算机的时代(1945-50年代中期)，没有操作系统，人们用纸带和卡片的方式编写程序，再由人取走输出结果，让下一用户上机。

缺点：

1. 用户独占计算机资源
2. CPU等待人工操作，CPU利用率极低



#### 3.1.2 脱机输入输出方式

此技术改良的就是上述缺点。

输入，输出是脱离主机进行的，将数据存入磁盘，再让主机从磁盘里拿。

![](https://images.gitee.com/uploads/images/2020/0309/133752_391086aa_5550632.png)

优点：

1. 减少CPU空闲时间，提高利用率
2. 提高I/O速度



### 3.2 操作系统三大基本类型



#### 3.2.1 批处理系统



##### 单道批处理系统

单道：冯诺依曼体系计算机中内存里只有一项作业

工作流程:

![](https://images.gitee.com/uploads/images/2020/0309/160837_e738ceec_5550632.png)

运行特征：

1. 单道性：内存中只有一个作业
2. 顺序性：磁带上的各个作业按顺序进入内存(队列)
3. 自动性：磁带上的一批作业可以自动逐个进行，无需人工干预，大大提高计算机资源利用率。



##### 多道批处理系统

多道程序设计：所有作业都放在**外存**上排成**后备队列**，由作业调度程序按一定作业调度算法从队列中选择**若干个作业**，使它们共享CPU和系统资源。它们在内存中交替运行，共享系统中的各种软硬件资源，使计算机资源得到充分利用。

工作流程：

![](https://images.gitee.com/uploads/images/2020/0309/161848_60423035_5550632.png)



##### 常见的作业调度算法

**前置知识点**

评价作业调度算法的优劣，看两个时间**平均周转时间**和**平均带权时间**

> 周转:从一个作业到达至运行结束
>
> > 周转时间 = 作业完成时间 - 作业到达时间
> >
> > > 平均周转时间 = (作业完成时间 - 作业到达时间)/作业数量
>
> 服务:CPU为某一作业单独工作
>
> > 作业服务时间:CPU为某一作业单独工作的时间
> >
> > > 带权周转时间 = 作业周转时间 / 作业服务时间
>
> 进程:程序运行过程的抽象【程序运行时的过程】



**先来先服务算法 (First Come First Service)**

顾名思义，和队列一样先进先出，哪个作业先来，CPU优先执行哪个，一般不单独使用。

![](https://img-blog.csdn.net/20180926200418968?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0OTAyNDM3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

A，B的周转时间
$$
T_{A的周转时间} = A完成时间 - A到达时间 = 1-0 = 1
$$

$$
T_{B的周转时间} = B完成时间 - B到达时间 = 101 - 1 = 100
$$

A，B的带权周转时间

$$
T_{A的带权周转时间} = \frac{A周转时间}{A服务时间} = \frac{1}{1} = 1
$$

$$
T_{B的带权周转时间} = \frac{B周转时间}{B服务时间} = \frac{100}{100} = 1
$$

**全部的周转时间和带权周转时间**

![](https://images.gitee.com/uploads/images/2020/0309/170359_a22a844e_5550632.png)



**短作业优先算法 (Shortest Job First)**

短作业调度算法的核心在于，运行时间越短的作业就先执行。
这里有两个需要注意的地方：

> 1.**无论什么情况，第一个进入的作业，必定第一个执行**
> 2.在后续比较作业长短时，作业必须是到达的

盗张图哈(原文链接已在下方给出)

![](https://img-blog.csdn.net/20180926201621994?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0OTAyNDM3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

FCFS就是先来先服务算法，SJF是短作业优先算法。

由于A在0时刻先进入内存，那么首先对A进行服务，4个单位时间后完成，周转时间为4，带权周转时间为1。此时在第4时刻，B，C，D三个作业都已加载至内存，执行短作业优先算法，发现D的服务时间最短，CPU就先为D服务周转时间 = 6 - 3 = 3 ，带权周转时间 = 3 / 2 =1.5 



**高响应比优先调度算法 (Highest Response Ratio Next)**

FCFS主要考虑的是作业的等待时间，而SJF主要考虑的是作业周转时间。此二者结合就是高相应比优先调度算法(HRRN)

注意：该算法的优先级需要每执行一个进程后重新计算一次。

先看**相应比**的优先值是如何计算出来的。

设优先值为P，等待时间为Tw，服务时间为Ts

$$
P = \frac{T_w+T_s}{T_s} = 1 + \frac{T_w}{T_s}
$$


![](https://images.gitee.com/uploads/images/2020/0312/204947_e838060c_5550632.png)

上图，A先到先执行，B在单位时间2时到达，CPU在3时结束，等待1个单位时间，但是此时只有B这一项作业，CPU正常执行。**在时刻9时，B作业结束**，**此时，C，D，E三项作业早已进入内存**，此时，**决定谁先执行的算法就是高相应比优先调度算法**。

C 在单位时间4时就开始等B执行结束，等到时刻9，等了5个时间单位。如上图所示。

同理算出D，E的等待时间。

然后计算优先级
$$
P_C = 1 + \frac{T_w}{T_s} = 1 + \frac{5}{4} = 2.25
$$

$$
P_D = 1 + \frac{T_w}{T_s} = 1 + \frac{3}{5} = 1.6
$$

$$
P_E = 1 + \frac{T_w}{T_s} = 1 + \frac{1}{2} = 1.5
$$

C的优先级最高，执行C。

本节参考自:[常见的作业调度算法](https://blog.csdn.net/qq_34902437/article/details/82791747)



说了那么多，好像没有在数学层面看出**单道批处理系统**和**多道批处理系统**的差距，让咱们假设一个情景来计算一下二者的效率问题。

设，内存中现在存在3道程序，分别为P1，P2，P3，它们的一些参数如下图。

![](https://images.gitee.com/uploads/images/2020/0315/093335_6f65f6e9_5550632.png)

现在我们想计算，这两种操作系统的

> 1. 作业全部完成所需时间 
>
> 2. CPU利用率 

**单道批处理系统相关计算**

单道吗，没什么好说的，内存中只能同时存在一项作业，那么总运行时间为
$$
Total\_Time = 60+80+20+120+40+40+30+20+80 = 490ms
$$

$$
CPU\_use\_ratio = \frac{350}{490} * 100\% = 71.43\%
$$



**多道批处理系统相关计算**

多道批处理系统，相对麻烦点，咱们来一步一步分析一下。

基于**先到先服务算法**，优先执行P1，心算比较一下发现，P2，P3在P1第一步计算结束后已经同时进入内存，此时需要通过**高相应比优先调度算法**，决定谁先执行，接着画出时序图，解题。

给出P2，P3两道程序的优先级
$$
P_2 = 1 + \frac{T_w}{T_s} = 1 + \frac{10}{160} = 1.06
$$

$$
P_3 = 1 + \frac{T_w}{T_s} = 1 + \frac{5}{110} = 1.05
$$

P2优先执行，可以进而画出时序图

![](https://images.gitee.com/uploads/images/2020/0315/101605_67215388_5550632.png)

注意上图180ms位置，此时P1剩最后一次计算操作(20ms)而P3已经进入内存并已等待180ms,继续计算优先级
$$
P_1 = 1 + \frac{T_w}{T_s} = 1 + \frac{0}{20} = 1.00
$$

$$
P_3 = 1 + \frac{T_w}{T_s} = 1 + \frac{180}{110} = 2.64
$$

优先执行P3，同理在210ms时计算P1和P2优先级，P1>P2,p1先执行。

在230ms处，计算P2和P3的优先级，P2>P3,P2先执行。
$$
Total\_Time = 如图所示 = 350ms
$$

$$
CPU\_use\_ratio = \frac{350}{350} * 100\% = 100.00\%
$$



完成这三道程序，多道比单道节约了140ms

140ms是什么概念，emmm，咱先看看1ms内CPU能干些什么...

> CPU至少可以执行1000万条指令，(以主流的主频3GHz不到，4个核心计)
>
> CPU到内存之间可以传输几千万个字节的数据
>
> 可以读写普通硬盘1/10次，固态硬盘几十次。
>
> 可以在局域网内传输大概十万个字节数据

由此可见差距之大..



多道批处理系统特征：

1. 多道性：内存中同时放置几个作业
2. 宏观上并行
3. 微观上串行:各个作业交替运行

优点：

1. 资源利用率高
2. 作业吞吐量大：单位时间内完成的工作量大

缺点：

1. 用户交互性差:不能调试和修改
2. 作业平均周转时间长



#### 3.2.2 分时操作系统

解决多道操作系统中用户交互性差的问题

多名用户可操作一台计算机

#### 3.2.3 实时系统

**工业控制系统**

**信息查询系统**

**多媒体系统**

**嵌入式系统**



#### 3.2.3 其它操作系统



### 3.3 操作系统的四个基本特性	

​							 								

> **并发**:指多个事件在同一时间间隔内发生。宏观上表现的是同时发生，微观上串行交替发生 

> **共享**:系统中的资源可供内存中多个并发的进程同时使用,如内存，硬盘 

> **虚拟**:物理上真实存在的实体映射为多个不存在的逻辑上的若干对应物，就像时分多路复用和空分多路复用技术一样，都是同一段时间，同一片内存，却可以产生多人使用，一人独占的情形 

> **异步**:在多道程序环境下，允许多个程序并发，但由于资源有限，进程的执行并不是一贯到底，而是走走停停，以不可预知的进度向前推行



### 3.4 操作系统的五项基本功能



1、处理机管理：主要控制和管理CPU的工作。（CPU管理）

> 单CPU下和多CPU下。

2、存储管理：主要进行内存的分配和管理  （内存管理）

> 内存分配、地址映射、内存保护与共享、虚拟内存等。

3、设备管理：主要管理基本的输入输出设备 （IO管理）

> 完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。
> 主要包括缓冲管理、设备分配、设备处理、虛拟设备等。

4、文件管理：负责对计算机文件的组织、存储、操作和保护等。（文件管理）

> 文件存储空间的管理、目录管理、文件读写管理和保护等。

5、进程管理：也称为作业管理，是指对计算机所进行的操作进行管理。（作业管理）

> 进程控制、进程同步、进程通信、死锁处理、处理机调度等。



## Part 4 进程和状态转换

### 4.1 前驱图

定义：**前驱图**：有向无循环图。图中的**节点**表示一个程序段或一个进程，乃至一条语句；节点间的**有向边**表示结点间存在的偏序或前驱关系。

![](https://images.gitee.com/uploads/images/2020/0324/184113_c030a321_5550632.png)

你要是问我啥是偏序关系的话，那我只能给你数学定义了。

```
设R是集合A上的一个二元关系，若R满足：
Ⅰ 自反性：对任意x∈A，有xRx；
Ⅱ 反对称性：对任意x,y∈A，若xRy，且yRx，则x=y；
Ⅲ 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz。  
则称R为A上的偏序关系，通常记作≼。注意这里的≼不必是指一般意义上的“小于或等于”。
若然有x≼y，我们也说x排在y前面（x precedes y）。
```

简单来讲，前驱图将程序或者程序中的语句抽象成了节点，把运行次序抽象成了边。

所以我们可以得知上图的所有前驱关系

```
P1→P2，P1→P3，P1→P4，P2→P5，P3→P5，P4→P6，P4→P7，P5→P8，P6→P8，P7→P9，P8→P9
```

注意，由于前驱图是用来表示执行次序的，所以在图中**不能出现回路**【从偏序关系的反对称性中也可以看出】



### 4.2 顺序性

知道了如何表示程序的运行次序了，下面我们就要给程序执行的方式分分类了。

当内存中**只有一个**程序**独占所有系统资源**且**不受**任何外界**干扰**，程序内部的程序块是按某种次序依次进行的，**仅**当前一操作结束后，后一操作才能进行的运行方式叫做**程序的顺序执行**

比如：单道批处理系统是典型的顺序性的执行方式

我们把一个程序运行阶段高度概括成3个部分

> Input ：输入
>
> Calculate ：计算
>
> Output：输出

那么顺序执行的程序就像一个糖葫芦

![](https://images.gitee.com/uploads/images/2020/0324/185917_6bdb859e_5550632.png)

同理，程序中的语句也一样

![](https://images.gitee.com/uploads/images/2020/0324/190415_ffc40bc3_5550632.png)

所以顺序执行的程序有如下3个特征:

> **顺序性**：一个程序的各个部分的执行，严格地按照某种先后次序执行。【从语句的顺序性上可以看出】
>
> **封闭性**：程序在封闭的环境下运行，即程序运行时独占全部系统资源。【由于程序的三大步骤在内存中是糖葫芦状的，所以只能一个一个吃】
>
> **可再现性**：只要程序执行时的环境和初始条件相同，当程序重复执行时，不论它是从头到尾不停顿地执行，还是“停停走走”地执行，都将获得相同的结果。【微观语句的顺序执行和宏观程序运行阶段的顺序执行共同保证了可再现性】



### 4.3 并发性

在一段时间内机器上有两个或两个以上的程序同处于**开始运行但尚未结束的状态**，并且次序不是事先确定的。

我们管这样的状态叫做并发。

![](https://images.gitee.com/uploads/images/2020/0324/192642_0201d2a7_5550632.png)

在上图中，我们在内存中一共有3道程序，如果**横向来看**，每个不同程序间是顺序执行的，**向右下看**，每个程序自己是顺序执行的，而当我们朝**左下看**的时候，你会发现，当程序P2进行I2输入时,程序P1在执行C1计算，以此类推，你会发现计算机在同一时刻对不同的程序执行不同的操作。

做个小小的分析:

> 1. 内存中有多个程序处于执行状态
> 2. 它们共享计算机资源
> 3. 因为共享，所以它们的执行顺序是间断的。假设P1是山楂，P2是橘子，P3是草莓，那么原来一串是一串的糖葫芦就变成混着串的了，打乱了原来顺序执行的“糖葫芦模式”。原来是山楂->山楂->山楂，橘子->橘子->橘子，...,现在是,山楂->橘子->草莓->橘子->山楂...



所以并发执有如下执行特点：

> **间断性：**程序并发执行时，由于它们共享资源或程序之间相互合作完成一项共同任务，因而使程序之间相互制约。
>
> **失去封闭性：**程序在并发执行时，是多个程序共享系统中的各种资源，因而这些资源的状态将由多个程序来改变，致使程序的运行失去了封闭性。
>
> **不可再现性：**由于程序的并发执行，打破了由另一程序独占系统资源的封闭性，因而破坏了可再现性。
>
> 封闭性和不可再现性之间的逻辑关系是这样的。封闭，就代表程序无论从宏观流程上还是微观代码上来看都是顺序执行的，绝不可能出错。不封闭，就会导致程序宏观流程的次序发生改变，假如两个售票客户端同时向服务器操作系统申请修改数据库中的票务信息【只有1张票】，该怎么改呢？从时间上来看，就会产生3种情况，快，慢，同时。快的自然是，一个人抢到了，另一个人没有，慢了同理，但是同时呢？把唯一一张票卖给2个不同的人？这里就会看出，并发的异常和不可再现性。



说到底，我们需要一种新的机制来解决并发所带来的时空矛盾。P1,P2同时申请相同资源时，如何分配？需要知道优先级对吧！那么程序是无法表示优先级的，所以我们引出了“进程”这个概念。



### 4.4 进程



#### 4.4.1 进程实体的组成部分

**进程实体 = 程序块 + 数据块 + 进程控制块(Process Control Block)**

进程控制块PCB是操作系统中的必须配置的一种特殊的数据结构。

PCB这个数据结构中存储了这些信息

> 1.进程标识符 name:
> 每个进程都必须有一个唯一的标识符，可以是字符串，也可以是一个数
> 字。
> 2.进程当前状态 status
> 3.进程相应的程序和数据地址，以便把PCB与其程序和数据联系起来。
> 4.进程资源清单：
>
> 列出所拥有的除CPU外的资源记录，如拥有的I/O设备，打开的文件列表等。
> 5.**进程优先级 priority**:【重点在这里！！！】
> 进程的优先级反映进程的紧迫程度，通常由用户指定和系统设置。
> 6.CPU现场保护区 cpustatus
> 7.**进程同步与通信机制**
> 8.进程所在队列PCB的链接字
> 9.与进程有关的其他信息。 如进程记账信息，进程占用CPU的时间等



#### 4.4.2 进程的定义

进程是进程实体的运行过程，是操作系统分配调度资源的独立单位



#### 4.4.3 进程的特征

**动态性**

进程的动态性其实是进程实体的动态性，如果进程实体看作一个巨大的结构体，那么我们想用它的时候就要**创建**，想执行操作的时候就会**执行**它的配套操作函数，不用它的时候就先在内存里存着【**阻塞**】，在彻底不用它的时候就要销毁【**消亡**】。

由此可见，进程就像程序中的变量一样，是存在生命周期的。



**并发性**

我们引出进程的概念就是为了解决并发性的矛盾，其中的PCB就是专门针对并发性所提出的优化手段，同时也是OS的重要特征。



**独立性**

进程实体可以独立运行，独立接受调度，独立分配资源



**异步性**

进程在OS中是异步进行的，每个进程都是独立的，以不可预知的速度向前推进，这也就导致了不可再现性。但是，随着进程的引入，在PCB中，我们设立了进程同步机制，确保了可再现性。



**结构性**

每个进程都有一个PCB对其进行控制，从结构上看进程实体 = 程序段 + 数据段 + PCB



#### 4.4.4 进程和程序之间的区别



> 进程是动态的，程序是静态的【代码是不会自己变的】
>
> 进程是并发的，而程序只能顺序执行
>
> 进程的暂时的，程序是永久的【程序是一个文档】
>
> 进程和程序的结构不同
>
> > 进程实体 = 程序块 + 数据块 + 进程控制块(Process Control Block)
> >
> > 程序 = 数据结构 + 算法 + 语言



#### 4.4.5 进程的基本状态及转换



##### 进程的三种基本状态

以CPU的使用情况为核心，把I/O等其他操作,使用CPU计算和等待CPU分成3段就出现了最基本的三种状态。



**就绪态(Ready)**

指进程所有需要的资源都已经获取，就只剩使用CPU,这些进程就是处于就绪态。



**执行态(Running)**

指进程已将获得CPU使用权限



**阻塞态(Block)**

指正在执行的进程，由于某些事件(I/O请求，申请资源),暂时无法继续执行的状态



##### 不同状态之间的转换

![](https://images.gitee.com/uploads/images/2020/0330/141517_58411a10_5550632.png)



##### 进程的五种状态

为了方便操作系统管理，同时为了迎合广大用户的需求，在三种状态的基础上，再增加两种状态

**创建状态**

创建进程的过程很复杂

简述过程就是

1. 申请空白PCB
2. 为该进程分配资源
3. 将进程转成就绪状态，进入就绪队列

如果，进程的所需资源不能得到满足(内存不够)，不能被调度执行，该进程就处于创建状态。



**终止状态**

进程终止的原因有：

1. 当一个进程自然结束
2. 出现了无法克服的错误
3. 被OS终止
4. 被其它拥有中止权的进程所终结

终止之后，会出现下述特征：

1. 该进程的PCB被OS回收，清除空间返还OS
2. 失去执行资格

![](https://images.gitee.com/uploads/images/2020/0331/194551_7387430b_5550632.png)



##### 进程的七种状态



**挂起状态**

什么是挂起，举个例子，用手机打电话。

就绪状态就像是正在拨号，就等对方接听

执行状态就是两人正在交流

阻塞状态就像是对方像你询问某些信息但是你现在手头没有，需要去查，**之后再给他拨回去**

而挂起状态就像是，你聊着聊着突然第二个人给你打过来，串线了，手机会提醒你是否挂起当前会话是一样的，此时与上一个人的通话并未结束，只是在等待。

由此可以看出，挂起状态和阻塞状态是有很大区别的。

阻塞的根本原因在于：**缺少某些资源，导致进程无法继续执行**。

挂起的根本原因在于：**由于计算机资源有限，需要这种机制“暂停”某些进程，将资源分配给更需要的进程**。

当CPU占用率飙升至99%时，一定会挂起很多无用进程，依次提高运算能力。不过需要注意一点的是，**被挂起的进程可以随时随地转换回就绪态**。

下面是挂起状态出现的其他原因（来源：百度）：

```
（1）终端用户的请求。当终端用户在自己的程序运行期间发现有可疑问题时，希望暂停使自己的程序静止下来。亦即，使正在执行的进程暂停执行；若此时用户进程正处于就绪状态而未执行，则该进程暂不接受调度，以便用户研究其执行情况或对程序进行修改。我们把这种静止状态成为“挂起状态”。
（2）父进程的请求。有时父进程希望挂起自己的某个子进程，以便考察和修改子进程，或者协调各子进程间的活动。
（3）负荷调节的需要。当实时系统中的工作负荷较重，已可能影响到对实时任务的控制时，可由系统把一些不重要的进程挂起，以保证系统能正常运行。
（4）操作系统的需要。操作系统有时希望挂起某些进程，以便检查运行中的资源使用情况或进行记账。
（5）对换的需要。为了缓和内存紧张的情况，将内存中处于阻塞状态的进程换至外存上。
```

由于**挂起**是一种**暂停**，所以，在就绪态被挂起，就变成了**静止就绪状态**。同理，在阻塞态被挂起就变成了**静止阻塞状态**，原来正常的就绪态就成为了**活动就绪状态**，原来正常的阻塞态就变成了**活动阻塞状态**。

这一加，可不就变成七种状态了吗。

![](https://images.gitee.com/uploads/images/2020/0331/202047_5adf78dc_5550632.png)



此外，在UNIX系统中，进程之间存在继承关系，即进程可以被创建

![](https://images.gitee.com/uploads/images/2020/0413/153408_358026bd_5550632.png )

A 是 B 的父进程，D 是 I 的父进程，我们把这颗树的根叫做**祖先(Ancestor)**，这张图叫做**进程图**。



#### 4.4.6 进程控制

上面讲了进程的基本状态，但是，并没有细致地说明操作系统是**如何进行状态切换的**，下面就是在简述OS是如何进行进程状态控制的。



**操作系统内核**

操作系统按功能分层，其中最靠近硬件层面的功能，就成为了内核，它负责管理进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。内核常驻在内存当中，以便提高执行运行效率。为了**保护**操作系统不被其它程序所干扰，CPU的工作状态分为：**内核态** 和 **用户态**。

> **内核态：**允许访问cpu的全部指令，可以访问所有的寄存器和存储区
>
> **用户态：**只允许访问cpu的非特权指令，访问制定的寄存器和存储区。如果在用户态下企图运行一条特权指令，cpu就视其为非法指令，终止其运行。

也就是说操作系统有着**最高的权限**，方便它调度，分配资源。

操作系统的内核主要有两大方面的功能：

**支撑功能**

> 1. 中断处理 【输入，进程调度，设备驱动，系统调用...】
> 2. 时钟管理 【时间片轮转调度】
> 3. 原语操作(Primitive) 

**资源管理功能**

> 1. 进程管理
> 2. 存储器管理
> 3. 设备管理 【I/O设备...】



**原语(Primitive)**

**原语概念：** 是由若干条机器指令所构成，用以完成特定功能的一段程序。

**原语特性：** 这段程序在执行期间不可分割。也就是说原语的执行不能被中断。所以，原语操作具有**原子性**【要么全都完成，要么全都不做】。

原语可以分成如下的几类：

> 创建原语(Create)：创建一个有标识符的进程
>
> 撤销原语(Destroy)：撤销一个进程-进程控制块PCB
>
> 阻塞原语(Block)：首先中断CPU执行，并保存该进程的[CPU现场](https://baike.baidu.com/item/保护现场/6100961?fr=aladdin)，然后把阻塞状态赋予该进程，并将它插入具有相同实体的阻塞队列中。
>
> 唤醒原语(Wakeup)：先把被唤醒进程从阻塞队列中移出，设置该进程当前状态为就绪状态，然后再将该进程插入到就绪队列中，注意：不能自己唤醒自己。
>
> 挂起原语(Suspend)：让某些进程暂时不参与资源竞争
>
> 激活原语(Active)：激活指定标识符的进程



#### 4.4.7 线程

由于我们想要并发，发明了进程，而随着时间的推移，我们发现进程如果作为接受操作系统调度的最小单位，会非常耗时。这是因为进程间切换需要以下步骤：

1) 决定是否作[上下文切换](https://baike.baidu.com/item/上下文切换)【进程切换】以及是否允许作上下文切换。包括对进程调度原因的检查分析，以及当前执行进程的资格和CPU执行方式的检查等。在操作系统中，上下文切换程序并不是每时每刻都在检查和分析是否可作上下文切换，它们设置有适当的时机。



(2) 保存当前执行进程的上下文。这里所说的当前执行进程，实际上是指调用上下文切换程序之前的执行进程。如果上下文切换不是被那个当前执行进程所调用，且不属于该进程，则所保存的上下文应是先前执行进程的上下文，或称为“老”进程上下文。显然，上下文切换程序不能破坏“老”进程的上下文结构。



(3) 使用进程调度算法，选择一处于就绪状态的进程。



(4) 恢复或装配所选进程的上下文，将CPU控制权交到所选进程手中

总结一下就是，切换进程需要：**判断**，**检查**，**保存现场【状态】**，**更改状态**

其中上下文切换非常耗时，为了进一步提高操作系统对计算机资源的使用效率，我们发明了**线程**

线程的发明思想其实很简单，就是 **解耦合**。

进程曾经有两大特点

> 1. 接收OS调度
> 2. OS分配资源的单位

现在，你不是接收调度时费时吗？我们把这个功能抽出来，捏吧捏吧，线程就出来了。

从之前的体系改变成了：

进程：接收OS所分配的资源

线程：接收OS调度

进程和线程之间的关系：

>  一个进程下有多个线程，线程之间共享同一进程下的资源，**线程本身没有任何资源**

![](https://img-blog.csdnimg.cn/2019042015123733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI1ODkwOA==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/20190420151245233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI1ODkwOA==,size_16,color_FFFFFF,t_70)

多线程就像盖一栋楼，盖楼这个事情是进程，OS给你原材料，而里面接收调度的水管工，电气工，油漆匠，建筑工人....就是进程，一项资源不全，我们可以干其它的事情，而不用整个停工。



### 4.5进程的同步和互斥

之前讲了那么多的内容，我们貌似还是没有解释并发所导致的**不可再现性**的问题。现在让我们深入地探寻一下进程之间存在着那些关系？不可再现性如何解决？

#### 4.5.1 进程的同步

现在有两个进程A，B

运行到了某个时刻，A进程发现缺少数据，而这个数据只能由进程B提供，我们此时进程A和B的关系是**同步关系**，它们之间互相合作，协同工作，形成了一种**直接制约关系**。

![](https://images.gitee.com/uploads/images/2020/0422/153357_46904895_5550632.png)

同步->合作->直接制约（缺另一个进程的数据）

#### 4.5.2 进程的互斥

现在有两个进程A，B，外加一台打印机

进程A的工作是打印中文文档，进程B的工作是打印英文文档。你会发现这两个进程之间的关系是由**中间媒介(打印机)** 联系在一起的。两个进程**互相竞争**计算机资源，由于这个资源只能**互斥使用(在同一时刻只能有一个进程使用)**，所以它们之间形成了**间接制约关系**，打印机或公共变量这一类的系统资源也变成了**临界资源**。

![](https://images.gitee.com/uploads/images/2020/0422/153427_205b96ef_5550632.png)

互斥->竞争->间接制约（中间有互斥资源，一台打印机）

对于系统中资源的共享程度我们就可以分成：

> 互斥：多个进程**不能同时使用同一资源**
>
> 死锁：多个进程**互不相让**，谁都不能得到足够的资源。
>
> 饥饿：某些谦让的进程**一直得不到资源**

#### 4.5.3 进程同步和互斥的解决方法

在真正去解决进程同步和互斥所带来的问题之前我们要明确一些概念和原则。

##### 访问过程

**进入区(entry section)**

检查当前进程可否进入临界区的一段代码。

如果当前进程可以进入临界区，通常设置相应“正在访问临界区”标志，防止其他进程同时进入临界区。

**临界区(critical section)**

进程中访问**临界资源**的一段代码。

**退出区(exit section)**

用于将“正在访问临界区”的进程的标志清除

**剩余区(remainder section)**

代码中的其余部分。

![](https://images.gitee.com/uploads/images/2020/0422/154035_08a951bf_5550632.png)

##### 四大原则

> **空闲让进**：系统中只要资源空闲就立刻让进入临界区的进程使用。
>
> **忙则等待**：如果临界资源正在被占用，其它进程则必须需要等待，保证互斥访问。
>
> **有限等待**：一个在临界区的进程的等待时间是有上限的，不能无限期地等待。
>
> **让权等待**：若在进入区等待临界资源的进程身上有**处理机使用权限**则必须释放处理机，进入阻塞状态。



##### 软件方法

软件方法的核心思想就是，在进入区设置和判断一些标识符，如果已有进程在临界区，则在进入区通过循环检查进行等待。临界区进程访问结束后，需要在退出区修改标志。

此时的问题就变成了，如何设置标志？如何检查标志？

**算法 1 单标志算法**

![](https://images.gitee.com/uploads/images/2020/0422/155957_c6415ef4_5550632.png)

Pi 在进入区做**空循环等待**操作，什么时候公共整型变量**turn == i**，什么时候 Pi 进入临界区访问临界资源，访问结束后，立即将 **turn** 改为 j ，以便进程 Pj 访问临界区资源。

此算法灵活度很低，因为访问临界资源的次序是强制性的，可能在某一时刻 Pj 并不想访问临界资源，此时临界资源处于空闲状态，资源利用率低。



**算法 2  双标志、先检查后修改**

![](https://images.gitee.com/uploads/images/2020/0422/160719_45f56199_5550632.png)

标志数组 flag 表示：**哪个进程正在访问临界区资源**。对于 Pi 来说，如果循环判定 Pj 正在使用临界资源则等待。

当 Pj 不使用时，Pi 立即进入临界区，首先改自己的标识符为**true**，意味着 Pi 进程正在访问临界区资源，访问完毕后立即更改标识符为 false。

此算法解决了强制访问的问题，但是如果当 Pi 和 Pj 同时对临界资源进行访问，可能会出现同时访问临界区资源的情况。

**算法 3 双标志、先修改后检查**

![](https://images.gitee.com/uploads/images/2020/0422/161428_6027b0e1_5550632.png)

标志数组 flag 表示：**哪个进程想访问临界区资源**

Pi 首先在进入说了一声**“我想进临界区！！”**，然后循环判定，如果发现 Pj 也想进入临界区，那么 Pi 谦让地在进入区等待，待 Pj 访问结束后，Pi 再进入临界区。当 Pi 访问结束后，它又会喊了一嗓子**“我不想进临界区了！！”** 。

采用这种算法来处理进程间互斥关系可以，但是当两个进程同时访问的话，就会因为过于谦让，使两个进程同时等待，谁也无法访问临界区，进而进入”死等“状态。

**算法 4 先修改、后检查、后修改者等待**

![](https://images.gitee.com/uploads/images/2020/0422/162535_7189a257_5550632.png)

公共变量 turn 表示：**谁先修改的标志**。

标志数组 flag 表示：**哪个进程想访问临界区资源**。

对于 Pi 循环等待的时候，就看两个东西，如果 Pj 不想进那就 Pi 直接访问临界区资源，体现出**空闲让进**原则，如果 Pj 想进，且是 Pj 先改的 turn ，那么 Pi 在进入区等待，体现出**先来先进**。



##### 信号量方法 (Semaphore)

由于软件方法只能解决少量进程互斥访问临界资源的问题，因此我们引出**信号量**的概念。

**信号量是什么？**

以一个停车场的运作为例。假设停车场只有三个车位，一开始三个车位都是空的。这时如果同时来了五辆车，看门人允许其中三辆直接进入，然后放下车拦，剩下的车则必须在入口等待，此后来的车也都不得不在入口处等待。这时，有一辆车离开停车场，看门人得知后，打开车拦，放入外面的一辆进去，如果又离开两辆，则又可以放入两辆，如此往复。

在这个停车场系统中，车位是公共资源，每辆车好比一个**进程**，看门人起的就是**信号量**的作用。在操作系统中，我们会为每一种临界资源都设置一个信号量。

抽象来讲，**信号量**是一种管理调度临界资源的手段。

信号量有很多种，常见的有：

> 1. 整型信号量（integer semaphore)：信号量是整数。
>
> 2. 记录型信号量（record semaphore)：每个信号量s，除有一个整数值s.value（计数）外，还有一个进程等待队列s.list,s.list内部是阻塞在该信号量的各个进程的标识。
>
> 3. 二进制信号量(binary semaphore)：只允许信号量取0或1值。

我们为了方便理解，着重讲一讲**记录型信号量**，我们可以把一个**管理打印机资源的记录型信号量**理解成一个C++样式的结构体：

```c++
#include<queue>
//假设我们有一个数据类型用来存各个进程标识符[其实可能就是PCB]
typedef struct printerSemaphore
{
    int value;
    queue<PCB> list;
}sem_t;
```

哦吼，现在我们把信号量这一概念具象成了一种数据结构，那么我们就用学习数据结构的思考方式来类比学习信号量吧。**事先声明，以下代码均为描述思路，不要拘泥于语法细节。**

对于一个信号量，首先要有两个基本操作，就是**初始化**和**销毁**，我们可以通过如下两个函数进行操作。

```c++
int sem_init(sem_t *sem, int pshared, unsigned int value);
int sem_destory(sem_t *sem);
```

由于，信号量的功能是**管理临界资源**，其核心就是监视**临界资源的数量**，所以引出两种**原语**操作：

1. 等/挂起 信号 wait() => P 原语 ： 申请资源  sem->value--
2. 给/发 信号 signal()/post() => V 原语 ：释放资源 sem->value++

这个value就代表着有多少个临界资源：

> value > 0：有空闲临界资源
>
> value = 0：正好没有临界资源
>
> value < 0：|value|是等待队列中的进程个数

```c++
int sem_wait(sem_t *sem) //P原语
{
	sem->value--;
    if(sem->value < 0)//临界资源正好用完或者本来就不够，该进程进入等待队列，进程状态变为阻塞态
    {
        sem->queue.push(/*该进程的信息*/);
        //阻塞调用进程
    }
    return sem->value;
}

int sem_post(sem_t *sem) //V原语
{
    sem->value++;
    if(sem->value <= 0)//临界资源经释放后，等待队列中依然有进程，将队头进程取出，置为就绪态
    {
        PCB p = sem->queue.front();
        //置队头进程为就绪态
        sem->queue.pop();
        p.status = '就绪态';
    }
    return sem->value;
}
```

###### 信号量的分类

根据进程间的制约关系不同，即，同步和互斥关系，我们把信号量分成两类：

**公用信号量**：   一般为**进程互斥**而设，每个进程都可以对它施加P操作和V操作，联系着一组进程。初值一般为**1**。

**私用信号量**：一般为**进程同步**而设，仅允许拥有它的进程对它施加P操作（申请资源的进程），其他进程对它施加V操作释放资源)。初值一般为**0**或某个正整数**n**。

![](https://images.gitee.com/uploads/images/2020/0430/115421_e4aecad2_5550632.png)

按照之前的分区方式，**P原语在进入区，V原语在退出区**

![](https://images.gitee.com/uploads/images/2020/0501/143519_8b1fc31c_5550632.png )

##### 经典的进程同步互斥模型

**生产者-消费者模型**

![](https://images.gitee.com/uploads/images/2020/0501/144415_b873f6c6_5550632.png)

现在有一个大小为 N 的共用缓冲池。如图所示，生产者和消费者的功能。我们认为：

1. 只要缓冲池中仍有空闲的缓冲区就可以,**不断地生产。**

2. 同样，只要有缓冲区仍有物品就可以**不断地消费。**

3. **同时只能有一个生产者或消费者，添加或删除物品**

由此，我们可以明确的判断出，生产者和消费者之间是**同步和互斥关系**，而每个生产者之间或消费者之间为**互斥关系**，

根据信号量理论，我们提出3个信号量。

> 1. 对于所有的生产者，我们设计**信号量full**，判断缓冲池是否已满，代表占用区，**初值 0**
> 2. 对于所有消费者，我们设计**信号量empty**，判断缓冲池是否为空，代表空闲区，**初值 N**
> 3. 对于生产者和消费者之间，我们设计**信号量mutex**，用于解决**同步和互斥**问题，由于只有一个共用缓冲池，**初值为 1**

那么对于一个生产者进程就会有如下过程：

![](https://images.gitee.com/uploads/images/2020/0501/145813_60e94e34_5550632.png)

**在进入区，首先执行两个 P 原语【申请】操作：**

1. 让 empty 信号量的值 -1 ，代表空闲区的减少
2. 让 mutex 信号量 -1，代表缓冲池资源正在被占用

**在退出区，执行两个 V 原语【释放】操作：**

1.  让 mutex 信号量 +1 ，代表释放缓冲池资源
2.  让 full 信号量 +1，代表占用区数量 的增加

对于一个消费者也会有如下过程：



![](https://images.gitee.com/uploads/images/2020/0501/145847_f587b103_5550632.png)



**在进入区，首先执行两个 P 原语：**

1. 让 full 信号量 -1 ，申请占用区资源
2. 让 mutex 信号量 -1，申请缓冲池资源，代表缓冲池资源正在被占用

**在退出区，执行两个 V 原语：**

1. 让 mutex 信号量 +1，释放缓冲池资源
2. 让 empty 信号量 +1，释放空闲区资源

对于互斥问题，一旦 mutex 信号量的 value 值为**负整数**，进程就会进入等待队列，进程状态由执行态转换成阻塞态。

对于同步问题，一旦 full ， empty，信号量的 value 值为 N 或 0，进程也会进入相应的等待队列，进程状态由执行态转换成阻塞态。

![](https://images.gitee.com/uploads/images/2020/0501/152010_ba33efd5_5550632.png)



**读者-写者模型**

问题描述：读者和写者共若干人，都可以向同一个存储区进行操作。

写者执行写操作时,每次只允许一位写者进行写操作。读者执行读操作，可以多名读者同时进行读操作，读者读的时候写者不能写。

也就是说：这个模型存在两种关系：

**写者之间是互斥关系**

**读者和写者之间是互斥的**

设置**wmutex信号量解决，写者之间和读者和写者之间的两个互斥问题**

设置**readcount临界变量用来计数读者的数量**

设置**rmutex用来保护rcount临界资源**

```c
semaphore wmutex = 1;

semaphore rmutex = 1;

int rcount = 0;// 记录有几个读者
```

对于读者Reader

```c
while(true)
{
	wait(rmutex);
	if(rcount == 0)
	{
		wait(wmutex);//第一个读者进来时，保证写者不能写
	}
	rcount++;
	signal(rmutex);//readcount已经互斥访问完毕
	//临界区代码-读的过程
	wait(rmutex);
	rcount--;
	if(rcount == 0)
	{
		signal(wmutex);//保证最后一个读者读完
	}
	signal(rmutex);
}
	
```

对于写者Writer：

```c
while(true)
{
	wait(wmutex);
    //临界区代码-写的过程
    signal(wmutex);
}
```



## Part 5 处理机调度与死锁

### 5.1 处理机调度

在很早之前我们就介绍过有关多道批处理系统的作业调度算法，但是，我们一直没有明确过作业的定义，以及它和进程的关系。现在让我们看看**作业**是什么。

作业 ( Job )：你可以把它看作一个大的任务，比如，教室进行大扫除，扫除内容包括，扫地，墩地，擦黑板……

而每个扫除的**具体内容由不同进程分担**。

所以，**作业是为了完成一个特定任务的进程的集合**。宏观上的作业调度，微观上就体现在了进程调度，线程调度上。

**按数据所处计算机的位置**，可以分为，**高级调度**，**中级调度**。

**按数据处理的层次**，可以分为，**中级调度**，**低级调度**。

#### 5.1.1 高级调度

高级调度的对象是**作业**，因此又称**作业调度**，**宏观调度**。调度的位置是在 **外存**，高级调度负责决定在**后备队列中哪些作业可以被调入内存**。

时间单位为：分钟，小时，天

#### 5.1.2 中级调度

中级调度又称**内存调度**，它负责把作业的相关进程载入内存中，以便CPU进行处理。

#### 5.1.3 低级调度

低级调度又称**微观调度**，**进程调度**，调度的主体为**进程或线程**，此时要选择就绪队列中的进程，让其切换到执行状态。

![](https://images.gitee.com/uploads/images/2020/0519/184747_24df85b8_5550632.png)

#### 5.1.4 性能准则

和算法，计算机网络一样，我们需要一个标准来评判一个操作系统的调度算法的好坏。这个就是我们之前提及到的，**平均周转时间**，**平均带权周转时间**。

复习一下：

周转时间：一个作业的终止时间 - 开始时间

平均周转时间：所有作业的周转时间的均值

带权周转时间：一个作业的周转时间 ➗ CPU使用时间 (服务时间)

平均带权周转时间：所有作业的带权周转时间的均值

作业，进程，线程调度算法一共有 6 种，在多道批处理系统的时候我们已经详细地介绍了其中的 3 种：

1. 先到先服务调度算法 (First Come First Service)
2. 短作业优先调度算法 (Shortest Job First)
3. 高响应比优先调度算法 (Highest Response Ratio Next)

剩下的就还有

1. 高优先权优先调度算法
2. 时间片轮转调度算法
3. 多级反馈队列调度算法

我们快速的复习一下，前三种算法的注意事项：

先到先服务，没啥可说的，会算之前的4种时间即可。

短作业优先，**第一个到的作业无论服务时间多长，必须第一个被调度**

高相应比优先算法，**每次调度前都要重新计算相应比(优先权)**

设优先值为P，等待时间为Tw，服务时间为Ts

$$
P = \frac{T_w+T_s}{T_s} = 1 + \frac{T_w}{T_s}
$$

##### 高优先权优先调度算法

优先权一共分为两种**静态优先权** 和 **动态优先权**，

**静态优先数法**：静态优先权是在创建进程时确定的，在整个运行期间不再改变。

**动态优先数法**：在进程创建时创立一个优先数，但在其生命周期内优先数可以动态变化。

按照调度算法的不同又可以分为两类：

1. **非抢占式**：是指一旦把处理机分配给就绪队列中优先权最高的进程后便让该进程一直执行，直到该进程完成或发生某事件而被阻塞时，才把处理机分批给其它进程。

2. **抢占式**：是指根据某种原则，停止某个正在执行的进程，将已分配给该进程的处理机，重新分配给另一个优先权更高的进程。



##### 时间片轮转算法

原理：系统将所有就绪进程按**先进先出**的规则排队，**按一定的时间间隔（时间片）**把处理机分配给队列中的进程。



##### 多级反馈队列调度算法

多级反馈队列算法是**时间片轮转算法**和**高优先权优先算法**的综合和发展。它体现在可以动态调整**进程优先级**和**时间片大小**。

![](https://images.gitee.com/uploads/images/2020/0519/190421_eef2ed27_5550632.png)



### 5.2 死锁

在之前的学习中，通过信号量等方式我们解决了进程同步互斥的问题，又通过各种调度算法，使作业成功地进入了内存，但是一个新的问题又摆在了我们的面前。假设如下场景：

在内存中有，进程 A，B 两个进程，有两个临界资源，x，y。

刚开始，A申请x，B申请y，基于空闲让进的原则，它们都可以访问到临界区资源。

然而接下来恐怖的事情发生了：

$\color{red}{A 想申请 y， B 想申请 x}$，这两个进程互不相让，僵持在这里，如果没有机制去解决**这个问题(死锁)**的话，整个系统都可能会崩溃。为了探求解决方式，我们就需要深入地探讨一下死锁是什么，以及它形成的原因和最后的解决方法。

#### 5.2.1 死锁的定义

如果一组进程中的每一个进程都在等待仅由该组进程中的其它进程才能引发的事件，该组进程就是死锁的(Deadlock)

翻译成大白话就是：$\color{red}{我这有你想要的，没有不行，你这有我想要的，没有不行，但就是谁都不给谁}$。

引发死锁的是对某种资源的竞争而导致的，那计算机中出来临界资源这种资源划分方式，还有什么划分方式呢？

按照资源的可再生性我们分为：

> 可重用性资源：可供用户重复使用的资源，一般都是通过这样的访问次序进行访问，请求资源，使用资源，释放资源，非常像信号量机制下控制访问临界资源一样。比如：打印机，文件。
>
> 可消耗性资源：由生产者生产，由消费者消费的资源，例如：进程间通信的消息。

按照资源的调度方式我们可分为：

> 可抢占式资源：优先级高的进程可以剥夺走优先级低的进程的资源使用权限。
>
> 不可抢占式资源：资源一旦被分配就不可以被抢占

![](https://images.gitee.com/uploads/images/2020/0525/194426_6339182f_5550632.png)

#### 5.2.2 死锁产生的原因

死锁的根源在于进程间对资源的竞争，所以**互斥**一定是产生问题的原因之一。

单纯的互斥只是起了一个头，对于**不可抢占的资源**进行互斥访问时，极有可能出现死锁。

由于进程在内存中是异步进行的，所以，**错误的进程推进顺序**也有可能造成死锁。

由于死锁之后，进程**只会贪婪地请求资源，而不会主动释放资源**，这会导致死锁无法解决。

总结一下，产生死锁的4个必要条件，$\color{red}{当以下四项同时出现时，死锁一定会发生}$。

**互斥**

 任何时刻只允许一个进程使用资源。

**请求和保持**

进程在请求其他资源时，不主动释放已经占用的资源。

**不可抢占**

进程已获得的资源在未使用完之前不能被抢占，只能在进程使用完时由自己释放。

**环路等待**

发现死锁时，必然存在一个进程—资源的环路。



#### 5.2.3 死锁的解决方法

为了避免或解决死锁的出现，我们只需要把上述的4个条件中的任意一个条件给破坏掉，自然死锁就不会出现。

互斥访问是对临界资源的正确访问形式，所以不能摒弃。

那我们就可以有如下两个大思路：

> 1. 将资源事先分配好，不要在进程执行过程中再去申请其他资源
> 2. 把资源变成**可抢占性资源**



##### 银行家算法 （Banker's Algorithm）

在学习该算法如何解决死锁问题之前，先看一看操作系统中**安全状态**的定义：

> 所谓安全状态是指系统能按某种顺序如<P1,P2,...,Pn>(称<P1,P2,...Pn>序列为安全序列)，来为每个进程分配其所需资源，直到最大需求，使每个进程都可顺序完成。

翻译成人话就是：将内存中的所有进程进行排班，当所有进程的资源需求都被满足时，可以找出一组执行序列，这个序列就是安全序列，有安全序列的一定是安全状态，不会发生死锁。

定义一坨数据结构：

1）可利用资源向量**Available**
是个含有m个元素的数组，其中的每一个元素代表一类可利用的资源数目。如果Available[j]=K，则表示系统中现有Rj类资源K个。

> 例如：Available[0]代表打印机资源，[1]代表读卡器资源。

2）最大需求矩阵**Max**
这是一个n×m的矩阵，它定义了系统中n个进程中的每一个进程对m类资源的最大需求。如果Max[i,j]=K，则表示进程i需要Rj类资源的最大数目为K。

> 例如：PID为 i 的进程，最多需要 j 类资源的个数

3）分配矩阵**Allocation**
这也是一个n×m的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果Allocation[i,j]=K，则表示进程i当前已分得Rj类资源的 数目为K。

> 例如：给PID 为 i 的进程，分配 j 类资源的个数

4）需求矩阵**Need**
这也是一个n×m的矩阵，用以表示每一个进程尚需的各类资源数。如果Need[i,j]=K，则表示进程i还需要Rj类资源K个，方能完成其任务。

> Need[i,j] = Max[i,j] - Allocation[i,j]

###### 算法原理

我们可以把操作系统看作是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。
为保证资金的安全，银行家规定：

> (1) 当一个顾客对资金的最大需求量不超过银行家现有的资金时就可接纳该顾客
> (2) 顾客可以分期贷款，但贷款的总数不能超过最大需求量
> (3) 当银行家现有的资金不能满足顾客尚需的贷款数额时，对顾客的贷款可推迟支付，但总能使顾客在有限的时间里得到贷款
> (4) 当顾客得到所需的全部资金后，一定能在有限的时间里归还所有的资金.

换到计算机的场景下就是：

> (1) 当一个进程对某种资源的最大需求量不超过系统拥有的最大值
>
> (2) 进程可以多次请求资源，但是不得超过它的最大需求量
>
> (3) 当操作系统现有资源不能满足进程需求的时候，可以延迟满足，但总能让进程的等待时间有限
>
> (4) 当进程得到所有资源时，一定能在有限时间内归还给操作系统

![](https://images.gitee.com/uploads/images/2020/0601/115833_aeedb9e5_5550632.png)

整个流程简述下来是这样的：
$$
Request_i[j]:是一个进程i对j类资源的请求向量\\j = 0,1,...
$$


0. 资源，需求初始化，即，生成：Available，Max矩阵

1. 进程 i 发出请求，如果Request~i~[j] > Need\[i][j]，报错返回,因为它申请的资源数量超过了它所需的最大资源数量。

2. Request~i~[j] ≤ Available[j] ，进入步骤 3，否则表示尚无该类资源，进程需要等待。

3. 操作系统尝试分配资源，并更改下列数据结构：

   > i. Available[j] = Available[j] - Request~i~[j]
   >
   > ii. Allocation\[i][j] = Allocation\[i][j] + Request~i~[j]
   >
   > iii. Need\[i][j] = Need\[i][j] - Request~i~[j]

4. 操作系统执行**安全性算法**，如果此次分配是安全状态则分配，否则，此次试探的分配方式作废。

###### 安全性算法

再定义两个数据结构，work向量和finish向量。

> work向量：当前进程资源释放后，操作系统中所有可利用的资源,**初始值 work  := Available**
>
> finish[i]向量：代表是否可以分配给该进程 i 

![](https://images.gitee.com/uploads/images/2020/0601/135512_73615f9e_5550632.png)

安全性算法，本质上就是，释放原先进程占用的资源加到work向量上，然后任意找一个进程需求量小于work向量的进程，把finish向量的所有值改成true，这就是安全的。我们就可以对进程 i 分配资源。

看到例题，以P0进程为例：

![](https://images.gitee.com/uploads/images/2020/0601/150307_b0523ea0_5550632.png)

问题一：当P0如下申请时，操作系统可以进行分配吗？

P0：Request(0,2,0)

首先看，Request(0,2,0) < Need(7,4,3),步骤1，通过。

Request(0,2,0) < Available(3,3,2),步骤2，通过，

开始尝试对P0分配资源：

> Available : (3-0,3-2,2-0) = (3,1,2)
>
> Allocation : (0+0,1+2,0+0) = (0,3,0)
>
> **Need:(7-0,4-2,3-0) = (7,2,3)**

开始执行安全性算法：

初始化Work和finish

Work:= Available = (3,1,2)

开始寻找：

P3的Need（0，1，1）可以被当前Work（3，1，2）所满足：Work+=Allocation：（5，2，3），finish[3] = true

P1的Need（1，2，2）可以被当前Work（5，2，3）所满足：Work+=Allocation：（7，2，3），finish[1] = true

P2的Need（6，0，0）可以被当前Work（7，2，3）所满足：Work+=Allocation：（10，2，5），finish[2] = true

P4的Need（4，3，1）不可以被当前Work（10，2，5）所满足：finish[4] = false

P0的Need（7，4，3）不可以被当前Work（10，2，5）所满足：finish[0] = false

所以，P0进程的请求不能被满足。

问题二，当前状态是否安全？

当前Work:=Available = （3,3,2）

P1的Need（1，2，2）可以被当前Work（3，3，2）所满足：Work+=Allocation：（5，3，2），finish[1] = true

P3的Need（0，1，1）可以被当前Work（5，3，2）所满足：Work+=Allocation：（7，4，3），finish[3] = true

P4的Need（4，3，1）可以被当前Work（7，4，3）所满足：Work+=Allocation：（7，4，5）,finish[4] = true

P2的Need（6，0，0）可以被当前Work（7，4，5）所满足：Work+=Allocation：（10，4，7），finish[2] = true

P0的Need（7，4，3）可以被当前Work（10，4，7）所满足：Work+=Allocation：（10，5，7），finish[0] = true

所有进程的finish都是true，这个状态是安全的。

## Part 6 存储器管理

### 6.1 存储器的层次结构

存储器可以分为4层，**越在底层读取速度越慢，容量越大，价格越便宜**，反之亦然。

![](https://images.gitee.com/uploads/images/2020/0608/120207_a6c8114b_5550632.png)



寄存器：有限，最快，易变

> 一个寄存器只能存 1 bit，**容量有限**
>
> 由于它是最底层的硬件之一，**它的读写速度最快**
>
> 由于它是最底层的硬件之一，**它里面存的数据最容易发生改变**

快速缓存：少量，非常快，易变

> Cache的出现是为了解决CPU处理过快，而内存读取速度过慢造成的资源浪费而发明的

内存：若干GB，中等速度，易变

外存：容量大，最慢，不易变

> 通过物理介质进行信息的永久性保存，不像前面的存储器需要依赖电源

#### 6.1.1 内存管理的主要功能

> 1. 内存分配与回收
> 2. 逻辑地址和物理地址的转换
> 3. 内存空间的共享与保护
> 4. 内存扩充

![](https://images.gitee.com/uploads/images/2020/0608/130539_b9db183f_5550632.png )

### 6.2 程序的装入和链接

C语言写完的程序，要通过**编译和链接**两步，如下图所示这样：

![](https://images.gitee.com/uploads/images/2020/0608/130747_88d1fa1c_5550632.png)

所谓装入程序，**实际上就是创建PCB，形成进程的过程**。

### 6.3 连续分配存储管理

#### 6.3.1 单一连续分配

在单道批处理系统中，采用单一连续分配，即**除了系统区，一个进程独占整个内存资源，不论内存是否又空余，内存空间中能有一个进程在执行**。

![](https://images.gitee.com/uploads/images/2020/0608/131158_d56225d3_5550632.png )

#### 6.3.2 分区分配

##### 固定分区(fixed partitioning)

顾名思义，操作系统将内存划分成若干大小相同的区域，操作系统占用一个区域，其它进程按其所需大小分配到不同的区域。

![](https://images.gitee.com/uploads/images/2020/0608/131519_e456d1f8_5550632.png )

特点是可以适应多道批处理系统，但是产生的内碎片可能会被浪费。

**内碎片**：分区内部剩余空间

**外碎片**：分区之间剩余空间



##### 动态分区(dynamic partitioning)

同理，顾名思义，这种分区方式有OS动态分配大小，按照作业需求进行分配。

![](https://images.gitee.com/uploads/images/2020/0608/131941_6f19b058_5550632.png )

分区释放时，会把多个空闲分区合并。

优点在于没有内碎片的产生，缺点是带来了外碎片的问题。

![](https://images.gitee.com/uploads/images/2020/0608/132140_1ff9ba5f_5550632.png)

##### 6.3.2 常用的动态分区分配算法

###### 6.3.2 首次适应算法(first-fit)

![](https://images.gitee.com/uploads/images/2020/0609/175627_52474ffb_5550632.png )

###### 6.3.2.2 循环首次适应算法（next-fit）

![](https://images.gitee.com/uploads/images/2020/0609/175731_3fdd91bb_5550632.png)

###### 6.3.2.3 最佳适应算法（best-fit）

![](https://images.gitee.com/uploads/images/2020/0609/175944_2c29a31f_5550632.png)

###### 6.3.2.4 最坏适应算法（worst-fit）

![](https://images.gitee.com/uploads/images/2020/0609/181509_780f2b86_5550632.png )

###### 6.3.2.5 快速适应算法（quick-fit）

![](https://images.gitee.com/uploads/images/2020/0609/181633_c9564bab_5550632.png)

#### 6.3.3 离散连续分配

之前我们所说的一切分区方式，都是**连续地址分配的**，现在我们所探寻的是**地址离散的连续分配方式**

##### 分页式存储管理

分配原理，先分割。

将进程的逻辑地址按一定大小分成为**固定大小的页**，物理内存分成$\color{red}{同样大小的页框(块)}$。

在分配时，可以离散地存在内存中。

![](https://images.gitee.com/uploads/images/2020/0609/182312_5e27422a_5550632.png )

分配方式自然就是以页为单位，向页框里装填，由于二者大小相同，**所以，只有那个进程最后的页框会浪费空间，而其他的是被完全利用的**。

既然，分页式存储是按页分的，那么怎么确定页呢，怎么通过逻辑地址找物理地址就成了主要矛盾。

![](https://images.gitee.com/uploads/images/2020/0609/182722_2089bd71_5550632.png)

操作系统会建立一种叫做页表的数据结构，用来查询对应页号的物理地址。那么页号怎么求？物理地址怎么求？

假设，每个页的大小为L，某进程逻辑地址为A，那么页号P为：
$$
P = int(\frac{A}{L})
$$
光有页号不行，这样我们只知道物理块中的首地址信息，我们还需要一个量才能完全访问这个页号所对应的物理地址，它就是**页内偏移量 d**：
$$
d = int(A)\,mod\,L
$$
所以最后的物理地址为：
$$
物理地址=物理页框号 + 页内偏移量
$$
![](https://images.gitee.com/uploads/images/2020/0609/185456_fd075fd8_5550632.png)

我们要先算出逻辑地址对应的页号和页内偏移量,然后参照页表自然就可以算出物理地址了：

![](https://images.gitee.com/uploads/images/2020/0609/190407_527dcc5d_5550632.png )

![](https://images.gitee.com/uploads/images/2020/0609/190508_05ec1042_5550632.png )

![](https://images.gitee.com/uploads/images/2020/0609/183606_f2ddb390_5550632.png)

问题1：

主存 64KB = 64*1024 B = 65536 B

分成16块，页框和页的大小相同，也就是说，每页大小为 65536/16 = 4096 B = 4 KB

作业占4块，作业总长度为 4*4096 = **16384 B** = **16 KB**

问题2：

由于题目直接告诉我们了实际的页块号，我们直接乘每页大小就可以直接得出首地址

![](https://images.gitee.com/uploads/images/2020/0609/184938_640e736f_5550632.png )

问题3：

[0,100]的内存地址为：

**4 KB * 2 + 100 B = 8292 B**

[1，50]的内存地址为：

**4KB \* 4 + 50 = 16KB + 50 = 16434 B**

[2，0]的内存地址为：

**4KB \* 1 + 0 = 4KB + 0 = 4096 B**

[3，60]的内存地址为：

**4KB \* 6 + 60 = 24KB + 60 = 24636 B**



为了提高查找效率，我们通过将一部分页表存入**高速缓冲存储器**（又称快表，或关联存储器TLB, Translation Lookaside Buffer)中，**快表**拥有并行查找的能力，它可以同时查找页号与其对应的块号。

![](https://images.gitee.com/uploads/images/2020/0610/135435_f1f7e54a_5550632.png )

**分页式存储管理的优点：**

1. 没有外碎片，每个内碎片不超过页的大小。

2. 一个程序不必连续存放。

**缺点：**程序必须全部装入内存，没有足够内存就不能执行程序。



##### 段式存储管理

将内存看成一张二维表，如下组织**段表**：

![](https://images.gitee.com/uploads/images/2020/0610/135824_08854390_5550632.png)

通过程序的逻辑将一个进程分成若干段，每一段都有一个段首地址和它所需的内存大小(段长度)

![](https://images.gitee.com/uploads/images/2020/0610/140051_bad29bc3_5550632.png )

分段式存储管理的寻址过程如下：

![](https://images.gitee.com/uploads/images/2020/0610/140133_36684a85_5550632.png)

在段式存储管理中，可能会出现两次**越界中断**错误：

1. 在寻找段号的时候 (段号越界)
2. 在加段位移量时 (位移量出界)

**段式存储管理优点**：

1. **没有内碎片**

2. **外碎片可以通过内存紧缩消除。**

缺点：进程必须全部装入内存

##### 段页式存储管理

将上述两种技术混合杂交，就产生了段页式存储管理。我们不难看出，**分段对用户友好**，**分页对系统友好**，那么干脆我们两种方法一起用，**将程序按逻辑分段，然后操作系统将每一段分页**，页是物理单位，段是逻辑单位。用分段方法来分配和管理用户地址空间，用分页方法来管理物理存储空间。

![](https://images.gitee.com/uploads/images/2020/0610/141010_55b54159_5550632.png)

#### 6.3.4 虚拟存储器技术

之前的分配方式都必须将作业的所有部分调入内存中才能运行，而这节内容介绍的是虚拟存储技术，它通过将部分内存调入外存再调出的方式，从逻辑上提高了内存空间。

##### 局部性原理

指在一较短的时间内程序的执行**仅局限于某个部分**程序所访问的存储空间也局限于某个特定的区域。

大白话就是：只需要一部分数据程序就可以执行，不需要同时在内存中拥有所有的数据。

可以在**时间**和**空间**两个方面体现出来：

1. 在同一时间，一个CPU指令或数据都集中在较短时间执行或访问。
2. 当前指令和邻近的几条指令，当前访问的数据和邻近的数据都集中在一个较小区域内。

##### 虚拟存储器原理

操作系统只调用当前程序需要的数据**调入**内存，将不需要的数据调出至外存，当内存已满时，可以通过某种算法将已分配的空间**置换**到外存。

所以虚拟存储器有如下3大特性：

1. 多次性：指一个作业被分成多次调入内存运行。
2. 对换性：指允许在作业的运行过程中进行换进、换出，换进和换出能有效地提高内存利用率。
3. 虚拟性：指逻辑上进行内存扩容，实际上并没有物理地改变内存大小。

##### 虚拟存储器的实现方式

虚拟存储器的实现建立在**离散分配**的存储器管理方式的基础上。

###### **请求分页存储管理方式**（详细介绍）

我们采用离散分配中的，分页式存储管理模式来构造虚拟存储器。

首先，既然我们要自己安排一个虚拟存储器，首先要明确我们要干什么。

1. 如何判断一个页面不在内存中
2. 如果页面缺失，如何调入内存中
3. 如果没有空闲物理块(页框)时，如何淘汰已经存在的页面

首先回答问题一，在页表中添加状态为1为在主存，0为不在。

当所需页面缺失时，我们有两种调入策略：

1. 请求调页：顾名思义，你请求什么页面我就给你调什么页面，存在的问题就是一个刚刚被调出的页面可能很快就要再被调入，相当于同一个页面在内外存之间”反复横跳“，形成”抖动“，IO开销大，降低系统效率。
2. 预调页：事先把所需页面相邻的几个同时调入内存，这个可能造成物理块的浪费。

既然我们是基于分页式存储管理的虚拟存储器，那么自然不可能把一个作业的所有页面全部装入物理块，那对于一个作业而言这物理块应该怎么分呢？而且当已经分配的物理块占满了，我又该如何”置换“页面呢？

在请求分页系统中，可采取**两种内存分配**策略，即:**固定分配**和**可变分配**策略。

在进行**置换时**，也可采取两种策略，即: **全局置换**和**局部置换**策略。

全局就是在**整个内存中的页面**中选择置换，局部是在**当前进程所分配的页面**中进行置换。

它们排列组合一下有4种分配置换方式，可用的有三种：

  **1)** **固定分配局部置换**

   **(Fixed Allocation, Local Replacement)** 

  **2)** **可变分配全局置换**

   **(Variable Allocation, Global Replacement)** 

  **3)** **可变分配局部置换**

   **(Variable Allocation, Local** **Replacemen**

这些很好理解，待我举个例子。

假设作业A可以被分成7个页面，如果采用方式 1 的 **固定分配局部置换**，可能会出现下述现象：

对于作业A，操作系统就给他分配**4个物理块**，当4个物理块都满了，OS会**在给作业A分配的4个物理块中选择一个置换掉**。

如果采用方式 2 的**可变分配全局置换**，可能会出现下述现象：

对于作业A，操作系统依旧给他分配**4个物理块**，当4个物理块都满了，OS会**在给所有作业分配的n个物理块中选择一个置换掉**。如果频繁地**缺页**，操作系统无奈只能在给它多开几个物理块，假设增加到**6个物理块**。

如果采用方式 3 的**可变分配局部置换**，可能会出现下述现象：

对于作业A，操作系统还是只分配给他**4个物理块**，当4个物理块都满了，OS会**在给作业A分配的4个物理块中选择一个置换掉**。同样，如果频繁地**缺页**，操作系统无奈只能在给它多开几个物理块，假设增加到**6个物理块**

总结：

可变不可变指的是：物理块的数量是否可以增加。

局部和全局指的是：置换页面时，是在自己的那些个物理块里置换还是所有物理块里面置换。

现在开始回答第三个问题：如果没有空闲物理块(页框)时，如何淘汰已经存在的页面 ？

**页面置换算法**

１) **最佳算法（OPT，Optimal ）※**

２) **最近最久未使用算法（LRU，Least Recently Used）※**

３)**先进先出算法（FIFO）※**

４)**时钟算法（ Clock ）**

５) **最不常用算法（ LFU，Least Frequently Used ）**

６) **页面缓冲算法（ Page Buffering ）**     



######  **最佳算法（OPT，Optimal ）※**

它只是一种理论算法，是不可能被实现的，充其量是一种思想，思路如下：

选择置换**“未来不再使用的”**或**“在离当前最远位置上出现的”**页面。

 **采用最佳置换算法，通常可保证获得最低的缺页率。** 

但这是一种“预测未来的”算法，我们只能把它当成一种指标来看。

我们还是要拿例子看一看的：

假定系统为某进程分配了**三个物理块**，一个作业要依次访问如下页面：**7，0，1，2，0，3，0，4，2，3，0，3，2，1，2，0，1，7，0，1。**

请采用OPT页面置换算法，求出在访问过程中发生缺页中断的次数及缺页率（缺页次数／总访问次数 * 100% ）。

计算方法很简单，先画张表：

先把要访问的页号抄下来，缺页画×。由于OS只给它分配了三个物理块，我们用三行来表示。初始化之后来到了**第四列页号为2的地方**，此时物理块已经装满，需要置换页面，我们依照OPT算法思想，发现**3个物理块中，页号为7的页面会最晚被访问，所以把它置换掉**。其它同理。

![](https://images.gitee.com/uploads/images/2020/0616/162527_8e8b634a_5550632.png )

缺页中断次数（数叉）：9次

缺页率 = 9/20*100% = 45%



###### **先进先出算法（FIFO）※**

顾名思义，它和队列很像，核心思路就是**哪个页面最早进入的物理块，哪个页面就最先被置换掉**。我们再举一个例子：

假定系统为某进程分配了三个物理块,一个作业要依次访问如下页面：**０，１，２，３，0，１，４，０，１，２，３，４。**

请采用FIFO页面置换算法，求出在访问过程中发生缺页中断的次数及缺页率。（缺页率＝缺页次数／总访问次数 * 100%） 

这次我们采用FIFO算法思想来解释。我们依旧先把要访问的页号抄下来画图，缺页的画×。由于OS只给它分配了三个物理块，我们用三行来表示，$\color{red}{注意，此时画在最下面的一个物理块充当队头}$。**看第四列页号3的位置**，由于是先进先出，**之前页号0在队头，现在页号3的页面要进入物理块，此时0就会被新来的3置换掉**。

![](https://images.gitee.com/uploads/images/2020/0616/163658_029b6b3e_5550632.png )

缺页次数：9

缺页率：9/12*100% = 75%

顺带一提，FIFO算法可能会有一个Belady异常，即**分配给一个作业的物理块越多反而缺页率越高**。下图是在原来的基础上多给它分配一个物理块。

![](https://images.gitee.com/uploads/images/2020/0616/164143_3bf7a772_5550632.png )

缺页次数：10次

缺页率：10/12*100% = 83.33%



###### **最近最久未使用算法（LRU，Least Recently Used）※**

核心思路：**选择置换内存中最久未使用的页面**。性能接近最佳算法。思路听着很玄幻，其实就是栈的思想。

$\color{red}{哪个页面被访问了，哪个页面就放到栈顶，当物理块满时，置换掉栈底页面即可}$

我们依旧是举例说明：

假定系统为某进程分配了三个物理块，一作业要依次访问如下页面：**7，0，1，2，0，3，0，4，2，3，0，3，2，1，0，1，7，0，1。**请采用LRU页面置换算法，求出在访问过程中发生缺页中断的次数及缺页率（缺页次数／总访问次数 * 100% ）

老规矩画表，注意：$\color{red}{此时从上到下数的第一个物理块为栈顶，最后一个物理块为栈底}$。看**第四列，页号为2的物理块状态，在此之前，物理块为1，0 ，7号页面的数据，由于7号页面在栈底，长时间没有被使用，所以优先置换页号为7的页面**。

![](https://images.gitee.com/uploads/images/2020/0616/170222_8938995b_5550632.png)



缺页次数：12次

缺页率：12/19*100% = 63.16%

###### **请求分段存储管理方式** （暂不介绍）





## Part 7 I/O设备管理

大家平时使用计算机，从来没有仔细研究过鼠标，键盘，音箱，打印机是如何和计算机进行交互的吧，从这一章开始，我们就要探寻I/O设备和电脑之间的关系了。

### 7.1 I/O系统的功能

I/O系统的功能，站在用户的视角上看确实简单，无外乎就是点点鼠标，按下键盘。但是，为了全面地认识I/O系统，我将从**用户角度**，**CPU角度**，**设备控制器角度(下图中间的那个框就统称为设备控制器)**这三方面阐述I/O系统的基本功能。

![](https://images.gitee.com/uploads/images/2020/0627/112003_e49a8a10_5550632.png)



#### 7.1.1 用户角度

##### 0x01 隐藏物理设备的细节

以磁盘读写操作为例，想读写一个机械硬盘，理论上必须提供，数据，读写命令，磁盘的盘面号，磁道号，扇区号等。这些信息让人去往里写实在是过于困难。所以我们为了隐藏掉这些细节信息，向上层抽象成了简单的读/写命令，比如read，write等。

##### 0x02 设备无关性

在0x01的基础之上，当电脑进行热插拔硬件设备时，操作系统不需要重新编译，而是可以通过**驱动程序**，自动安装I/O设备，做到即插即用。



#### 7.1.2 CPU角度

##### 0x03 提高CPU和I/O设备的利用率

I/O设备可以并行操作，所以为了提高利用率，CPU和I/O设备之间也应当可以并行操作。为此，I/O系统需要快速相应用户的请求，并减少每个I/O设备运行时对CPU的干预时间。

##### 0x05 对I/O设备进行控制

对I/O设备进行控制是驱动程序的功能，大致可采用如下4种方式，详细的内容后面会讲解。

(1). 使用轮询的可编程I/O方式

(2). 使用中断的可编程I/O方式

(3). 直接存储器访问方式（DMA方式）

(4). 通道控制方式 



#### 7.1.3 设备控制器

##### 0x05 确保对设备正确的共享

我们可以按设备的共享属性分类,这种分类方式可将I/O设备分为如下三类：
(1) 独占设备 : 进程应**互斥**地访问这些设备
(2) 共享设备 ：同一时间可以允许多个进程同时访问

##### 0x06 错误处理

电气设备非常容易出现错误，但是，并不是什么错误都要直接往上报。以磁盘读写为例，可能对于一个区域写一次写不进行，操作系统会多次尝试，如果一直写不进去无法自动纠错了，这时再上报。



### 7.2 I/O设备的类型 



#### 7.2.1 按传输速率分类
按传输速度的高低，可将I/O设备分为三类。
第一类是**低速设备**,这是指其传输速率仅为每秒钟几个字节至数百个字节的一类设备。属于低速设备的典型设备有键盘、 鼠标器、语音的输入和输出等设备。
第二类是**中速设备**，这是指其传输速率在每秒钟数千个字节至数万个字节的一类设备。典型的中速设备有行式打印机、激光打印机等。
第三类是**高速设备**，这是指其传输速率在数百千个字节至数十兆字节的一类设备。典型的高速设备有磁带机、 磁盘机、 光盘机等。



#### 7.2.2 按信息交换的单位分类 

第一类是**块设备(Block Device)**，这类设备用于存储信息。 由于信息的存取总是以数据块为单位， 故而得名。 它属于有结构设备。典型的块设备是磁盘，每个盘块的大小为512 B~4 KB。磁盘设备的基本特征是其传输速率较高，通常每秒钟为几兆位。另一特征是可寻址，即对它可随机地读/写任一块。此外，磁盘设备的I/O常采用DMA方式。

第二类是**字符设备(Character Device)**，用于数据的输入和输出。 其基本单位是字符， 故称为字符设备。如打印机、交互式终端等。



#### 7.2.3 按设备的共享属性分类
这种分类方式可将I/O设备分为如下三类：
(1) 独占设备
(2) 共享设备
(3) 虚拟设备



### 7.3 设备控制器

在了解I/O系统基本功能，各类硬件的分类之后。从这节开始，我们就要讲述I/O系统如何控制设备了。

#### 7.3.1 设备控制器的定义

它是计算机中的一个实体，其主要职责是控制一个或多个I/O设备，以实现I/O设备和计算机之间的数据交换。它是CPU和I/O设备之间的**接口**，它接受从CPU发来的命令，并去控制I/O设备工作，从而使处理机从繁杂的设备控制事务中解脱出来。 

说人话就是：设备控制器是CPU和I/O设备之间的**接口**，它接受从CPU发来的命令，并去控制I/O设备工作，解放CPU.

![](https://images.gitee.com/uploads/images/2020/0627/113216_0d74502a_5550632.png)



#### 7.3.2 设备控制器的功能

1) 接收和识别命令 
2) 数据交换 
3) 标识和报告设备的状态 
4) 地址识别 
5) 数据缓冲 
6) 差错控制 



#### 7.3.3 I/O通道(I/O Channel)设备的引入

虽然在CPU与I/O设备之间增加了设备控制器后，已能大大减少了CPU对I/O的干预，但当主机所配置的外设很多时，CPU的负担仍然很重。

引入I/O通道的目的：使一些原来由CPU处理的I/O任务转由通道来承担，从而把CPU从繁杂的I/O任务中解脱出来。

实际上，I/O通道是一种**特殊的CPU**。它具有执行I/O指令的能力，并通过执行通道(I/O)程序来控制I/O操作。但I/O通道又与一般的处理机不同，主要表现在以下两个方面： 
 一是其指令**类型单一**，这是由于通道硬件比较简单， 其所能执行的命令，主要局限于与I/O操作有关的指令。

二是**通道没有自己的内存**，通道所执行的通道程序是放在主机的内存中的，换言之，是通道与CPU共享内存。 

##### 字节多路通道(Byte Multiplexor Channel)

![](https://images.gitee.com/uploads/images/2020/0627/113746_da23cede_5550632.png )



##### 数组选择通道(Block Selector Channel) 

由于字节多路通道不适于连接高速设备（它得分类），这推动了按数组方式进行数据传送的**数组选择通道**的形成。

**数组选择通道**虽然可以连接多台高速设备，但由于它只含有一个分配型子通道，在一段时间内只能执行一道通道程序， 控制一台设备进行数据传送， 致使当某台设备占用了该通道后，便一直由它独占， 即使是它无数据传送，通道被闲置， 也不允许其它设备使用该通道， 直至该设备传送完毕释放该通道。可见，这种通道的利用率很低。 



##### 数组多路通道(Block Multiplexor Channel)

数组选择通道虽有很高的传输速率，但它却每次只允许一个设备传输数据。

**数组多路通道**是将数组选择通道传输速率高和字节多路通道能使各子通道(设备)分时并行操作的优点相结合而形成的一种新通道。**它含有多个非分配型子通道**，因而这种通道既具有很高的数据传输速率，又能获得令人满意的通道利用率。也正因此，才使该通道能被广泛地用于连接多台高、中速的外围设备。 

![](https://images.gitee.com/uploads/images/2020/0627/114422_91008053_5550632.png )



### 7.4 I/O控制方式

##### 使用轮询的可编程I/O方式

  程序I/O控制方式是指由**用户进程直接控制内存或CPU和外围设备**之间进行信息传送的方式。它也被称为“忙-等”模式或者循环测试方式 。

  **这种方式的控制者是用户进程。**

程序I/O方式工作原理：

 (1)通过CPU发出启动输入设备输入数据命令
 (2)通过查询状态寄存器中的忙/闲标志，CPU
    不断地检查数据是否已经输入完毕
 (3)若数据输入完毕之后从缓冲将数据读到内 
    存中

写成伪代码就是：

```python
if CPU starts I/O:
	while True:
        flag = check_status_rigester()
        if flag == True:
            flush_in_memory()
            break
```

优点：

    CPU和外设的操作能通过状态信息得到同步，而且硬件结构比较简单。

缺点： 

    CPU利用效率较低，传输完全在CPU控制下完成，对外部出现的异常事件无实时响应能力。

适用于：

    CPU执行速度较慢，且外围设备较少的系统。如单片机系统 。


##### 使用中断的可编程I/O方式

**中断**是指计算机在执行期间，系统内发生任何非寻常的或非预期的急需处理事件，使得CPU暂时中断当前正在执行的程序而转去执行相应的事件处理程序。

工作原理：中断源需要CPU为它服务时，向CPU发出请求，当CPU执行完当前命令，当前允许中断的情况下响应请求。具体过程包括：

（1）CPU发出命令启动外设，阻塞当前进程；
（2）外设准备好数据；
（3）向CPU发出中断请求；
（4）CPU接受中断请求；
（5）响应中断，执行中断服务程序。

写成伪代码就如同：

```python
if "I want to I/O":
	block_progress(PCB)
    hardware_product_data()
    wake_up_CPU()
    accept_request()
    run_progress()
```

优点：

```
提高了CPU的效率，等待外设将数据准备好之后，中断CPU，请求CPU服务。
具有实时响应能力，外部中断源始终处于主动地位，可以随时得到响应。
及时处理异常情况，提高计算机可靠性。
```

缺点：

```
每处理一次I/O数据就会占用CPU的时间
```



##### 直接存储器访问方式（DMA方式）

DMA是**直接内存访问(Direct Memory Access)**的缩写，它是一种**完全由硬件执行I/O数据交换**的工作方式。
原理：DMA控制器从CPU完全接管对总线的控制，数据交换不经过CPU，而直接在内存和I/O设备之间进行。采用DMA方式工作时，由DMA控制器向内存发出地址和控制信号，进行地址修改，对传送字的个数计数，并且以中断方式向CPU报告传送操作的结束。

① 数据传输的基本单位是数据块，即在CPU与 I/O设备之间，每次传送至少一个数据块
② 所传送的数据是从设备直接送入内存的，或者相反
③ 仅在传送一个或多个数据块的开始和结束时，才需CPU干预，整块数据的传送是在控制器的控制下完成的。

可见，DMA方式较之中断驱动方式，又是成百倍地**减少了CPU对I/O的干预，进一步提高了CPU与I/O设备的并行操作程度。 **

说人话就是：DMA减轻了CPU的负担，它通过和总线直接相连，当进行I/O时除了内存，地址修改能操作要向CPU报备之外，其它的一律由DMA承担。

**一般来讲一个DMA只负责一个设备，下图是对原来DMA的改进**

![](https://images.gitee.com/uploads/images/2020/0628/184303_0e2306c9_5550632.png)

![](https://images.gitee.com/uploads/images/2020/0628/184436_e6c9e026_5550632.png )

##### 通道控制方式 

I/O通道方式是DMA方式的发展，当外部设备过多时，一个DMA可能不够用，而且中间的内存寻址也是一个巨大的开销。要是氪金多加一个没准效率不会提高反而下降。所以就有了通道方式，它可以进一步减少对CPU的干预，即把对**一个数据块**的读(或写)为单位的干预，减少为对**一组数据块**的读(或写)及有关的控制和管理为单位的干预。同时，又可实现CPU、通道和I/O设备三者的并行操作，从而更有效地提高整个系统的资源利用率。

通道是**一种有特殊功能的处理器**，是通过执行通道程序，并与设备控制器共同实现对I/O设备的控制的。
通道程序是由**一系列通道指令（或称为通道命令）**所构成的。这些通道指令受CPU启动，并在操作结束时向CPU发中断信号。

![](https://img-blog.csdn.net/20171130091852029)



#### 7.5 磁盘 (机械硬盘)

磁盘存储器不仅容量大，存取速度快，而且可以实现随机存取，是当前存放大量程序和数据的理想设备，故在现代计算机系统中，都配置了磁盘存储器，并以它为主来存放文件。这样，对文件的操作，都将涉及到对**磁盘的访问**。本节重点介绍，磁盘读取的3种算法。



##### 7.5.1 机械硬盘的结构

宏观结构图：

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy95VnlMbXRpYkNYT0tGMkU5R1lpYWljdWlhSUZDYVBhVHBJeVlvMnRzY3hrMzB2MzgyVFRWRHFUN1d6b0p3NmpsNnpEQkVwbUhpY3ZnQTM2cDk0a2VtN3R0S0p3LzY0MA?x-oss-process=image/format,png)

盘片图：

![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1594527951556&di=9914ca59fd322cb85fb37ff1db28e0df&imgtype=0&src=http%3A%2F%2Fphotocdn.sohu.com%2F20140121%2FImg393885460.jpg)

介绍几个重点概念：

**磁道**：磁盘在格式化时被划分成许多同心圆，这些同心圆轨迹叫做磁道（Track）。磁道从外向内从0开始顺序编号。硬盘的每一个盘面有300～1 024个磁道，新式大容量硬盘每面的磁道数更多。信息以脉冲串的形式记录在这些轨迹中，这些同心圆不是连续记录数据，而是被划分成一段段的圆弧，这些圆弧的角速度一样。由于径向长度不一样，所以，线速度也不一样，外圈的线速度较内圈的线速度大，即同样的转速下，外圈在同样时间段里，划过的圆弧长度要比内圈划过的圆弧长度大。

**扇区**：操作系统以扇区（Sector）形式将信息存储在硬盘上，每个扇区包括512个字节的数据和一些其他信息。一个扇区有两个主要部分：存储数据地点的标识符和存储数据的数据段

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy95VnlMbXRpYkNYT0tGMkU5R1lpYWljdWlhSUZDYVBhVHBJeVlmdHdVOXB0a25pYnVXcklIdFZNaWE4aWNxSEN6ZmV4QlhPa0t5VTBEaWFlOE8wdWQwbDRzaDJONlBRLzY0MA?x-oss-process=image/format,png)



##### 7.5.2 磁盘的调度算法

**3种算法的初始磁道号均为100**

###### 0x00 先来先服务算法

核心思路：不用考虑磁头移动的距离，来一个访问需求就去找

进程提出磁盘I/O请求的顺序：55,58,39,18,90,160,150,38,184

![](https://images.gitee.com/uploads/images/2020/0712/095447_df1486b6_5550632.png)

正确答案：

![](https://images.gitee.com/uploads/images/2020/0712/094319_87704ab5_5550632.png)



###### 0x01 最短寻道时间优先 **SSTF(Shortest Seek Time First)** 

算法思路，谁离当前磁头近就去谁那

进程提出磁盘I/O请求的顺序：55,58,39,18,90,160,150,38,184

![](https://images.gitee.com/uploads/images/2020/0712/100458_26ed0e70_5550632.png)

正确答案:

![](https://images.gitee.com/uploads/images/2020/0712/095811_1af1e5d6_5550632.png)

###### 0x02 扫描算法 （SCAN）

核心思路：在SSTF的基础之上(距离最短)，添加了一个方向，即从初始磁道开始沿着给定的方向去找最近的磁道，一个方向上的都找完以后再向相反方向寻找。

假设该磁盘共有200个磁道，初始方向：**沿着磁道号增大的方向**。

进程提出磁盘I/O请求的顺序：55,58,39,18,90,160,150,38,184

![](https://images.gitee.com/uploads/images/2020/0712/102047_f375ccf6_5550632.png)

正确答案：

![](https://images.gitee.com/uploads/images/2020/0712/101133_f3e8740d_5550632.png)



##### 7.5.3 例题

假设磁盘有200个磁道，磁盘请求队列中是一些随机请求，他们按照到达的次序分别处于98、183、37、122、14、124、65、67号磁道上，当前磁头在53号磁道上，并向磁道号减小的方向上移动。请给出按FCFS、SSTF、SCAN算法进行磁盘调度时满足请求的次序，并计算出他们的平均寻道长度。

参考答案:

![](https://images.gitee.com/uploads/images/2020/0712/101101_39447da5_5550632.png)