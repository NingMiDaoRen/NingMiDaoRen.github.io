---
title: 机器学习-提高版
declare: true
toc: true
date: 2021-07-19 10:21:02
tags: [机器学习]
categories: [机器学习]
---



<meta name="referrer" content="no-referrer" />  

<!--more-->



# 机器学习-提高版

[TOC]

## Part 1 开篇

为进一步开展机器学习相关领域科研工作，需要更深入更底层地了解机器学习模型，框架，思想，特此书写提高版博客一篇。在此感谢浙江大学胡老师的网络公开课。

## Part 2 没有免费午餐定理 （No Free Lunch Theorem）

如果我们不对特征空间有**先验假设**，则所有算法的平均表现是一样的。

我们认为：特征差距下的样本更有可能是一类。

## Part 3 支持向量机 (Support Vector Machine)

### 3.1 线性可分与不可分问题

<img src="https://images.gitee.com/uploads/images/2021/0719/105842_bf9f5726_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:80%;" />

但凡是用一条**直线**就能将样本分成两类的样本集，称之为**线性可分训练样本集 （Linear Separable）**。如果无法使用一条直线将样本分开，那我们称之为**非线性可分训练样本集**，如下图所示：

![输入图片说明](https://images.gitee.com/uploads/images/2021/0719/110413_77afb32e_5550632.png "屏幕截图.png")

那么，如果数据集线性可分，将会有无数条直线存在，哪条直线是最好的呢？

![输入图片说明](https://images.gitee.com/uploads/images/2021/0721/092041_1ad846f4_5550632.png "屏幕截图.png")

那么首先需要针对“最好”这一情况假设相关的性能指标，然后说明某条直线可以达到最优的性能指标，这样就能确定出那条直线。

#### 0x01 指标定义

从某条直线开始，向其两侧**平行移动出**两条直线，和样本集相交时停止，平移两直线（下图中虚线）间间隔最大时，且原直线到两平行直线间距离相等时，我们称找到了一个分类边界。

![输入图片说明](https://images.gitee.com/uploads/images/2021/0721/092326_3d449902_5550632.png "屏幕截图.png")

所以，支持向量机的模型度量指标为，**间隔d **。**d 越大，模型性能越好。**将其变成数学语言，可描述为：
$$
数据集(X_i,y_i)_{(i=1 \to n)},y_i = +1,-1
$$

$$
线性模型:\omega^TX+b \\
y_i=+1, \omega^TX+b≥0\\
y_i= -1，\omega^TX+b<0
$$


$$
\forall (X_i,y_i) \ y_i=+1, \omega^TX+b≥0\\
y_i= -1，\omega^TX+b<0\\ 
即，y_i[\omega^TX+b]≥1
$$

其中，线性模型又被称为**超平面(Hyperplane)**.那么怎么找模型参数ω和b呢？

### 3.2 优化问题

![输入图片说明](https://images.gitee.com/uploads/images/2021/0721/121548_d4bd5d59_5550632.png "屏幕截图.png")



### 3.3
