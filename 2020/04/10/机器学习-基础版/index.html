<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://ningmidaoren.github.io">
  <title>机器学习-基础版 | 甯宓的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="article">
<meta property="og:title" content="机器学习-基础版">
<meta property="og:url" content="https://ningmidaoren.github.io/2020/04/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E7%89%88/index.html">
<meta property="og:site_name" content="甯宓的博客">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/181016_87637a55_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/181116_5b7c5762_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/181142_79f0966c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/181936_e80890f9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/211434_2c945803_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0410/211634_8934882d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/104115_8c256e08_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/104255_3adfe843_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/105303_53c034fe_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/105416_bc3aa140_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/111704_cb4d6e42_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0411/112049_e046b14b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/173247_a4edf54d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/191019_85f9a9fd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/173308_373271f7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/174153_a5c42b7d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/192116_c8df6788_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/192726_9df1890f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/192904_2a8fde0c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/194518_d3ab9478_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/194913_8bfe06ea_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/195124_f8eb9385_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/200308_8e0969e7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/200436_383b6034_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0412/200645_a9fb9662_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/092557_1977b276_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/092954_1eab65a5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/093354_986eeacc_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/093511_4540ed0a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/093724_255fa791_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/093958_d6660863_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0418/095424_506023b9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0501/121249_3050903e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/103820_428bfb81_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/104619_64ca995f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/114419_25dce2fb_5550632.jpeg">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/114604_b5ba2575_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/113817_d54c38b2_5550632.gif">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0505/150018_b874198b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0508/192434_2b3e87e7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0510/100856_5031c2c2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0510/103938_24e97ca1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0510/110024_d9d39569_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/111407_b08b3f6f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/111816_196c60d7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/112948_2d830849_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/113549_6e81bd72_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/113807_2fbfce19_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0516/114543_f7bac632_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0517/182641_c964752f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0517/183208_5642237f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0517/183334_1e96ea2a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0123/113908_c4026501_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0123/114347_94a711ef_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0123/114921_79266689_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0520/202201_ac4ea9e7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0520/203159_3e1c86d8_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0520/203517_2846935b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/193842_0cb13551_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/194124_2fdeb6fa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/194442_74e76c21_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/194700_a25d3b5f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/195022_4fa7353c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/195243_bdc69b08_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/195849_92fc2b29_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/195926_fb81e54f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/200202_26de6ab7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/200714_8dbda46a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/200752_4a56ba8d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0523/201043_627c28ba_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/161748_6a980835_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/162104_546ca44b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/162111_a993cef8_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/162118_bfbfcd98_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/163434_741ba5f2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/163813_822d997d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/164110_1ebcd21d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/164800_9cd10783_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/165102_c71c05a2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/165432_a0c5962c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0524/165611_59874ae4_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/202956_85b345b3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/203656_70436049_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/204551_4eb151dc_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/205058_4151c1a1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/205419_32d18fa7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0525/205609_7552332d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/202054_50285d83_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/202259_83b73d2f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/202745_44c28e81_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/203730_230aeb89_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/204214_c56a669a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/205159_4d61dd50_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/205644_3b350c6b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/210654_60dc1132_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/211208_5d28207f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/104619_64ca995f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0503/114604_b5ba2575_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/215232_f2a990c9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0528/221023_5a780998_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0531/194005_e45563bf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0531/194329_b1a5a254_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0531/194412_9658376c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0531/194721_444d5ad6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0604/192615_e1b69cf0_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/174042_80c4ab72_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/180821_2ad451fe_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/183832_d9fe636d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/181212_7e574104_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/183253_606eff6f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/184033_b7099781_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/185952_6f5ef4d1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0605/190711_29986289_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0606/111059_66ef2190_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0606/111237_2713c6f2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0606/205910_e27d4578_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0606/205941_f1d957aa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0611/184507_80ece889_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0611/190546_68646bab_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0611/190708_66fbbd4a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0611/191013_7be127aa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0611/191221_794cc28a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0612/194523_ecf1e02b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0612/195149_adf51773_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0612/195717_b114d489_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0612/200001_33be60d2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0613/195812_5b0347c6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0613/204738_bc778e83_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0613/205101_874f23b1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0613/205523_3315ed35_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0617/205419_89407319_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0617/205644_0e6805c4_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0617/210015_72996c2e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0617/210632_73355f8c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0606/111059_66ef2190_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0619/184328_907290ae_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0619/184858_e5dd9465_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0619/184956_0fd99a49_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0619/185416_a254dffd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0724/160141_e15ceb7a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0724/160407_f4d0bedd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/111659_d12cff34_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/112410_374589d4_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/113301_7b77bbe3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/113652_3ba93e1f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/114130_41d3f5fd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/114203_ed8dbdc6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0725/114237_cff5cccf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/103340_5721abc5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/103747_08fcca36_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/105015_5152e362_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/105129_c00cb1f7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/111952_e8b05490_5550632.png">
<meta property="og:image" content="c:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200731115700887.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/114323_8dae70c5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/120352_01633a67_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0731/121145_6845c449_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0805/143329_dc614068_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0805/145912_4532e951_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0805/150153_f66c7674_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0805/150456_5d5c3ebc_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0805/150900_fe1ae2b9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/150350_c9fcd06b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/150824_62a67bea_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/150550_ca96f98e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/151444_1aba2222_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/151821_e47a3ed6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/152041_6fea7511_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/152438_3376c567_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/152725_9b82655a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/152929_cea72ca6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/153123_bf7a7763_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/154540_b4af671e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/154927_e7020f4e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/155011_1b346198_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/155554_99f22659_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0923/155912_7a748835_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0929/094130_24a25676_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0929/100620_e35d3265_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/0929/101026_875cc28c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1029/192044_736c4a32_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1029/192522_86c642b6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1029/192911_30d6594c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1105/193658_df3785ec_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1105/195632_8b13ece3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1105/200216_aab79a1d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1105/201756_f7b01eac_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1105/201902_af4d2a8a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/144608_d28be25c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/145714_bc96224b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/145857_34c34a14_5550632.png">
<meta property="og:image" content="c:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201130152931236.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/153526_5c4a6c7a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/173336_6da22807_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/174945_08d481b1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/175734_008ec2e6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/181648_eacc35bf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/183132_1e224cfc_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/183526_0b9575b8_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1130/184216_dc1468b6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/132645_6ee08e5e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/133019_1fba0da8_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/133242_f9da934a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/133537_ded7cb9b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/133737_a411e9a5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/135423_d959e62b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/135804_a344ce8a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1201/140513_3ee999e1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1202/185041_fa58f10a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1202/185620_6d555777_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1202/185928_433dcc25_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1202/190654_7c83f6b2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1203/181130_4745e453_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1203/182215_5f2676ee_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1203/182952_d893c33c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/161936_637454fd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/132133_0977ea56_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/132918_b7d9d2d7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/134642_9b2b2bb9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/134826_f8062df6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/135254_a86a6e66_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/141911_2de6b837_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1204/152038_b70c15e7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1207/175342_9bc70054_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/195909_48fe752a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/195940_b70d294a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/202518_087ebad1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/202857_56ed962d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203040_3d098fbd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203204_832b4290_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203354_26b2ce74_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203711_7d0729fd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203955_43428945_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/205340_d4db8738_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/205828_1964c0aa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/205956_4d06fb09_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/155344_60de69f6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/160219_fb9ec68a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/161411_71d87bf1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/161717_9186cf65_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/162413_71405c0b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/143604_f71db46b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1208/203711_7d0729fd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/145927_d7b77f5d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/150819_2406eb79_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/151634_453dda9f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/152323_a4339890_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/180852_ad135a7f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/181428_ce81599d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/183434_c6b67682_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/184158_2d33d131_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1214/184712_97caff6b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/150853_d48404b9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0125/172540_e92682f9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0125/173945_4dc708c3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0125/203715_55b1cc1a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0125/205321_28963647_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0125/205445_c4b9944a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/160802_9e52c3a2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/161014_4bab46ab_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/161325_b459805b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/161417_0f2035cf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/161505_1d03dbcd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0126/161701_b72b02b5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/143618_59057d44_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/143730_ea09ef8b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/144332_daf77f64_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/144549_7d78a8b3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/145840_1a8ecca9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/150109_dc833963_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0127/150626_a590fced_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/145509_b786998e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/145918_bb0e1790_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/150023_2ca76dd4_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/151355_f303935f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/151429_c434d55c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0128/151649_462050c4_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0129/154351_100a4d24_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0129/154620_1988d197_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0129/155347_60c9b240_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/170841_091cc1ae_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/171436_5a8b4b07_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/172722_612743cb_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/173501_20b41804_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/175128_134f7222_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/180224_e7b70f95_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0204/180907_72f5a9e1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0208/151507_5457b0c9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0208/153246_80235eb5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0208/154231_7e69d0f9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0209/153401_73d368bf_5550632.png">
<meta property="og:image" content="https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3530606917,2044115981&fm=26&gp=0.jpg">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0209/155004_5442bdca_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0210/172823_ff193627_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0210/174155_97150835_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0215/163647_32a416bd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0215/164053_32253790_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1207/175342_9bc70054_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0216/192658_9f78c5fa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0216/193139_ceab531e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/134056_1deaf769_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/134158_7602ff36_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/135117_42e46d1f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/135449_8ad400c9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/135624_c67127df_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/135652_b44a0edc_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/140059_9e7da43b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/141608_adb5f271_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0217/141823_9d9d6f12_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0219/144817_48ea3368_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0219/150853_7000cd21_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0219/160638_21d48851_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0219/160916_d17b4903_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0219/161224_9c8c464e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0222/135316_10abee74_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/152152_001175db_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/152344_3da79f23_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/155251_fa97b307_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/155513_7cd0e234_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0225/160905_12d15b53_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0227/155209_0d338c60_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/151448_57775065_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0227/164448_df3cf800_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0227/165041_7cea3966_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/152650_6962bd24_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/152904_9fa4c2ff_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/153211_115a7ddb_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/153529_b426f907_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/153902_8f6bf457_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/154607_efb7fce5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/154639_30538903_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0301/144409_e741abcf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0302/152947_eaa307f3_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0302/161653_79d023d7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0302/161849_28fcb274_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0302/162017_6166d5cb_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/152637_9b1f050a_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/152842_85ada914_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/153241_c444d52d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/153430_ec488dea_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/153803_e0880845_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/154808_f9c14b2f_5550632.gif">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/155048_0291d57b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/155325_2963da1e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/155546_1a5dd879_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/155909_109e266e_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/160156_aedb1aa7_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/160322_0332a8d5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/161145_fd9bda32_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/161348_ebd70649_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/161627_5e7a0d4d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/163301_6dfcffe1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/163958_0938760b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/164836_96046c9d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/165159_009ae726_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0303/170408_2fb9acad_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1216/152718_d976c487_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1216/152751_cb2a27a9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1216/154920_6dbcd9cf_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1216/173525_dc98af68_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1217/145642_5c59c1c1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1218/110057_0f8ce02d_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1218/105850_f0038aa5_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1219/172625_cb1541c9_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1222/132416_3d3f5fbe_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2020/1222/132738_3ed9f71f_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0315/114316_04fdcca2_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0315/120831_d84f5bfa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0316/220725_2e684151_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/0323/212933_076ee2fa_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1011/151247_8b6a0f37_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1012/203043_f8f811a1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1012/203650_d85b94d6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1013/162112_2631c49f_5550632.png">
<meta property="og:image" content="c:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20211013162136914.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1013/185159_60ea31c6_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1013/185210_e2c58d8c_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/123353_88e54fcd_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/123609_39fab514_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/123821_68026c41_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/123851_20b4d431_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/124434_10c8bff1_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1213/125240_0171229b_5550632.png">
<meta property="og:image" content="https://images.gitee.com/uploads/images/2021/1215/083731_38de89d3_5550632.png">
<meta property="article:published_time" content="2020-04-10T05:37:29.000Z">
<meta property="article:modified_time" content="2021-12-15T01:09:16.000Z">
<meta property="article:author" content="甯宓">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.gitee.com/uploads/images/2020/0410/181016_87637a55_5550632.png">
  
    <link rel="alternative" href="/atom.xml" title="甯宓的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/me.png" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">甯宓</a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories">分类</a></li>
	        
				<li><a href="/archives">归档</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">一只来自软工的小渣渣</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/NingMiDaoRen" title="github"><i class="icon-github"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/me.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">甯宓</h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/NingMiDaoRen" title="github"><i class="icon-github"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 33.333333333333336%"><a href="/">主页</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/categories">分类</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/archives">归档</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-机器学习-基础版" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习-基础版
    </h1>
  

        
        <a href="/2020/04/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E7%89%88/" class="archive-article-date">
  	<time datetime="2020-04-10T05:37:29.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-04-10</time>
</a>

        <!-- 开始添加字数统计-->
        
          <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">50,887字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长≈ </span>
        <span class="post-count">205分</span>
      </span>
    </span>
</div>

          
        <!-- 添加完成 -->

        

        
          <span class="archive-article-date">
              阅读量 <span id="busuanzi_value_page_pv"></span>
          </span>
        

      </header>
    

    <!-- 目录内容 -->
    
        <p class="show-toc-btn" id="show-toc-btn" onclick="showToc();" style="display:none">
              <span class="btn-bg"></span>
              <span class="btn-text">文章导航</span>
              </p>
        <div id="toc-article" class="toc-article">
            <span id="toc-close" class="toc-close" title="隐藏导航" onclick="showBtn();">×</span>
            <strong class="toc-title">文章目录</strong>
               <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习-基础版"><span class="toc-number">1.</span> <span class="toc-text">机器学习-基础版</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-1-序"><span class="toc-number">1.1.</span> <span class="toc-text">Part 1 序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-本人学习机器学习的初衷"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 本人学习机器学习的初衷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-机器学习的应用场景"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 机器学习的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-什么是机器学习"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 什么是机器学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-2-机器学习的分类"><span class="toc-number">1.2.</span> <span class="toc-text">Part 2 机器学习的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-监督学习-Supervised-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 监督学习(Supervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-无监督学习-Unsupervised-Learning"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 无监督学习(Unsupervised Learning)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-线性回归-Linear-Regression-机器学习的第一步"><span class="toc-number">1.3.</span> <span class="toc-text">Part 3 线性回归(Linear Regression)-机器学习的第一步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-什么是线性回归"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 什么是线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-如何实现线性回归模型"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 如何实现线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-代价函数-Cost-Function"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 代价函数(Cost Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-代价函数和预测函数的关系"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 代价函数和预测函数的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-等高线图-Contour-plot"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3.2.3 等高线图(Contour plot)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-梯度下降-Gradient-decent-algorithm"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">3.2.4 梯度下降(Gradient decent algorithm)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-5-梯度下降总结"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">3.2.5 梯度下降总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-线性回归模型算法实现"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 线性回归模型算法实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-4-线性代数-优化梯度下降算法"><span class="toc-number">1.4.</span> <span class="toc-text">Part 4 线性代数-优化梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-矩阵与向量"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 矩阵与向量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-矩阵-Matrix"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 矩阵 (Matrix)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-向量-Vector"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 向量 (Vector)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-矩阵运算"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 矩阵运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-加法和标量乘法"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 加法和标量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-矩阵向量乘法"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 矩阵向量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-矩阵间乘法"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 矩阵间乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-4-矩阵乘法性质"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">4.2.4 矩阵乘法性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-5-逆和转置"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">4.3.5 逆和转置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-5-多元线性回归"><span class="toc-number">1.5.</span> <span class="toc-text">Part 5 多元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-多元梯度下降法"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 多元梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-多元梯度下降-特征缩放"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 多元梯度下降-特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-均值归一化"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">5.2.1 均值归一化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-多元梯度下降-学习率"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 多元梯度下降-学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-特征和多项式回归"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 特征和多项式回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-正规方程-Normal-equation"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.5 正规方程 (Normal equation)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-1-矩阵不可逆时的正规方程方法"><span class="toc-number">1.5.5.1.</span> <span class="toc-text">5.5.1 矩阵不可逆时的正规方程方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-题外话-线性回归模型的数学原理"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.6 题外话-线性回归模型的数学原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-6-Octave编程"><span class="toc-number">1.6.</span> <span class="toc-text">Part 6 Octave编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-基本数学运算"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 基本数学运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-逻辑运算"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 逻辑运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-声明矩阵和向量"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 声明矩阵和向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-从文件中读写矩阵"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 从文件中读写矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-数据计算"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 数据计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-1-矩阵相关计算"><span class="toc-number">1.6.5.1.</span> <span class="toc-text">6.5.1 矩阵相关计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-2-常用函数"><span class="toc-number">1.6.5.2.</span> <span class="toc-text">6.5.2 常用函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-数据可视化"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 数据可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-控制语句-if-for-while"><span class="toc-number">1.6.7.</span> <span class="toc-text">6.7 控制语句-if-for-while</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-8-定义函数"><span class="toc-number">1.6.8.</span> <span class="toc-text">6.8 定义函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-9-向量化"><span class="toc-number">1.6.9.</span> <span class="toc-text">6.9 向量化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-7-Logistic-回归算法"><span class="toc-number">1.7.</span> <span class="toc-text">Part 7 Logistic 回归算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-分类问题的引出"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 分类问题的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-二元分类问题的假设函数"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 二元分类问题的假设函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-1-Sigmoid-function-Logistic-function"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">7.2.1 Sigmoid function &#x2F; Logistic function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-2-分类问题的代价函数"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">7.2.2 分类问题的代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-3-简化分类问题的代价函数"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">7.2.3 简化分类问题的代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-4-Logistic-Regression-Gradient-Descent"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">7.2.4 Logistic Regression Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-5-高级优化-知道个名称就行了"><span class="toc-number">1.7.2.5.</span> <span class="toc-text">7.2.5 高级优化-知道个名称就行了</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-多元分类问题"><span class="toc-number">1.7.3.</span> <span class="toc-text">7.3 多元分类问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-8-过拟合-Overfitting"><span class="toc-number">1.8.</span> <span class="toc-text">Part 8 过拟合 (Overfitting)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-什么是过拟合"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 什么是过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-解决过拟合的途径"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 解决过拟合的途径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-正则化-Regularization"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 正则化 (Regularization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-线性回归的正则化"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.4  线性回归的正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-Logistic-回归的正则化"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 Logistic 回归的正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-9-机器学习实战-房价预测"><span class="toc-number">1.9.</span> <span class="toc-text">Part 9 机器学习实战-房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-数据集来源以及数据集说明"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 数据集来源以及数据集说明</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-数据集的介绍"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">0x00 数据集的介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-数据集的类别和规模"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">0x01 数据集的类别和规模</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-实验设计"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 实验设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-概述"><span class="toc-number">1.9.2.1.</span> <span class="toc-text">0x00 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-源代码"><span class="toc-number">1.9.2.2.</span> <span class="toc-text">0x01 源代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x02-实验结果及可视化展示"><span class="toc-number">1.9.2.3.</span> <span class="toc-text">0x02 实验结果及可视化展示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-踩到的坑"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 踩到的坑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-多元梯度下降的向量化"><span class="toc-number">1.9.3.1.</span> <span class="toc-text">0x00 多元梯度下降的向量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-均值归一化"><span class="toc-number">1.9.3.2.</span> <span class="toc-text">0x01 均值归一化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-10-神经网络-新的征程"><span class="toc-number">1.10.</span> <span class="toc-text">Part 10 神经网络-新的征程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-神经网络的引出"><span class="toc-number">1.10.1.</span> <span class="toc-text">10.1 神经网络的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-神经元与大脑"><span class="toc-number">1.10.2.</span> <span class="toc-text">10.2 神经元与大脑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-神经网络结构"><span class="toc-number">1.10.3.</span> <span class="toc-text">10.3 神经网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-神经网络向量化"><span class="toc-number">1.10.4.</span> <span class="toc-text">10.4 神经网络向量化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#通过例子进行深入理解"><span class="toc-number">1.10.4.0.1.</span> <span class="toc-text">通过例子进行深入理解</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#引入"><span class="toc-number">1.10.4.0.1.1.</span> <span class="toc-text">引入</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-多元分类问题"><span class="toc-number">1.10.5.</span> <span class="toc-text">10.5 多元分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-1-问题概述"><span class="toc-number">1.10.5.1.</span> <span class="toc-text">10.5.1 问题概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-2-代价函数"><span class="toc-number">1.10.5.2.</span> <span class="toc-text">10.5.2 代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-3-反向传播算法"><span class="toc-number">1.10.5.3.</span> <span class="toc-text">10.5.3 反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#算法描述"><span class="toc-number">1.10.5.3.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#深入理解反向传播算法"><span class="toc-number">1.10.5.3.2.</span> <span class="toc-text">深入理解反向传播算法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-4-梯度检验-Gradient-Checking"><span class="toc-number">1.10.5.4.</span> <span class="toc-text">10.5.4 梯度检验 (Gradient Checking)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-5-随机初始化"><span class="toc-number">1.10.5.5.</span> <span class="toc-text">10.5.5 随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-6-组合到一起"><span class="toc-number">1.10.5.6.</span> <span class="toc-text">10.5.6 组合到一起</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#直观理解梯度下降在神经网络中的作用"><span class="toc-number">1.10.5.6.1.</span> <span class="toc-text">直观理解梯度下降在神经网络中的作用</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-11-机器学习诊断法-Machine-Learning-diagnostic"><span class="toc-number">1.11.</span> <span class="toc-text">Part 11 机器学习诊断法 (Machine Learning diagnostic)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-评估假设函数"><span class="toc-number">1.11.1.</span> <span class="toc-text">11.1 评估假设函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-模型选择问题"><span class="toc-number">1.11.2.</span> <span class="toc-text">11.2 模型选择问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-诊断偏差与方差"><span class="toc-number">1.11.3.</span> <span class="toc-text">11.3 诊断偏差与方差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-1-偏差与方差"><span class="toc-number">1.11.3.1.</span> <span class="toc-text">11.3.1 偏差与方差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-2-正则化和偏差与方差的关系"><span class="toc-number">1.11.3.2.</span> <span class="toc-text">11.3.2 正则化和偏差与方差的关系</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-学习曲线-Learning-curves"><span class="toc-number">1.11.4.</span> <span class="toc-text">11.4 学习曲线 (Learning curves)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-12-机器学习系统设计"><span class="toc-number">1.12.</span> <span class="toc-text">Part 12 机器学习系统设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-情景引入"><span class="toc-number">1.12.1.</span> <span class="toc-text">12.1 情景引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-误差分析"><span class="toc-number">1.12.2.</span> <span class="toc-text">12.2 误差分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-1-偏斜类问题-Skewed-classes"><span class="toc-number">1.12.2.1.</span> <span class="toc-text">12.2.1 偏斜类问题(Skewed classes)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#查准率-（Precision）"><span class="toc-number">1.12.2.1.1.</span> <span class="toc-text">查准率 （Precision）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#召回率-（Recall）"><span class="toc-number">1.12.2.1.2.</span> <span class="toc-text">召回率 （Recall）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-2-查准率和召回率的权衡"><span class="toc-number">1.12.2.2.</span> <span class="toc-text">12.2.2 查准率和召回率的权衡</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-数据集的规模与机器学习算法"><span class="toc-number">1.12.3.</span> <span class="toc-text">12.3 数据集的规模与机器学习算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-13-支持向量机-Support-Vector-Machine"><span class="toc-number">1.13.</span> <span class="toc-text">Part 13 支持向量机 (Support Vector Machine)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-支持向量机的引出"><span class="toc-number">1.13.1.</span> <span class="toc-text">13.1 支持向量机的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-对SVM的直观理解"><span class="toc-number">1.13.2.</span> <span class="toc-text">13.2 对SVM的直观理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-大间隔分类器的数学原理-初步"><span class="toc-number">1.13.3.</span> <span class="toc-text">13.3 大间隔分类器的数学原理-初步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-4-核函数"><span class="toc-number">1.13.4.</span> <span class="toc-text">13.4 核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-5-核函数与SVM"><span class="toc-number">1.13.5.</span> <span class="toc-text">13.5 核函数与SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-6-SVM的使用"><span class="toc-number">1.13.6.</span> <span class="toc-text">13.6 SVM的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-14-无监督学习-Unsupervised-Learning"><span class="toc-number">1.14.</span> <span class="toc-text">Part 14 无监督学习 (Unsupervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#14-1-启航"><span class="toc-number">1.14.1.</span> <span class="toc-text">14.1 启航</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-2-K-means算法"><span class="toc-number">1.14.2.</span> <span class="toc-text">14.2 K-means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-1-算法流程概述"><span class="toc-number">1.14.2.1.</span> <span class="toc-text">14.2.1 算法流程概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-2-K-means-的代价函数（优化目标）"><span class="toc-number">1.14.2.2.</span> <span class="toc-text">14.2.2 K-means 的代价函数（优化目标）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-3-簇的随机初始化"><span class="toc-number">1.14.2.3.</span> <span class="toc-text">14.2.3 簇的随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-4-分簇的数量"><span class="toc-number">1.14.2.4.</span> <span class="toc-text">14.2.4 分簇的数量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-3-第二类无监督学习-降维-Dimensionality-Reduction"><span class="toc-number">1.14.3.</span> <span class="toc-text">14.3 第二类无监督学习-降维(Dimensionality Reduction)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-3-1-降维的应用-数据可视化"><span class="toc-number">1.14.3.1.</span> <span class="toc-text">14.3.1 降维的应用-数据可视化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-3-2-PCA-Principal-Component-Analysis"><span class="toc-number">1.14.3.2.</span> <span class="toc-text">14.3.2 PCA (Principal Component Analysis)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#PCA的概述"><span class="toc-number">1.14.3.2.1.</span> <span class="toc-text">PCA的概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#如何选择主要成分数量"><span class="toc-number">1.14.3.2.2.</span> <span class="toc-text">如何选择主要成分数量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#压缩重现-（Reconstruction）"><span class="toc-number">1.14.3.2.3.</span> <span class="toc-text">压缩重现 （Reconstruction）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#PCA应用时的小建议"><span class="toc-number">1.14.3.2.4.</span> <span class="toc-text">PCA应用时的小建议</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-4-异常检测-Anomaly-Detection"><span class="toc-number">1.14.4.</span> <span class="toc-text">14.4 异常检测 (Anomaly Detection )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-1-问题背景"><span class="toc-number">1.14.4.1.</span> <span class="toc-text">14.4.1 问题背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-2-高斯分布（Gaussian-distribution）"><span class="toc-number">1.14.4.2.</span> <span class="toc-text">14.4.2 高斯分布（Gaussian distribution）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#参数估计"><span class="toc-number">1.14.4.2.1.</span> <span class="toc-text">参数估计</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-3-异常检测算法"><span class="toc-number">1.14.4.3.</span> <span class="toc-text">14.4.3 异常检测算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-4-使用异常检测解决实际问题"><span class="toc-number">1.14.4.4.</span> <span class="toc-text">14.4.4 使用异常检测解决实际问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-5-异常检测与监督学习的比较"><span class="toc-number">1.14.4.5.</span> <span class="toc-text">14.4.5 异常检测与监督学习的比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-6-异常检测的特征选择"><span class="toc-number">1.14.4.6.</span> <span class="toc-text">14.4.6 异常检测的特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-7-多元高斯分布"><span class="toc-number">1.14.4.7.</span> <span class="toc-text">14.4.7 多元高斯分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-8-使用多元高斯分布的异常检测"><span class="toc-number">1.14.4.8.</span> <span class="toc-text">14.4.8 使用多元高斯分布的异常检测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-15-推荐系统-Recommender-System"><span class="toc-number">1.15.</span> <span class="toc-text">Part 15  推荐系统 (Recommender System)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#15-1-问题场景"><span class="toc-number">1.15.1.</span> <span class="toc-text">15 .1 问题场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-2-基于内容的推荐算法"><span class="toc-number">1.15.2.</span> <span class="toc-text">15.2 基于内容的推荐算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-3-协同过滤-Collaborative-Filtering"><span class="toc-number">1.15.3.</span> <span class="toc-text">15.3 协同过滤 (Collaborative Filtering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-4-组合到一起"><span class="toc-number">1.15.4.</span> <span class="toc-text">15.4 组合到一起</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-5-低秩矩阵分解-Low-rank-matrix-factorization"><span class="toc-number">1.15.5.</span> <span class="toc-text">15.5 低秩矩阵分解 (Low rank matrix factorization)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-16-学习大规模数据集"><span class="toc-number">1.16.</span> <span class="toc-text">Part 16 学习大规模数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#16-1-随机梯度下降-Stochastic-gradient-decent"><span class="toc-number">1.16.1.</span> <span class="toc-text">16.1 随机梯度下降(Stochastic gradient decent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-2-Mini-Batch-gradient-decent"><span class="toc-number">1.16.2.</span> <span class="toc-text">16.2 Mini-Batch gradient decent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-3-在线学习（Online-Learning）"><span class="toc-number">1.16.3.</span> <span class="toc-text">16.3 在线学习（Online Learning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-4-减少映射-（Map-reduce）与数据并行（Data-parallel）"><span class="toc-number">1.16.4.</span> <span class="toc-text">16.4 减少映射 （Map-reduce）与数据并行（Data parallel）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-17-机器学习实例：Photo-OCR"><span class="toc-number">1.17.</span> <span class="toc-text">Part 17 机器学习实例：Photo OCR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#17-1-Machine-Learning-Pipeline"><span class="toc-number">1.17.1.</span> <span class="toc-text">17.1 Machine Learning Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#0x00-滑动窗口"><span class="toc-number">1.17.1.0.1.</span> <span class="toc-text">0x00 滑动窗口</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-2-数据获取和人工数据合成"><span class="toc-number">1.17.2.</span> <span class="toc-text">17.2 数据获取和人工数据合成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-3-上限分析（Ceiling-Analysis）"><span class="toc-number">1.17.3.</span> <span class="toc-text">17.3 上限分析（Ceiling Analysis）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-18-总结与感谢"><span class="toc-number">1.18.</span> <span class="toc-text">Part 18 总结与感谢</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Appendix-作业及解析"><span class="toc-number">1.19.</span> <span class="toc-text">Appendix 作业及解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-1-Linear-Regression"><span class="toc-number">1.19.1.</span> <span class="toc-text">Programming Exercise 1 Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-2-Logistics-Regression"><span class="toc-number">1.19.2.</span> <span class="toc-text">Programming Exercise 2 Logistics Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-带有的正则化逻辑回归"><span class="toc-number">1.19.2.1.</span> <span class="toc-text">2.2 带有的正则化逻辑回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-3-Multi-class-Classification-and-Neural-Networks"><span class="toc-number">1.19.3.</span> <span class="toc-text">Programming Exercise 3 Multi-class Classification and Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-4-Neural-Networks-Learning"><span class="toc-number">1.19.4.</span> <span class="toc-text">Programming Exercise 4: Neural Networks Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-5-Regularized-Linear-Regression-and-Bias-v-s"><span class="toc-number">1.19.5.</span> <span class="toc-text">Programming Exercise 5: Regularized Linear Regression and Bias v.s.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-6-Support-Vector-Machines"><span class="toc-number">1.19.6.</span> <span class="toc-text">Programming Exercise 6: Support Vector Machines</span></a></li></ol></li></ol></li></ol>
             </div>
       <script type="text/javascript">
        function showToc(){
            var toc_article = document.getElementById("toc-article");
            var show_toc_btn = document.getElementById("show-toc-btn");
            toc_article.setAttribute("style","display:block");
            show_toc_btn.setAttribute("style","display:none");
            };
        function showBtn(){
            var toc_article = document.getElementById("toc-article");
            var show_toc_btn = document.getElementById("show-toc-btn");
            toc_article.setAttribute("style","display:none");
            show_toc_btn.setAttribute("style","display:block");
            };
       </script>
          
    <!-- 目录内容结束 -->


    <div class="article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />  

<a id="more"></a>

<h1 id="机器学习-基础版"><a href="#机器学习-基础版" class="headerlink" title="机器学习-基础版"></a>机器学习-基础版</h1><p>[TOC]</p>
<h2 id="Part-1-序"><a href="#Part-1-序" class="headerlink" title="Part 1 序"></a>Part 1 序</h2><h3 id="1-1-本人学习机器学习的初衷"><a href="#1-1-本人学习机器学习的初衷" class="headerlink" title="1.1 本人学习机器学习的初衷"></a>1.1 本人学习机器学习的初衷</h3><p>我学机器学习的初衷是因为手头有个推荐系统的项目，刚开始我觉得推荐算法是数据挖掘的内容，然而数据挖掘挖着挖着我就感觉有些不太对劲。数据挖掘它是一门综合性极强的学科，综合了统计学，机器学习等等学科。为了学好数据挖掘，同时我又对机器学习充满好奇，这就导致我无论如何都要学习且学好这门课程。</p>
<p>在此，感谢吴恩达老师的机器学习教程！！！</p>
<h3 id="1-2-机器学习的应用场景"><a href="#1-2-机器学习的应用场景" class="headerlink" title="1.2 机器学习的应用场景"></a>1.2 机器学习的应用场景</h3><blockquote>
<p>Google的网页搜索</p>
</blockquote>
<blockquote>
<p>垃圾邮件的筛除</p>
</blockquote>
<blockquote>
<p><strong>Data Mining</strong></p>
</blockquote>
<blockquote>
<p>Biology</p>
</blockquote>
<p>…</p>
<p>当一个机械造物，它自己会行走，飞行的时候，作为它的创造者，你是否能感受到人工智能的魅力？这门课程就是让你学会如何教育<strong>愚笨的电脑</strong>学会学习，并模拟人脑。</p>
<h3 id="1-3-什么是机器学习"><a href="#1-3-什么是机器学习" class="headerlink" title="1.3 什么是机器学习"></a>1.3 什么是机器学习</h3><p><strong>Definition 1：</strong> Field of study that gives computers the ability to learn without explicitly programmed.</p>
<p>关键点：在没有明确地编程的条件下，让电脑学会学习。</p>
<p>下面的定义是一个家喻户晓的定义，推荐理解原句</p>
<p><strong>Definition 2:</strong>  A computer program is said to <em>learn</em> from experience E with respect to some task T and some performance measure P, if its performance on T , as measured by P,improves with experience E.</p>
<p>对于一个邮件过滤系统，E代表：观察你所标记的垃圾邮件，T代表：分类垃圾邮件，P代表：分类的准确率 </p>
<h2 id="Part-2-机器学习的分类"><a href="#Part-2-机器学习的分类" class="headerlink" title="Part 2 机器学习的分类"></a>Part 2 机器学习的分类</h2><h3 id="2-1-监督学习-Supervised-Learning"><a href="#2-1-监督学习-Supervised-Learning" class="headerlink" title="2.1 监督学习(Supervised Learning)"></a>2.1 监督学习(Supervised Learning)</h3><p>我有一个朋友想卖掉他的房子，他不知道市场价格是多少，想让电脑帮他预测一下值</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/181016_87637a55_5550632.png" alt=""></p>
<p>绿色的字代表了我朋友房子的面积，那么通过机器学习，就可能给出如下两种结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/181116_5b7c5762_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/181142_79f0966c_5550632.png" alt=""></p>
<p>第一张图预测结果是145万元，第二张图预测结果是115万元。由此可知，由于我们用于<strong>拟合(fit)</strong>的函数不同导致了最后结果的不同。这个例子是监督学习的经典例子。</p>
<p>我们通过事先给算法一个<strong>包含正确答案数据集</strong>，而目的是给出更多正确的答案。像是这种预测类的问题，我们把它们称作<strong>回归问题(Regression)</strong>。</p>
<p>下面还有一个恶性肿瘤预测的例子：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/181936_e80890f9_5550632.png" alt=""></p>
<p>蓝色代表良性肿瘤，红色代表恶性肿瘤(Malignant),横坐标是肿瘤大小。我们现在的任务是对粉色箭头所指的进行<strong>分类(Classification)</strong>.分类问题中，可能分类的结果不止两个【手写数字识别】。</p>
<p>在这个例子中，只有肿瘤大小这一个<strong>属性(特征)</strong>，但是在其他的机器学习中可能存在很多个特征，例如肿瘤细胞的形状，肿瘤的厚度，…</p>
<p>有时我们要处理的特征可能很多，甚至用到无穷多个特征，这就需要<strong>支持向量机(Support Vector Machine)</strong>这一算法了。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>监督学习(Supervised Learning)，给算法一个正确结果的数据集。目的是解决下列问题</p>
<p>解决的问题：</p>
<blockquote>
<p>回归问题：预测一个<strong>连续值</strong>的输出</p>
<p>分类问题：预测<strong>离散值</strong>的输出</p>
</blockquote>
<h3 id="2-2-无监督学习-Unsupervised-Learning"><a href="#2-2-无监督学习-Unsupervised-Learning" class="headerlink" title="2.2 无监督学习(Unsupervised Learning)"></a>2.2 无监督学习(Unsupervised Learning)</h3><p>如果说监督学习的最大特点是，在训练模型的时侯，<strong>明显地标记出每个训练数据的“正确答案”</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/211434_2c945803_5550632.png" alt=""></p>
<p>那无监督学习最大的特点是：<strong>训练模型时，不标注每条数据的含义，让电脑自己分簇(Cluster)</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0410/211634_8934882d_5550632.png" alt=""></p>
<p>无监督学习通过<strong>聚类算法，鸡尾酒会算法(Cocktail Party Algorithm)等等</strong>对数据集进行分簇，进而找到数据的类型结构。</p>
<br>

<h2 id="Part-3-线性回归-Linear-Regression-机器学习的第一步"><a href="#Part-3-线性回归-Linear-Regression-机器学习的第一步" class="headerlink" title="Part 3 线性回归(Linear Regression)-机器学习的第一步"></a>Part 3 线性回归(Linear Regression)-机器学习的第一步</h2><h3 id="3-1-什么是线性回归"><a href="#3-1-什么是线性回归" class="headerlink" title="3.1 什么是线性回归"></a>3.1 什么是线性回归</h3><p>在上面我们讲述的房价预测就是监督学习中的回归问题，我们给定一个训练集，这个训练集里面包括了，<strong>(房子面积，出售价格)</strong>，然后我们需要在连续的面积([0,n])中任意取一个面积，来预测出售价格。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0411/104115_8c256e08_5550632.png" alt=""></p>
<p>对于训练集我们有着如下的规定：</p>
<p><strong>m</strong>：训练集中数据样例的个数</p>
<p><strong>x</strong>：特征 / 输入值</p>
<p><strong>y</strong>：目标值 / 输出值</p>
<p><strong>(x,y)</strong>： 一组训练样例<br>$$<br>(x^{i},y^{i})：第 i 个训练用例<br>$$</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0411/104255_3adfe843_5550632.png" alt=""></p>
<p>把这整个过程可以简化成：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0411/105303_53c034fe_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0411/105416_bc3aa140_5550632.png" alt=""></p>
<p>最终发现，重点在于找到其中的假设函数 <em>h</em> ，这个函数我们可以用一次函数来对训练集进行拟合，由于这个函数是线性的，问题又是回归问题，所以我们称这个模型为<strong>线性回归模型</strong>。</p>
<h3 id="3-2-如何实现线性回归模型"><a href="#3-2-如何实现线性回归模型" class="headerlink" title="3.2 如何实现线性回归模型"></a>3.2 如何实现线性回归模型</h3><h4 id="3-2-1-代价函数-Cost-Function"><a href="#3-2-1-代价函数-Cost-Function" class="headerlink" title="3.2.1 代价函数(Cost Function)"></a>3.2.1 代价函数(Cost Function)</h4><p><img src="https://images.gitee.com/uploads/images/2020/0411/111704_cb4d6e42_5550632.png" alt=""></p>
<p>找到了函数 <em>h</em> 我们也就能开始预测了。所谓找出函数 <em>h</em>，在本质上就是在找，<strong>模型参数</strong> θ0，θ1。不同的模型参数当然会导致函数h的不同。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0411/112049_e046b14b_5550632.png" alt=""></p>
<p>模型想要准确，就需要<br>$$<br>\frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^{(i)}) - y^{(i)} )^2<br>$$<br>尽量小<br>$$<br>h_\theta (x^i) 是第i个训练值的预测值，y^i是真实值<br>$$<br>整个式子描述了预测之和真实值之间的<strong>偏差程度</strong>，把它当作一个函数<br>$$<br>J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^{(i)}) - y^{(i)} )^2<br>$$<br>这个东西的数学定义叫做<strong>均方根误差</strong>，存在的意义在于表示<strong>预测值和真实值</strong>的偏差<strong>(而不是平均值和样本值的偏差)</strong></p>
<p><a href="https://baike.baidu.com/item/均方根误差/3498959?fr=aladdin" target="_blank" rel="noopener">均方根误差科普</a></p>
<p>我们现在要做的就是<br>$$<br>minimize( J(\theta_{0},\theta_{1}) )<br>$$<br>我们称<br>$$<br>J(\theta_{0},\theta_{1})<br>$$<br>为<strong>代价函数</strong> or <strong>均方根误差(MSE)函数</strong></p>
<p>当代价函数最小时，我们就找出了最合适的模型参数，进而确定了线性函数 h</p>
<h4 id="3-2-2-代价函数和预测函数的关系"><a href="#3-2-2-代价函数和预测函数的关系" class="headerlink" title="3.2.2 代价函数和预测函数的关系"></a>3.2.2 代价函数和预测函数的关系</h4><p>前面讲的太快，现在我们来慢慢分析，它们之间的关系。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/173247_a4edf54d_5550632.png" alt=""></p>
<p>PPT画图，不准请见谅。</p>
<p>数据集共有3条数据，其中它们的值我已可视化在图上。我们为了方便说明，把假设函数 <em>h</em> 设为：<br>$$<br>h_\theta(x) = \theta_1 x<br>$$<br>那么，它的代价函数就变成了：<br>$$<br>J(\theta_{1}) = \frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^i) - y^i )^2<br>$$<br><img src="https://images.gitee.com/uploads/images/2020/0412/191019_85f9a9fd_5550632.png" alt=""></p>
<blockquote>
<p>假设函数：h(x) 是关于 房子面积 x 的函数【其中带有参数θ】</p>
<p>代价函数：J(θ1) 关于模型参数 θ1 的函数</p>
</blockquote>
<p>当 θ1 = 1 时，我们预测函数的曲线长这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/173308_373271f7_5550632.png" alt=""></p>
<p>此时算一下代价函数<br>$$<br>J(\theta_{1}) = \frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^i) - y^i )^2<br>$$</p>
<p>$$<br>J(\theta_{1}) = \frac{1}{2m}\sum_{i = 0} ^m(\theta_1x^i - y^i )^2<br>$$</p>
<p>$$<br>J(\theta_{1}) = \frac{1}{2m}\sum_{i = 0} ^m(1*x^i - y^i )^2<br>$$</p>
<p>$$<br>J(\theta_{1}) = \frac{1}{2*3}[(0-0)+(0-0)+(0-0)] = 0<br>$$</p>
<p>画出代价函数 J(θ1)的图像：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/174153_a5c42b7d_5550632.png" alt=""></p>
<p>之后，我们设 θ1 为不同的值，-0.5，0.5，1.5，2.5 …. 画出它的预测函数图像和代价函数图像。</p>
<p>这是当θ取不同值时的预测函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/192116_c8df6788_5550632.png" alt=""></p>
<p>代价函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/192726_9df1890f_5550632.png" alt=""></p>
<p>代价函数最终效果图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/192904_2a8fde0c_5550632.png" alt=""></p>
<p>回想一下，我们的目的是什么：<br>$$<br>minimize(J(\theta_{1}))<br>$$<br>找到出 θ。此时你会发现，<strong>当代价函数取最小时，假设函数和测试集的拟合结果是最好的</strong>。【θ = 1,预测函数和训练用例完全重合】</p>
<h4 id="3-2-3-等高线图-Contour-plot"><a href="#3-2-3-等高线图-Contour-plot" class="headerlink" title="3.2.3 等高线图(Contour plot)"></a>3.2.3 等高线图(Contour plot)</h4><p>现在让我们回到原来的问题上</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/194518_d3ab9478_5550632.png" alt=""></p>
<p>当预测函数有两个模型参数时，也就意味着<strong>代价函数变成了关于 θ0 和 θ1 的二元函数</strong>，它长成这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/194913_8bfe06ea_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/195124_f8eb9385_5550632.png" alt=""></p>
<p>右侧，就是代价函数的等高线图，如何理解呢？很简单，想象用一个横截面把上面的三维碗状图水平切一刀，你会发现一个剖面，碗状图和横切面相交的地方，<strong>高度相同</strong>，但是<strong>对应着若干个不同的 θ0 和 θ1</strong>，把高度相同，但θ不同的点整理到一起，就有了右侧的等高线图。其中每一个椭圆都代表一个高度。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/200308_8e0969e7_5550632.png" alt=""></p>
<p>上图的红色标记处，代表了一组θ，对应着它的假设函数，如左面所示。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/200436_383b6034_5550632.png" alt=""></p>
<p>我们又找了一组，这个点相对靠近了最小值，感觉假设函数拟合的结果没有刚才的那么离谱。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0412/200645_a9fb9662_5550632.png" alt=""></p>
<p>这组，<strong>已经离代价函数的最小值很近了，但是不是它最小值</strong>，此时拟合的结果，比前两个都要好。</p>
<p><strong>如何自动且快速地找出代价函数最小值已经成为了一个至关重要的问题 ！</strong></p>
<h4 id="3-2-4-梯度下降-Gradient-decent-algorithm"><a href="#3-2-4-梯度下降-Gradient-decent-algorithm" class="headerlink" title="3.2.4 梯度下降(Gradient decent algorithm)"></a>3.2.4 梯度下降(Gradient decent algorithm)</h4><p>先把前面的过程梳理一下。</p>
<p>基于监督学习的过程，我们会产生一个含参数 θ 关于自变量 x 的假设函数<br>$$<br>h_\theta(x)<br>$$<br>为了想要预测结果相对准确，我们引入了代价函数，目的是确定最好的参数 θ ，进而确定假设函数。</p>
<p>而代价函数<br>$$<br>J(\theta) = \frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^i) - y^i )^2<br>$$<br>是一个关于参数 θ 的 n 元函数，只有参数 θ 未知，当代价函数取得最小值时，<strong>预测值和真实值之间的偏差最小，即假设函数拟合的结果就越好</strong>，如何求出 θ 就变成现在的主要矛盾了。</p>
<p>之前，介绍了等高线图，当图中的椭圆最小，接近于一个点时，代价函数是最小的，参数是最优的，假设函数是拟合程度最好的【不考虑过拟合】</p>
<p>下面，我们就看看，如何自动地快速地求出 n 元参数 θ。为了方便讨论，我们就先把代价函数看成只有2个自变量的函数，当然，它可以是一个 n 元函数。</p>
<p>现在我们有：<br>$$<br>J(\theta_{0},\theta_{1}) 或者是 J(\theta_{0},\theta_{1},…)<br>$$<br>欲求：<br>$$<br>min_{\theta_{0}{\theta_{1}}} =J(\theta_{0},\theta_{1})<br>$$<br>梯度下降的过程：</p>
<blockquote>
<ol>
<li>任意初始化这2个参数的值：θ0 = 0，θ1 = 0</li>
<li><strong>同时</strong>改变这两个参数的值，减小代价函数 J(θ0，θ1) 的值</li>
<li>一直取到最小</li>
</ol>
</blockquote>
<p>我们先直观的感受一下，梯度下降算法是怎么工作的。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/092557_1977b276_5550632.png" alt=""></p>
<p>这是两座山峰，啊，不对，是代价函数的图像，最下面是两个参数，高度是函数值。不过，我们先把它看成山峰，草堆，什么的，体验一下梯度下降算法是怎么玩的。</p>
<p>假设，你任取一个点，如下图加号所示位置。你站在这个点上：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/092954_1eab65a5_5550632.png" alt=""></p>
<p>现在，你想要快速下山，环顾周围 360° 你发现沿着这个方向走是最好的，于是你走了一步：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/093354_986eeacc_5550632.png" alt=""></p>
<p>站在这个点上，你像刚才一样，环顾四周，选择方向，再迈一步一步又一步：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/093511_4540ed0a_5550632.png" alt=""></p>
<p>恭喜你，此时走到了这条曲线的<strong>局部最低点</strong>，如果你的初始值换成另外一个值，可能结果就会变成这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/093724_255fa791_5550632.png" alt=""></p>
<p>这就是，梯度下降的直观感受，下面让我们直接进入正题。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/093958_d6660863_5550632.png" alt=""></p>
<p>梯度下降算法要做的，就是一直重复里面的式子直到收敛</p>
<p>莫慌，先解释一下，这堆参数都是啥。</p>
<blockquote>
<ol>
<li><p>:=   &gt;&gt;&gt; 它是赋值号，和MySQL一样，赋值号不用 = ， = 用于等值判断</p>
</li>
<li><p>θj  &gt;&gt;&gt; 它就是代价函数的那些参数，在这个例子中就是 θ0 和 θ1</p>
</li>
<li><p>α &gt;&gt;&gt; Learning Rate，学习率<strong>【α&gt;0】</strong>，它越大相当于你下山迈的步子越大，它小，相当于迈着小碎布下山。它决定了学习效率，关于它的制订规则以后再讲。</p>
</li>
<li><p>关于当前 θ 的偏导数项<br>$$<br>\frac{\partial J(\theta_{0},\theta_{1})}{\partial\theta_j} (j = 0,1)<br>$$</p>
</li>
</ol>
</blockquote>
<p>最后还有一个细节，我们要更新这两个 θ ，它们是<strong>同时更新</strong>的，而不是顺序更新的</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0418/095424_506023b9_5550632.png" alt=""></p>
<p>左侧是正确的梯度下降，右侧你会发现，顺序计算时，到 temp1 时，θ0 已经变了，函数也就变了，这就会造成一些奇奇怪怪的结果。</p>
<p>在下一节中，咱们来感受一下这个公式中各个部分的意义。</p>
<h4 id="3-2-5-梯度下降总结"><a href="#3-2-5-梯度下降总结" class="headerlink" title="3.2.5 梯度下降总结"></a>3.2.5 梯度下降总结</h4><p>为了方便描述，依旧假设代价函数是关于参数 θ 的一元函数。<br>$$<br>J(\theta_{1})<br>$$<br>我们按照梯度下降算法，我们要求的式子就变成了：<br>$$<br>\theta_{1} := \theta_{1} -\alpha \frac{d}{d_{\theta_{1}}}<br>$$<br>由于这个代价函数是一元函数，在数学定义中就是导数。</p>
<p>假设我们有如下这样一个代价函数图像，现在我们要找到函数的最小值时，参数 θ 的值。根据梯度下降算法，假设 θ 初值为 橙色点位置。众所周知，一元函数导数的几何意义表示为该点处切线斜率。下图绿色虚线的切线为正值， θ1 - 正值 * 正值，总会使 θ1 减小的，最终找到局部最小值点。同理如果 θ 的初始值在函数左侧，那么切线斜率为负值，相当于在<strong>原值绝对值</strong>基础之上减小，直至局部最小值点。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0501/121249_3050903e_5550632.png" alt=""></p>
<p>如果导数项(偏导数项)代表了下降的幅度【导数值越大，θ 减小或增加地越多】，那么<strong>学习率</strong>又有什么实质性的作用呢？</p>
<p>首先，<strong>学习率</strong>是一个正实数，无外乎就两种特殊的取值范围：[0,1],[1,正无穷]</p>
<blockquote>
<p>[0,1]：等于在导数项前乘了一个小数，使得算法收敛至局部最小时速度变慢</p>
<p>[1,正无穷]：等于在导数项前乘了一个扩大项，使得收敛速度变快，但是可能出现以下问题</p>
</blockquote>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/103820_428bfb81_5550632.png" alt=""></p>
<p>由于学习率过大，导致直接“超出”局部最小值，导致无法收敛。因为<strong>收敛条件就是导数为零的时候</strong>，<strong>只有在局部最小值【一个点】处的导数为0</strong></p>
<br>

<h3 id="3-3-线性回归模型算法实现"><a href="#3-3-线性回归模型算法实现" class="headerlink" title="3.3 线性回归模型算法实现"></a>3.3 线性回归模型算法实现</h3><p>我们之前讲了，两大块内容：</p>
<ol>
<li>梯度下降算法</li>
<li>线性回归模型</li>
</ol>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/104619_64ca995f_5550632.png" alt=""></p>
<p>把梯度下降运用到代价函数中：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/114419_25dce2fb_5550632.jpeg" alt=""></p>
<p>以上是我手推的过程，懒得看过程的，直接用结果也可以：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/114604_b5ba2575_5550632.png" alt=""></p>
<p>我们刚刚写出的梯度下降算法，还有一个别名，叫做<strong>“Batch”梯度下降算法</strong>，<strong>它每次计算都需要遍历整个训练集</strong>。采用此梯度下降算法的结果，如下动图所示，左侧为假设函数，右侧为代价函数，右侧红黑色动点，代表每一步的参数收敛过程。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/113817_d54c38b2_5550632.gif" alt=""></p>
<p>至此，恭喜你已经学会了第一个机器学习算法。由于采用迭代的方式进行参数更换过于耗时，我们需要更换参数迭代的方式，使用线性代数中的<strong>正规方程组方法</strong>优化算法效率。</p>
<br>

<h2 id="Part-4-线性代数-优化梯度下降算法"><a href="#Part-4-线性代数-优化梯度下降算法" class="headerlink" title="Part 4 线性代数-优化梯度下降算法"></a>Part 4 线性代数-优化梯度下降算法</h2><h3 id="4-1-矩阵与向量"><a href="#4-1-矩阵与向量" class="headerlink" title="4.1 矩阵与向量"></a>4.1 矩阵与向量</h3><h4 id="4-1-1-矩阵-Matrix"><a href="#4-1-1-矩阵-Matrix" class="headerlink" title="4.1.1 矩阵 (Matrix)"></a>4.1.1 矩阵 (Matrix)</h4><p>矩阵，学过线性代数的人可能都知道是啥。为了方便理解，我们统一把矩阵理解成：</p>
<p><strong>一个长方形的数组</strong>，矩阵名称通常用<strong>大写字母</strong>进行表示，小写字母通常用于表示矩阵内的元素或者是向量。<br>$$<br>A_{2×3} = \left[<br>\begin{array}{lcr}<br>a &amp; b &amp; c \<br>d &amp; e &amp; f<br>\end{array}<br>\right]<br>$$<br>这个矩阵是<strong>2行3列</strong>的，所以我们也管矩阵A叫做<strong>一个2×3的矩阵</strong>。矩阵中元素的访问如下：<br>$$<br>A_{11} = a,A_{12} = b<br>$$</p>
<h4 id="4-1-2-向量-Vector"><a href="#4-1-2-向量-Vector" class="headerlink" title="4.1.2 向量 (Vector)"></a>4.1.2 向量 (Vector)</h4><p>只有一行的矩阵，我们成为行向量，同理，只有一列的向量我们成为列向量。在机器学习中我们默认使用列向量。<br>$$<br>y = \left[<br>\begin{array}{lcr}<br>a \<br>b\<br>c\<br>d\<br>\end{array}<br>\right]<br>$$<br>向量是有维度的，向量内部有几个元素，它就是几维向量，比如上面这个就是一个<strong>4维(列)向量</strong>。</p>
<p>我们有两种方式访问向量，一种是从1开始访问，另一种是从0开始访问，就像不同编程语言规定的数组第一个元素的下标一样。<br>$$<br>y_0 = a,y_1 = b…<br>$$</p>
<p>$$<br>y_1 = a,y_2 = b…<br>$$</p>
<p>在线性代数中我们使用以1为起点的向量访问法</p>
<p>在机器学习中，为了方便运算我们采用以0起点的向量访问法</p>
<h3 id="4-2-矩阵运算"><a href="#4-2-矩阵运算" class="headerlink" title="4.2 矩阵运算"></a>4.2 矩阵运算</h3><h4 id="4-2-1-加法和标量乘法"><a href="#4-2-1-加法和标量乘法" class="headerlink" title="4.2.1 加法和标量乘法"></a>4.2.1 加法和标量乘法</h4><p><strong>同型(行数列数相同的)矩阵</strong>对应位置相加即可<br>$$<br>A_{2×3} = \left[<br>\begin{array}{lcr}<br>1 &amp; 2 &amp; 3 \<br>2 &amp; 3 &amp; 0.5<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>B_{2×3} = \left[<br>\begin{array}{lcr}<br>3 &amp; 2 &amp; 1 \<br>1 &amp; 4 &amp; 0.5<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>A_{2×3} + B_{2×3} =\left[<br>\begin{array}{lcr}<br>4 &amp; 4 &amp; 4 \<br>3 &amp; 7 &amp; 1<br>\end{array}<br>\right]<br>$$</p>
<p><strong>矩阵数乘</strong>：所有元素都成乘那个数即可<br>$$<br>3<em>A_{2×3} = 3</em>\left[<br>\begin{array}{lcr}<br>1 &amp; 2 &amp; 3 \<br>2 &amp; 3 &amp; 0.5<br>\end{array}<br>\right] =\left[<br>\begin{array}{lcr}<br>9 &amp; 6 &amp; 9 \<br>6 &amp; 9 &amp; 1.5<br>\end{array}<br>\right]<br>$$<br>减法和除法同理。</p>
<h4 id="4-2-2-矩阵向量乘法"><a href="#4-2-2-矩阵向量乘法" class="headerlink" title="4.2.2 矩阵向量乘法"></a>4.2.2 矩阵向量乘法</h4><p>在矩阵之间进行乘法之前，先看特例，<strong>矩阵乘向量</strong><br>$$<br>A_{3×2} = \left[<br>\begin{array}{lcr}<br>1 &amp; 3 \<br>4 &amp; 0 \<br>2 &amp; 1<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>y = \left[<br>\begin{array}{lcr}<br>1\<br>5\<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>z = A_{3×2}*y_2 = \left[<br>\begin{array}{lcr}<br>16\<br>4\<br>7<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>z_1 = A_{11} <em>y_1 + A_{12}</em>y_2\<br>z_2 = A_{21} <em>y_1 + A_{22}</em>y_2\<br>z_3 = A_{31} <em>y_1 + A_{32}</em>y_2<br>$$</p>
<p>通过观察得出，都是<strong>第x行 * 向量的加和</strong>,因为向量只有一列，所以就是<strong>在对应位置，矩阵第x行*向量第1列加和</strong></p>
<h4 id="4-2-3-矩阵间乘法"><a href="#4-2-3-矩阵间乘法" class="headerlink" title="4.2.3 矩阵间乘法"></a>4.2.3 矩阵间乘法</h4><p>矩阵相乘的前提必须是满足一下条件的矩阵<br>$$<br>C_{i×j} = A_{i×k} * B_{k×j}<br>$$<br>即<strong>第一个矩阵的列数必须等于第二个矩阵的行数</strong></p>
<p>运算法则相当简单，<strong>第一个矩阵的第 x 行 * 第二个矩阵的第 x 列，对应相乘再相加</strong><br>$$<br>A_{2×3} = \left[<br>\begin{array}{lcr}<br>1 &amp; 3 &amp; 2 \<br>4 &amp; 0 &amp; 1<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>B_{3×2} = \left[<br>\begin{array}{lcr}<br>1 &amp; 3 \<br>0 &amp; 1 \<br>5 &amp; 2<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>C_{2×2} = A_{2×3} * B_{3×2} =\left[<br>\begin{array}{lcr}<br>11 &amp; 10 \<br>9 &amp; 14 \<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>C_{11} = A_{11} * B_{11} + A_{12}<em>B_{21} + A_{13} * B_{31}\<br>=1</em>1 +3<em>0 + 2</em>5\<br>=11<br>$$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> m1[<span class="number">100</span>][<span class="number">10</span>],m2[<span class="number">10</span>][<span class="number">100</span>];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;n;i++)<span class="comment">//第一个矩阵的行标</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;k;j++)<span class="comment">//第二个矩阵的列标</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> l = <span class="number">0</span>;l&lt;m;l++)<span class="comment">//对应位置相乘再相加</span></span><br><span class="line">        &#123;</span><br><span class="line">            sum += m1[i][l]*m2[l][j];</span><br><span class="line">        &#125;</span><br><span class="line">        ans[i][j] = sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0505/150018_b874198b_5550632.png" alt=""></p>
<p>通过矩阵乘法，我们可以快速地计算出在三种不同地假设函数下房价的预测值</p>
<h4 id="4-2-4-矩阵乘法性质"><a href="#4-2-4-矩阵乘法性质" class="headerlink" title="4.2.4 矩阵乘法性质"></a>4.2.4 矩阵乘法性质</h4><p><strong>矩阵间乘法不满足乘法交换律</strong>：A*B != B * A</p>
<p><strong>矩阵间乘法满足乘法结合律</strong>：（A * B）* C = A * （B * C）</p>
<p>在实数乘法中我们有一个特殊的单位元：<strong>1</strong>，因为 <strong>1</strong> 乘任何数都为那个数本身，同理在矩阵乘法中，我们存在<strong>单位矩阵 E</strong>，它保证了所有与它相乘的矩阵，还是那个矩阵本身<br>$$<br>E_{2×2} =\left[<br>\begin{array}{lcr}<br>1 &amp; 0 \<br>0 &amp; 1<br>\end{array}<br>\right]<br>$$</p>
<p>$$<br>E_{3×3} =\left[<br>\begin{array}{lcr}<br>1 &amp; 0 &amp;0 \<br>0 &amp; 1 &amp;0 \<br>0 &amp; 0 &amp;1<br>\end{array}<br>\right]<br>$$</p>
<p>总之，单位矩阵就是<strong>主对角线上值都为1，其余位置都为0的特殊矩阵</strong>，而且单位矩阵行数和列数相等【方阵】，对于单位矩阵，我们可以对其进行交换。<br>$$<br>A_{3×3} * E = E * A_{3×3}<br>$$</p>
<h4 id="4-3-5-逆和转置"><a href="#4-3-5-逆和转置" class="headerlink" title="4.3.5 逆和转置"></a>4.3.5 逆和转置</h4><p><strong>矩阵的逆 (Inverse)</strong></p>
<p>在实数运算中我们会找到任意一个非零数的倒数，使得该数数乘其倒数的值为单位元 <strong>1</strong><br>$$<br>3<em>\frac{1}{3} =3</em>3^{-1} =1\<br>2<em>\frac{1}{2}=2</em>2^{-1} = 1\<br>13<em>\frac{1}{13}=13</em>13^{-1} = 1\<br>$$<br>同理，在矩阵运算中，我们也有这种性质，我们管这种矩阵叫做<strong>逆矩阵</strong>，它被记为：<br>$$<br>A^{-1}<br>$$<br>逆矩阵和该矩阵本身有着如下关系：<br>$$<br>A^{-1}A = AA^{-1} = E<br>$$</p>
<p>举个例子：<br>$$<br>A = \left[<br>\begin{array}{lcr}<br>3&amp;4 \<br>2&amp;16<br>\end{array}<br>\right]<br>\<br>A^{-1} = B =  \left[<br>\begin{array}{lcr}<br>0.4&amp;-0.1 \<br>-0.05&amp;0.075<br>\end{array}<br>\right]<br>\<br>AB = BA  =E<br>$$<br>我们把<strong>零矩阵【矩阵元素全为零】</strong>这类<strong>没有逆</strong>的矩阵称为<strong>奇异矩阵</strong>。</p>
<p><strong>矩阵的转置 (Transpose)</strong></p>
<p>矩阵的转置记作：<br>$$<br>A^{T}<br>$$<br>转置运算也很简单，就是<strong>行列互换</strong>例如：<br>$$<br>A = \left[<br>\begin{array}{lcr}<br>3&amp;4&amp;5 \<br>2&amp;1&amp;6<br>\end{array}<br>\right]<br>\<br>A^{T} = \left[<br>\begin{array}{lcr}<br>3 &amp; 2\<br>4 &amp; 1\<br>5 &amp; 6\<br>\end{array}<br>\right]<br>$$</p>
<br>

<h2 id="Part-5-多元线性回归"><a href="#Part-5-多元线性回归" class="headerlink" title="Part 5 多元线性回归"></a>Part 5 多元线性回归</h2><p>之前关于房价预测，只通过一个特征值<strong>面积</strong>进行房价预测，而如今，我们需要不止一个特征进行预测，例如添加<strong>卧室数量</strong>，<strong>房屋使用时间</strong>，<strong>层数</strong>等等特征值，以确保准确性。比如：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0508/192434_2b3e87e7_5550632.png" alt=""></p>
<p>我们规定以下字母的含义<br>$$<br>m:训练集训练数据总数\<br>n:特征值的数量\<br>x^{(i)}:训练集的第 i 条数据\<br>x^{(i)}_j:训练集的第 i 条数据的第j个特征值<br>$$<br>以上图为例：<br>$$<br>x^{(2)} = \left[<br>\begin{array}{lcr}<br>1416\<br>3\<br>2\<br>40\<br>232<br>\end{array}<br>\right]<br>\<br>x^{(2)}_2 = 2<br>$$<br>由于我们的特征值数量变多了，所以原来的假设函数：<br>$$<br>h_\theta(x) = \theta_0+\theta_1 x<br>$$<br>已经不合适了，因为它只能包含一个自变量。</p>
<p>所以我们把假设函数写为通式就变成了：<br>$$<br>h_\theta(x) = \theta_0+\theta_1 x_1+…+\theta_n x_n<br>$$<br>由于这个式子太长了，我们简化其写法，改成向量相乘的形式：<br>$$<br>假设x_0恒等于1\<br>\theta = \left[<br>\begin{array}{lcr}<br>\theta_0\<br>\theta_1\<br>\theta_2\<br>.\.\.\<br>\theta_n\<br>\end{array}<br>\right]<br>x = \left[<br>\begin{array}{lcr}<br>x_0\<br>x_1\<br>x_2\<br>.\.\.\<br>x_n\<br>\end{array}<br>\right]\<br>h_\theta(x) = \theta^{T}x = \theta_0x_0+\theta_1 x_1+…+\theta_n x_n<br>$$</p>
<h3 id="5-1-多元梯度下降法"><a href="#5-1-多元梯度下降法" class="headerlink" title="5.1 多元梯度下降法"></a>5.1 多元梯度下降法</h3><p>我们如今要推导出n个θ的梯度下降算法，由于梯度下降是在求θ的偏导数，所以，我们所谓的多元指的就是多模型参数θ。之前我们推导的梯度下降算法，是单维度【面积】的梯度下降算法，而如今我们的维度增加了，1个x对应2个模型参数θ，2个x对应3个模型参数θ……换句话说，参数θ增加了。</p>
<p>我们讲新的假设函数带入，代价函数中：<br>$$<br>J(\theta) = \frac{1}{2m}\sum_{i = 0} ^m(h_\theta (x^i) - y^i )^2<br>$$<br>得到梯度下降算法：<br>$$<br>Repeat{\<br>\theta_j := \theta_j-\alpha\frac{\partial }{\partial\theta_j}J(\theta_0,\theta_1…\theta_n)\<br>j = (0,1…n)\<br>}<br>$$<br>由于之前的假设：<br>$$<br>x^{i}_0 = 1<br>$$<br>我们可以对应单特征时，求模型参数偏导时的情况：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0510/100856_5031c2c2_5550632.png" alt=""></p>
<p>如此，我们就得到了多元梯度下降算法</p>
<h3 id="5-2-多元梯度下降-特征缩放"><a href="#5-2-多元梯度下降-特征缩放" class="headerlink" title="5.2 多元梯度下降-特征缩放"></a>5.2 多元梯度下降-特征缩放</h3><p>为了提高梯度下降的效率，我们才会进行特征缩放。我们以一个二元特征为例，进行说明：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0510/103938_24e97ca1_5550632.png" alt=""></p>
<p>忽略参数θ0，我们假设只有两个参数θ1和θ2，上图是代价函数的等高线图。左图，表示的是两个特征值取值范围相差很大时，做出的代价函数图像。</p>
<blockquote>
<p>特征值x1：房屋面积取值范围 [0,2000]feet²</p>
<p>特征值x2：卧室数量 [1,5] </p>
</blockquote>
<p>可以看到，由红线模拟的梯度下降算法过程，下降速度十分缓慢。而右图两个特征值<strong>都除以了它们的最大值</strong>，使得它们缩放到了相接近的<strong>取值区间</strong>，让等高线图的分部更加均匀，使得梯度下降时，使用更少的次数得出相同的结果。</p>
<p>一般来讲，只要让各个特征值的取值范围近似同就可以了，最好是缩放到[-1,1]之间：<br>$$<br>-1\leq x_i \leq1<br>$$<br>但是，缩放成以下结果我们也是可以接受的：<br>$$<br>0 \leq x_1\leq 3\<br>-2\leq x_2 \leq 0.5<br>$$<br>说到底，我们是<strong>通过缩放特征值取值范围的方式，减少梯度下降的计算次数，提高算法效率</strong></p>
<h4 id="5-2-1-均值归一化"><a href="#5-2-1-均值归一化" class="headerlink" title="5.2.1 均值归一化"></a>5.2.1 均值归一化</h4><p>这同样是一种缩放方式，比起粗暴地除以最大值不同，这种方法显得更优雅：<br>$$<br>μ:均值\<br>σ:标准差\<br>Z= \frac{x−μ}{σ}<br>$$<br>当然了，计算机科学不是纯粹的数学，我们没有必要那么精确，毕竟我们要解决实际问题，而一些实际问题多多少少没有那么严谨。事实上，我们可以用<strong>极差(最大值-最小值)</strong>来代替标准差。</p>
<p>把我们之前的例子拿过来，其实可以这样写：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0510/110024_d9d39569_5550632.png" alt=""></p>
<p>这样我们就近似地把两个特征值的范围缩放成相同取值范围了。</p>
<h3 id="5-3-多元梯度下降-学习率"><a href="#5-3-多元梯度下降-学习率" class="headerlink" title="5.3 多元梯度下降-学习率"></a>5.3 多元梯度下降-学习率</h3><p>为了提高梯度下降的效率，设置合适的学习率也是十分重要的。下面是代价函数收敛至局部最小的迭代次数和最小值的图像：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/111407_b08b3f6f_5550632.png" alt=""></p>
<p>会发现，当迭代至300，400次时，代价函数已经相对收敛至最小值了，正常的算法会呈现出这样的关系曲线。而，有时往往会出现以下的关系曲线，这都是<strong>学习率过大导致的,应当调小学习率</strong>：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/111816_196c60d7_5550632.png" alt=""></p>
<h3 id="5-4-特征和多项式回归"><a href="#5-4-特征和多项式回归" class="headerlink" title="5.4 特征和多项式回归"></a>5.4 特征和多项式回归</h3><p>特征，就是假设函数中的自变量，这一节讲的是<strong>如何构造假设函数</strong>，之前我们都是采用直线对训练集进行拟合，如今我们想用曲线来拟合数据，进而追求预测的准确性。</p>
<p>还是以房价为例：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/112948_2d830849_5550632.png" alt=""></p>
<p>第一种特征的选择方式是这样的：</p>
<blockquote>
<ol>
<li>frontage：房子前的土地长</li>
<li>depth：土地纵深</li>
</ol>
</blockquote>
<p>第二种是将两种特征聚合成矩形面积，进而变成单一特征：</p>
<blockquote>
<ol>
<li>area = frontage*depth</li>
</ol>
</blockquote>
<p>特征的数量不同，自然假设函数的形式就不同。不过再怎么说，以上两种选择都还是没有脱离用直线拟合的命运，</p>
<p>小学二年级我们就学过<strong>泰勒公式</strong>，我们可以知道通过使用<strong>高阶多项式</strong>可以近似函数。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/113549_6e81bd72_5550632.png" alt=""></p>
<p>同理，在机器学习中，如果你不想光用直线来做模型，我们也可以采用多项式进行建模。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/113807_2fbfce19_5550632.png" alt=""></p>
<p>对于，<strong>单一特征</strong>，我们有如上图的两种假设函数。</p>
<p>只用到二次方，它还是个抛物线，有可能超出对称轴后导致值变小，说人话就是，<strong>面积越大，房价越低</strong></p>
<p>三次方的函数图像和现实结果很像，所以我们采用三次函数进行拟合。<br>$$<br>x_1 = (size)\<br>x_2 = (size)^2\<br>x_3 = (size)^3\<br>此时，如果size的取值为[0,1000]m^2\<br>x_3 ∈ [0,10^9]<br>$$<br>此时，特征缩放的重要性就体现出来了。</p>
<p>而且对于这个问题，还有其它好多种建模方式，例如：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0516/114543_f7bac632_5550632.png" alt=""></p>
<p>对于特征选择，现在我们的工作就是尽量贴合实际自由选择，尽量不再用直线拟合。在以后的内容中，有算法可以来自动选择特征。</p>
<h3 id="5-5-正规方程-Normal-equation"><a href="#5-5-正规方程-Normal-equation" class="headerlink" title="5.5 正规方程 (Normal equation)"></a>5.5 正规方程 (Normal equation)</h3><p>之前我们使用梯度下降算法，采用<strong>迭代</strong>的方式一步一步找最小值</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0517/182641_c964752f_5550632.png" alt=""></p>
<p>有没有一种方式可以直接算出参数 θ ，而不必迭代计算呢？</p>
<p>我们假设代价函数看成一个关于 θ 的二次函数：<br>$$<br>J(\theta) = a\theta^2+b\theta+c<br>$$<br>二次函数求最值问题，我们只需要令导数为零，求出驻点即可。对于含有 n 个特征的 n 元函数，我们只需要<strong>对每个 θ 求偏导数令其为零，联立求解</strong>，就能得出 θ 的最小值，这就是正规方程所做的事情。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0517/183208_5642237f_5550632.png" alt=""></p>
<p>我们举一个例子来看看正规方程是如何运作的</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0517/183334_1e96ea2a_5550632.png" alt=""></p>
<p>假设训练集中只有 4 条数据 (m = 4) ，我们原来有4个代表房子的特征，再加一列 x0 恒等于 1。之后我们要做的就是计算<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$<br>参数就求出来了，不要问我为什么可以这么求。因为我也不会，工科啦，不要在意太多啦。</p>
<p><a href="[https://baike.baidu.com/item/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/10001812?fr=aladdin](https://baike.baidu.com/item/正规方程/10001812?fr=aladdin)">正规方程</a></p>
<p>矩阵X的选取方式很简单，将每一行的特征当作矩阵的每一行，再在第一列前插入 1 即可，向量 y 就是依次的真实值。</p>
<p>正规方程和梯度下降都能求模型参数，那么它们各自有什么优缺点供我们参考选择呢？</p>
<p><strong>梯度下降算法</strong></p>
<blockquote>
<p>缺点：</p>
<blockquote>
<ol>
<li>需要调整学习率 α</li>
<li>需要很多次迭代</li>
<li>需要进行特征缩放</li>
</ol>
</blockquote>
<p>优点：</p>
<blockquote>
<ol>
<li>无论有多少特征，都可以在有限的时间内计算出结果，例如 10 ^9 个特征</li>
</ol>
</blockquote>
</blockquote>
<p><strong>正规方程</strong></p>
<blockquote>
<p>缺点：</p>
<blockquote>
<ol>
<li>可以计算的特征数量有限，一般来说 &lt; 10000 个特征我们都可以选择正规方程法，</li>
</ol>
<p>因为计算矩阵转置非常耗时间。</p>
<ol start="2">
<li>使用场景首先，正规方程不适用于一些其他的机器学习算法。</li>
</ol>
</blockquote>
<p>优点：</p>
<blockquote>
<ol>
<li>不需要学习率</li>
<li>不需要进行特征缩放</li>
<li>一次算出答案，无需迭代</li>
</ol>
</blockquote>
</blockquote>
<h4 id="5-5-1-矩阵不可逆时的正规方程方法"><a href="#5-5-1-矩阵不可逆时的正规方程方法" class="headerlink" title="5.5.1 矩阵不可逆时的正规方程方法"></a>5.5.1 矩阵不可逆时的正规方程方法</h4><p>在正规方程中：<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$<br>我们会计算<br>$$<br>(X^TX)^{-1}<br>$$<br>在小学二年级就学过的线性代数中，我们知道，有些矩阵是不可逆的，也叫做<strong>奇异矩阵</strong>。在这里我只想说一说造成这种现象的原因以及解决办法。</p>
<blockquote>
<p><strong>特征选择重复或线性相关</strong></p>
<p>例如，在预测房价时，我选择两个特征一个是 feet²，另一个是 m²，很显然，它们的意义在本质上都是描述描述面积，而且 1m = 1.38feet。这两个量线性相关，所以可能会出现问题。</p>
<p><strong>特征值选择过多，训练集数目过少</strong></p>
<p>在一个m = 10 的训练集中，有1000个特征值，在加上 x_0 的话那就是一个 m×（n+1）的矩阵，此时也可能出现问题。</p>
</blockquote>
<p>如果是第一种问题，删除任意一个特征就好，如果是第二种问题，要么删除一些无关紧要的特征，要么使用之后所学习的<strong>正规化(regularization)</strong>方法。</p>
<h3 id="5-6-题外话-线性回归模型的数学原理"><a href="#5-6-题外话-线性回归模型的数学原理" class="headerlink" title="5.6 题外话-线性回归模型的数学原理"></a>5.6 题外话-线性回归模型的数学原理</h3><p>在实际工程应用中，我们总会出现这样一种情况<strong>已知一些离散的观测点，想要求出一个整体的函数规律</strong>。举个小例子：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0123/113908_c4026501_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这乍一看这跟机器学习没什么关系，但回归数学，本质却是一样的。它们的目的都是对数据集拟合出一条曲线。对上述问题进行分析，不难发现，这是一个*<em>数据规模为5条记录，假设函数为 l = θ0 + θ1</em>t，的线性回归模型 **。对于这样的问题我们往往这样处理：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0123/114347_94a711ef_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>接着，我们对Q求最小值，注意<strong>此时Q是关于模型参数θ的函数，其它量已知。将其转换为多元函数求最值的问题。</strong>对于这种问题，一般的数学做法就是<strong>求偏导数，联立偏导数方程，求驻点</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0123/114921_79266689_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这种方法在数学中叫做<strong>最小二乘法</strong>。而且很显然，当特征向量升维，假设函数复杂的时候，手算偏导数的方程组不现实，此时我们就分出来了两条路，一种是基于计算机的强大算力，对模型进行若干次迭代的<strong>梯度下降法</strong>，另一种是对方法本身进行优化，通过线性代数的方式得到的<strong>正规方程组法</strong>。</p>
<p>同时补充一下我对于梯度下降法这个名词的一些猜测，梯度在数学中，其实就是<strong>多元函数偏导数所构成的向量</strong>。由于此时我们不再计算梯度等于零时的值，那么我们只能尽量让其缩小了。这很可能就是梯度下降这个名词背后的故事。</p>
<br>

<h2 id="Part-6-Octave编程"><a href="#Part-6-Octave编程" class="headerlink" title="Part 6 Octave编程"></a>Part 6 Octave编程</h2><p>讲了这么多，是不是也想上手写一个Demo验证一下所学，接下来我们就开始学习 Octave 进行机器学习的实践之旅吧。</p>
<h3 id="6-1-基本数学运算"><a href="#6-1-基本数学运算" class="headerlink" title="6.1 基本数学运算"></a>6.1 基本数学运算</h3><p><img src="https://images.gitee.com/uploads/images/2020/0520/202201_ac4ea9e7_5550632.png" alt=""></p>
<p>同 C 语言</p>
<h3 id="6-2-逻辑运算"><a href="#6-2-逻辑运算" class="headerlink" title="6.2 逻辑运算"></a>6.2 逻辑运算</h3><p>同 C 语言</p>
<p><strong>注释为 %</strong></p>
<h3 id="6-3-声明矩阵和向量"><a href="#6-3-声明矩阵和向量" class="headerlink" title="6.3 声明矩阵和向量"></a>6.3 声明矩阵和向量</h3><p>语法：</p>
<p>每一行用 “;” 隔开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix &#x3D; [...;...;...]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; 1:0.1:2 %以1开始步长为0.1到2的向量</span><br></pre></td></tr></table></figure>



<p><img src="https://images.gitee.com/uploads/images/2020/0520/203159_3e1c86d8_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0520/203517_2846935b_5550632.png" alt=""></p>
<h3 id="6-4-从文件中读写矩阵"><a href="#6-4-从文件中读写矩阵" class="headerlink" title="6.4 从文件中读写矩阵"></a>6.4 从文件中读写矩阵</h3><p>现在我的硬盘上有如下两个文件：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/193842_0cb13551_5550632.png" alt=""></p>
<p>我们的目的是把它们读入到Octave的工作内存中。第一步就要切换文件路径，<strong>命令是 cd</strong>，和Windows的路径切换语法一样。接着可以通过<strong>ls</strong>语法查看当前目录下的文件</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/194124_2fdeb6fa_5550632.png" alt=""></p>
<p>接着我们通过使用<strong>load 文件名</strong>的方式就可以导入到Octave中了。此时可以通过<strong>who</strong>，和<strong>whos</strong>命令查看Octave工作空间下的所有变量信息</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/194442_74e76c21_5550632.png" alt=""></p>
<p>其中 <strong>who</strong> 只查询变量名，<strong>whos</strong>可以查出变量的具体信息。</p>
<p>如果我们想单独查看变量的属性，<strong>矩阵推荐使用size(…)，向量推荐使用length(…)命令</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/194700_a25d3b5f_5550632.png" alt=""></p>
<p>它们会告诉你矩阵的形状和向量的维度。接着我们就要安排如何访问矩阵和修改元素了。</p>
<p>首先，<strong>Octave中矩阵下标从 1 开始</strong>，切记，切记。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/195022_4fa7353c_5550632.png" alt=""></p>
<p>访问方式很简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">矩阵名(下标,下标)</span><br></pre></td></tr></table></figure>

<p>修改方式也很简单：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/195243_bdc69b08_5550632.png" alt=""></p>
<p>之后就是存储了，我们的第一种写法是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">save 文件名.mat</span><br></pre></td></tr></table></figure>


<p><img src="https://images.gitee.com/uploads/images/2020/0523/195849_92fc2b29_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/195926_fb81e54f_5550632.png" alt=""></p>
<p>目录下会生成一个压缩的二进制文件</p>
<p>重新打开加载到工作内存中时和打开文件的语法一样</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/200202_26de6ab7_5550632.png" alt=""></p>
<p>第二种写法是存成人能看懂的方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">save hello.txt -ascii</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0523/200714_8dbda46a_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/200752_4a56ba8d_5550632.png" alt=""></p>
<p>更多关于 save 命令的解释可以使用 <strong>help 命令名</strong>来具体查看，这里就不赘述了。</p>
<p>差点忘了，还有一个就是变量删除：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0523/201043_627c28ba_5550632.png" alt=""></p>
<h3 id="6-5-数据计算"><a href="#6-5-数据计算" class="headerlink" title="6.5 数据计算"></a>6.5 数据计算</h3><h4 id="6-5-1-矩阵相关计算"><a href="#6-5-1-矩阵相关计算" class="headerlink" title="6.5.1 矩阵相关计算"></a>6.5.1 矩阵相关计算</h4><p><img src="https://images.gitee.com/uploads/images/2020/0524/161748_6a980835_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/162104_546ca44b_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/162111_a993cef8_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/162118_bfbfcd98_5550632.png" alt=""></p>
<p>从上到下依次解释：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A&#39; %A的转置</span><br><span class="line">A. *E %A中对应元素与单位矩阵E相乘</span><br><span class="line">% . 代表矩阵内的元素</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0524/163434_741ba5f2_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; magic(9) %生成一个9*9的魔幻矩阵（每行每列，对角线的和都相等）</span><br><span class="line">sum(A,1) %按列求和</span><br><span class="line">sum(A,2) %按行求和</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0524/163813_822d997d_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum(A.*E) %A中对应元素乘单位矩阵，即只保留对角线上的元素，并按列求和</span><br><span class="line">sum(sum(A.*E)) %对角线的和</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0524/164110_1ebcd21d_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">temp &#x3D; pinv(A) %矩阵求逆</span><br></pre></td></tr></table></figure>



<h4 id="6-5-2-常用函数"><a href="#6-5-2-常用函数" class="headerlink" title="6.5.2 常用函数"></a>6.5.2 常用函数</h4><p><img src="https://images.gitee.com/uploads/images/2020/0524/164800_9cd10783_5550632.png" alt=""></p>
<blockquote>
<p>rand(n)：生成一个 n*n的随机矩阵</p>
<p>find(…)：返回符合条件的行和列</p>
</blockquote>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/165102_c71c05a2_5550632.png" alt=""></p>
<blockquote>
<p>max()，min()：找每列的最大最小值</p>
<p>A &lt; 5：返回一个布尔矩阵表示判断结果</p>
</blockquote>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/165432_a0c5962c_5550632.png" alt=""></p>
<blockquote>
<p>floor(…) ：向下取整</p>
<p>ceil(…)：向下取整</p>
</blockquote>
<p><img src="https://images.gitee.com/uploads/images/2020/0524/165611_59874ae4_5550632.png" alt=""></p>
<blockquote>
<p>prod(…)：所有元素相乘</p>
</blockquote>
<h3 id="6-6-数据可视化"><a href="#6-6-数据可视化" class="headerlink" title="6.6 数据可视化"></a>6.6 数据可视化</h3><p>学会将各种函数可视化，有助于更直观地学习机器学习算法，在这一节中我们介绍Octave中数据可视化的方式。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/202956_85b345b3_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clear</span><br><span class="line">x &#x3D; [0:0.01:0.98]; %生成一个初值为0，步长为0.01的到0.98的一维数组(向量)</span><br><span class="line">y1 &#x3D; sin(2*pi*4*x);%生成正弦函数</span><br><span class="line">y2 &#x3D; cos(2*pi*4*x);%生成余弦函数</span><br><span class="line">plot(x,y1); %以x和y1为数据画图</span><br></pre></td></tr></table></figure>

<p>这种画图方式只能保留一个函数的图像，$\color{red}{为了让两个函数同时存在}$我们可以这么做：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/203656_70436049_5550632.png" alt=""></p>
<p><strong>hold on</strong> 方法可以保留两个函数图像在同一个图里，但是没有横纵坐标，图例等等，就让人看得很难受。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/204551_4eb151dc_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plot(x,y1)</span><br><span class="line">hold on</span><br><span class="line">plot(x,y2,&#39;r&#39;)%顺带改个颜色</span><br><span class="line">xlabel(&#39;time&#39;)%横坐标含义</span><br><span class="line">ylabel(&#39;value&#39;)</span><br><span class="line">legend(&#39;sin&#39;,&#39;cos&#39;) %给不同图像起名</span><br><span class="line">title(&#39;myPlot&#39;) %给图像起标题</span><br><span class="line">print -dpng &#39;myPlot.png&#39; %存储图像为.png格式</span><br><span class="line">close %关闭图像</span><br></pre></td></tr></table></figure>

<p>既然可以把两个图像放到一张图中，自然也就可以分别呈现：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/205058_4151c1a1_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">figure(1);plot(x,y1,&#39;g&#39;);</span><br><span class="line">figure(2);plot(x,y2,&#39;r&#39;);</span><br></pre></td></tr></table></figure>

<p>我们还可以这样操作：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/205419_32d18fa7_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">subplot(1,2,1);</span><br><span class="line">plot(x,y1);</span><br><span class="line">subplot(1,2,2);</span><br><span class="line">plot(x,y2);</span><br></pre></td></tr></table></figure>

<p>我们还可以通过<strong>axis</strong>来调整x，y轴的单位长度：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0525/205609_7552332d_5550632.png" alt=""></p>
<h3 id="6-7-控制语句-if-for-while"><a href="#6-7-控制语句-if-for-while" class="headerlink" title="6.7 控制语句-if-for-while"></a>6.7 控制语句-if-for-while</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v &#x3D; zeros(10,1);%10维列向量</span><br><span class="line">for i&#x3D;1:10,</span><br><span class="line">	v(i) &#x3D; 2^i;</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0528/202054_50285d83_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i &#x3D; 1;</span><br><span class="line">while i&lt;&#x3D;5,</span><br><span class="line">	v(i) &#x3D; 100;</span><br><span class="line">	i++;</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0528/202259_83b73d2f_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">i &#x3D; 1;</span><br><span class="line">while true,</span><br><span class="line">	v(i) &#x3D; 999;</span><br><span class="line">	i++;</span><br><span class="line">	if i &#x3D;&#x3D; 6,</span><br><span class="line">		break;</span><br><span class="line">	end;</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0528/202745_44c28e81_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">v &#x3D; zeros(10,1);</span><br><span class="line">v(2) &#x3D; 10010;</span><br><span class="line">if v(2) &#x3D;&#x3D; 9999,</span><br><span class="line">  disp(&#39;GoodBye World&#39;);</span><br><span class="line">elseif v(2) &#x3D;&#x3D; 10010,</span><br><span class="line">  disp(&#39;Hello World&#39;);</span><br><span class="line">else</span><br><span class="line">  disp(&#39;There is no world&#39;);</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0528/203730_230aeb89_5550632.png" alt=""></p>
<h3 id="6-8-定义函数"><a href="#6-8-定义函数" class="headerlink" title="6.8 定义函数"></a>6.8 定义函数</h3><p>Octave中定义函数很特殊，一个函数是一个.m文件，$\color{red}{调用函数时一定要看好文件路径}$,文件内基本语法是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function 返回值 &#x3D; 函数名(参数1,参数2,...)</span><br><span class="line">  方法体</span><br></pre></td></tr></table></figure>

<p>Octave中可以一次返回多个值，这一点很特殊。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/204214_c56a669a_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%addAndsub文件内容</span><br><span class="line">function [x,y] &#x3D; addAndsub(a,b)</span><br><span class="line">  x &#x3D; a+b;</span><br><span class="line">  y &#x3D; a-b;</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/0528/205159_4d61dd50_5550632.png" alt=""></p>
<p>基于上面所学，让我们快速地写一个Demo来测试一下掌握语法情况。</p>
<p>我现在有一张图，我要根据这张图做出不同θ下的的代价函数</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/205644_3b350c6b_5550632.png" alt=""> </p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/210654_60dc1132_5550632.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function J &#x3D; costFunction(X,y,theta)</span><br><span class="line">  % matrix vector thetaMatrix</span><br><span class="line">  m &#x3D; size(X,1);</span><br><span class="line">  prediction &#x3D; X * theta;</span><br><span class="line">  sqrErrors &#x3D; (prediction - y).^2;</span><br><span class="line">  J &#x3D; 1&#x2F;(2*m) * sum(sqrErrors); </span><br><span class="line">endfunction %可加可不加</span><br></pre></td></tr></table></figure>

<p>之前说过，x_0 恒等于 1，对吧。以本题为例，预测结果就是一个3维列向量对吧 [1;2;3]，它和真实值[1;2;3]没有偏差，所以代价函数返回值为 0</p>
<p>我们改变参数 θ，再测一组：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/211208_5d28207f_5550632.png" alt=""><br>$$<br>J = \frac{1}{2<em>3}</em>(1^2+2^2+3^2) = 14 /6 = 2.33…<br>$$</p>
<h3 id="6-9-向量化"><a href="#6-9-向量化" class="headerlink" title="6.9 向量化"></a>6.9 向量化</h3><p>向量化这个名词可能很陌生但是大家之前其实已经用过了，我们假设，假设函数为如下所示：<br>$$<br>h_\theta(x) = \sum_{j = 0} ^n\theta_jx_j<br>$$<br>我们可以把θ，x当作向量：<br>$$<br>\theta = \left[<br>\begin{array}{lcr}<br>\theta_0\<br>\theta_1\<br>\theta_2\</p>
<p>\end{array}<br>\right]<br>\<br>x = \left[<br>\begin{array}{lcr}<br>x_0\<br>x_1\<br>x_2\</p>
<p>\end{array}<br>\right]<br>\<br>$$<br>所以假设函数就可以写成：<br>$$<br>h_\theta(x) = \theta^Tx<br>$$<br>换成编程语言来对比两种方法实现的话，就是这样的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%不采用向量化的结果</span><br><span class="line">prediction &#x3D; 0.0;</span><br><span class="line">for i &#x3D; 1:n+1,</span><br><span class="line">  prediction +&#x3D; theta[i]*x[i];</span><br><span class="line">endfor</span><br><span class="line">%采用向量化的结果</span><br><span class="line">prediction &#x3D; theta&#39;*x;</span><br></pre></td></tr></table></figure>

<p>现在，让我们向量化一个更复杂的式子，还记得梯度下降算法吗？我们试试能不能把它也向量化。首先我们回顾一下2个参数的梯度的下降算法以及它们推导之后的结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/104619_64ca995f_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0503/114604_b5ba2575_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/215232_f2a990c9_5550632.png" alt=""></p>
<p>这里罗列了<strong>训练集数目为 m 的，具有n元参数θ的代价函数的梯度下降算法</strong>的部分示意。接下来就来思考如何向量化吧。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0528/221023_5a780998_5550632.png" alt=""><br>$$<br>x^{(1)} = \left[<br>\begin{array}{lcr}<br>x_0^{(1)} = 1\<br>x_1^{(1)} = 2\<br>x_2^{(1)} = 12\<br>…<br>\end{array}<br>\right]<br>$$</p>
<p>梯度下降的目的是最小化代价函数，找出所有的模型参数θ，所以：$\color{red}{θ可以构成一个列向量}$，而学习率α和m又是一个常数，<strong>每一行特征值又可以看成一个特征列向量</strong>，所以第一步我们可以把式子化简成：<br>$$<br>x^{(i)} = \left[<br>\begin{array}{lcr}<br>x_0^{(i)}\<br>x_1^{(i)}\<br>x_2^{(i)}\</p>
<p>\end{array}<br>\right]<br>\\theta := \theta - \alpha<em>\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}<br>\向量 := 向量-数</em>数<em>求和(数</em>向量)<br>$$<br>由于数×向量还是向量，所以,再化简成：<br>$$<br>向量 := 向量-数<em>数</em>向量\<br>向量 := 向量-数<em>向量\<br>\theta := \theta -\alpha</em>\delta<br>$$<br>最终，我们就完成了一个梯度下降算法的向量化。</p>
<br>

<h2 id="Part-7-Logistic-回归算法"><a href="#Part-7-Logistic-回归算法" class="headerlink" title="Part 7 Logistic 回归算法"></a>Part 7 Logistic 回归算法</h2><h3 id="7-1-分类问题的引出"><a href="#7-1-分类问题的引出" class="headerlink" title="7.1 分类问题的引出"></a>7.1 分类问题的引出</h3><p>之前，我们学习了回归问题的一大经典算法，现在我们要开始学习<strong>分类问题</strong>的一大经典算法了。我们首先看几个分类问题的实例：</p>
<blockquote>
<p>判定邮件是否为垃圾邮件</p>
<p>判定肿瘤是否为恶性肿瘤</p>
<p>…</p>
</blockquote>
<p>这里我们的算法输出值<strong>由连续的值变成了离散的值</strong>，即：<br>$$<br>y∈{0,1}\<br>0 =&gt;负类\<br>1=&gt;正类<br>$$<br>当然了，分类问题可不只是只能分2类，当然可以分n类，为了方便学习，我们从二元分类开始学起。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0531/194005_e45563bf_5550632.png" alt=""></p>
<p>这是一个<strong>通过肿瘤大小对恶性肿瘤进行分类</strong>的例子。</p>
<p>其中，<strong>1 代表检查呈阳性(恶性肿瘤)，0 代表检查呈阴性(良性肿瘤)</strong></p>
<p>如果我们用我们之前的线性回归来解决这个问题，可能会得到这样一条假设函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0531/194329_b1a5a254_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0531/194412_9658376c_5550632.png" alt=""></p>
<p>我们可以假设分类器的输出阈值为<strong>0.5</strong>，这样看来，这个分类器是可以正确地区分肿瘤类型的。但是当训练集发生变化，我们可能会得到以下结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0531/194721_444d5ad6_5550632.png" alt=""></p>
<p>这条蓝色的函数，是一条很糟糕的曲线，同样在0.5处的阈值，它却把阳性肿瘤分类成了阴性肿瘤。而且通过线性回归算法做分类问题，最后假设函数的输出值可能远大于1，或远小于0。</p>
<p>由此看出，<strong>线性回归算法不能直接套用在分类算法上，由此我们要探究一种新的算法进行分类</strong>。（当然不用自己探究，之后直接给出）</p>
<h3 id="7-2-二元分类问题的假设函数"><a href="#7-2-二元分类问题的假设函数" class="headerlink" title="7.2 二元分类问题的假设函数"></a>7.2 二元分类问题的假设函数</h3><p>在线性回归中我们构建了假设函数:<br>$$<br>h_\theta(x) = \theta^Tx<br>$$<br>而且在之前的分析中我们得知，线性回归的假设函数不能直接作用于分类问题上，那么构建分类问题的假设函数就是现在的重要目标。</p>
<h4 id="7-2-1-Sigmoid-function-Logistic-function"><a href="#7-2-1-Sigmoid-function-Logistic-function" class="headerlink" title="7.2.1 Sigmoid function / Logistic function"></a>7.2.1 Sigmoid function / Logistic function</h4><p>分类问题采用假设函数的是<strong>Sigmoid</strong>函数(Logistic函数)</p>
<p>函数的定义长这样：<br>$$<br>h_\theta(x) = g(\theta^Tx)\<br>g(u) = \frac{1}{1+e^{-u}}\<br>h_\theta(x) = \frac{1}{1+e^{\theta^Tx}}<br>$$<br><img src="https://images.gitee.com/uploads/images/2020/0604/192615_e1b69cf0_5550632.png" alt=""></p>
<p>Sigmoid函数的图像长成这个样子，通过它，我们就可以实现一个二元分类器了。</p>
<p>这个函数的函数值$\color{red}{永远在0，1之间}$，我们可以这样解释这个函数值：<br>$$<br>P(y=1|x,\theta):在特征向量为x，模型参数为\theta时，结果为1的概率\<br>P(y=1|x,\theta)+P(y=0|x,\theta) = 1<br>$$<br>以判断肿瘤是否为恶为例：<br>$$<br>x = \left[<br>\begin{array}{lcr}<br>x_0\<br>x_1<br>\end{array}<br>\right]<br>=\left[<br>\begin{array}{lcr}<br>1\<br>肿瘤大小<br>\end{array}<br>\right]\<br>h_\theta(x) = 0.7 \<br>有70%的概率是恶性肿瘤<br>$$</p>
<h4 id="7-2-2-分类问题的代价函数"><a href="#7-2-2-分类问题的代价函数" class="headerlink" title="7.2.2 分类问题的代价函数"></a>7.2.2 分类问题的代价函数</h4><p><img src="https://images.gitee.com/uploads/images/2020/0605/174042_80c4ab72_5550632.png" alt=""></p>
<p>首先，我们来回顾一下分类问题中的已知条件，然后再捋一捋接下来要干什么。</p>
<p>到此为止介绍的机器学习算法无外乎都要经过4个步骤：</p>
<blockquote>
<ol>
<li><p><strong>构造假设函数</strong></p>
</li>
<li><p><strong>构造代价函数</strong></p>
</li>
<li><p><strong>梯度下降代价函数</strong></p>
</li>
<li><p>模型参数回代</p>
</li>
</ol>
</blockquote>
<p>我们之前讨论过了<strong>分类问题</strong>为什么不能直接使用线性回归的假设函数，进而给出了分类问题专用的假设函数<strong>Sigmoid函数</strong>。现在我们要做的就是探寻分类问题的<strong>代价函数</strong>，我们学习的方式和之前类似，咱们先试试原来线性回归的代价函数行不行。<br>$$<br>h_\theta(x) = \frac{1}{1+e^{\theta^Tx}}\<br>J_\theta(x) = \frac{1}{m}\sum_{i=1}^m\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\<br>$$<br>在这里，我们把1/2分配到求和号里面去了，这并不影响最终结果。</p>
<p>由于后面一串太长了，为了方便描述我们假设：<br>$$<br>Cost(h_\theta(x^{(i)}),y^{(i)}) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2<br>$$<br>把上标去掉：<br>$$<br>Cost(h_\theta(x),y) = \frac{1}{2}(h_\theta(x)-y)^2<br>$$</p>
<p>$$<br>J_\theta(x) =\frac{1}{m}Cost(h_\theta(x),y)<br>$$</p>
<p>现在，我们只要最小化这个代价函数，<strong>理论上就可以得到模型参数θ了</strong>，但是，天下没有这么好的事，事实上我们并不能对这个代价函数进行最小化，这是由于这个函数性质导致的。</p>
<p>由于假设函数h(x)，或者说sigmoid函数，不是一个<strong>线性的函数</strong>，导致整个cost的图像是一个<strong>关于参数θ的非凸函数</strong>，它可能长这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/180821_2ad451fe_5550632.png" alt=""></p>
<p>我抱着严谨的态度(不信邪)，使用随机数随机生成了单特征值的参数θ为1的cost函数，结果它长成这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/183832_d9fe636d_5550632.png" alt=""></p>
<p>咱们都玩过梯度下降算法，梯度下降算法需要一个初始值，然后它就会去找一个局部最小值，然而，对于上述图形而言，它们存在多个局部最小值。也就是说，直接用线性回归的假设函数是不行的。我们要找到一个<strong>关于参数θ的凸函数</strong>。也就是说要长成这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/181212_7e574104_5550632.png" alt=""></p>
<p>庆幸的是，这种函数咱们也不用自己找，数学家已经帮咱们干完这活儿了。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/183253_606eff6f_5550632.png" alt=""></p>
<p>我们分类问题的代价函数是这样一个二元分段函数，暂且不看函数的意思，我们先看看采用这种代价函数之后的结果图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/184033_b7099781_5550632.png" alt=""></p>
<p>和之前的基本条件一样，咱们可以明显地看出这个函数在最小值处有一个收敛的点，好像是1，是的，由于这个函数的θ我预设的就是1，参数θ跑出来是1也没有毛病。这证明了，采用这个函数当分类问题的代价函数是没有问题的。</p>
<p>我们再回头解释一下这个函数：<br>$$<br>Cost(h_\theta(x),y) = \left{<br>\begin{aligned}<br>-log(h_\theta(x))\quad if;y = 1\<br>-log(1-h_\theta(x))\quad if;y = 0\<br>\end{aligned}<br>\right.<br>$$</p>
<p>$$<br>Cost(0.7,1)<br>$$</p>
<p>这个意思就是说，假设函数跑出来的结果是0.7，y = 1，0.7到1之间的代价值为 -log( h(x) )</p>
<p>我们来直观地感受一下，当 y = 1 时的函数图像吧。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0605/185952_6f5ef4d1_5550632.png" alt=""></p>
<p>这个函数有个很有趣的性质，<strong>当真实值y = 1 时，且假设函数的预测值为 1 时，函数值无限接近于 0</strong>，这意味着，<strong>预测值和真实值 y = 1之间不用付出代价</strong>，同理如果对于一个真实值为 1 的输出，假设函数预测成了 0，<strong>那么预测值和真实值之间的代价就是无穷大</strong>。</p>
<p>看完了假设函数的其中一半，我们再看它的另外一半：<br>$$<br>Cost(h_\theta(x),y)=-log(1-h_\theta(x))\quad if;y = 0\<br>$$<br><img src="https://images.gitee.com/uploads/images/2020/0605/190711_29986289_5550632.png" alt=""></p>
<p>当真实值为 0 时，你预测的结果要是 1，那么所付出的代价就是正无穷。</p>
<p>结合两个图像一起看，总结成一句话就是，$\color{red}{预测的越准，算法所付出的代价就越小}$。</p>
<h4 id="7-2-3-简化分类问题的代价函数"><a href="#7-2-3-简化分类问题的代价函数" class="headerlink" title="7.2.3 简化分类问题的代价函数"></a>7.2.3 简化分类问题的代价函数</h4><p>之前的代价函数张这样：<br>$$<br>J_\theta(x) =\frac{1}{m}Cost(h_\theta(x),y)\<br>Cost(h_\theta(x),y) = \left{<br>\begin{aligned}<br>-log(h_\theta(x))\quad if;y = 1\<br>-log(1-h_\theta(x))\quad if;y = 0\<br>\end{aligned}<br>\right.<br>$$<br>我们可以发现，函数Cost是一个分段函数，我们现在希望把分段函数整合成一个新的等价的代价函数，这里我们直接给出，然后证明其正确性：<br>$$<br>Cost(h_\theta(x),y) = -y<em>-log(h_\theta(x))-(1-y)*log(1-h_\theta(x))\<br>J_\theta(x) =\frac{1}{m}Cost(h_\theta(x),y)\<br>$$<br>由于我们的输出值只有两个，即y = 0,y = 1。那我们看看当y = 0时，Cost的值为：<br>$$<br>Cost(h_\theta(x),0)  = -0</em>-log(h_\theta(x))-(1-0)*log(1-h_\theta(x))\<br>=-log(1-h_\theta(x))<br>$$<br>当y = 1 时：<br>$$<br>Cost(h_\theta(x),1) = -log(h_\theta(x))<br>$$<br>由此可见这个新的代价函数和原来的是等价的，那么按照套路我们就要对这个新的Logistic 回归的代价函数进行梯度下降了。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0606/111059_66ef2190_5550632.png" alt=""></p>
<h4 id="7-2-4-Logistic-Regression-Gradient-Descent"><a href="#7-2-4-Logistic-Regression-Gradient-Descent" class="headerlink" title="7.2.4 Logistic Regression Gradient Descent"></a>7.2.4 Logistic Regression Gradient Descent</h4><p><img src="https://images.gitee.com/uploads/images/2020/0606/111237_2713c6f2_5550632.png" alt=""></p>
<p>在我给出答案之前，先呈上我的手推结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0606/205910_e27d4578_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0606/205941_f1d957aa_5550632.png" alt=""></p>
<p>然后我再直接给出正确的公式吧<br>$$<br>\theta_j := \theta_j - \alpha<em>\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))</em>x_j^{(i)}<br>$$<br>会惊奇的发现，逻辑回归算法的梯度下降结和线性回归算法的梯度下降值是一样的，然而由于假设函数的不同，注定导致它们不是同一种算法。</p>
<h4 id="7-2-5-高级优化-知道个名称就行了"><a href="#7-2-5-高级优化-知道个名称就行了" class="headerlink" title="7.2.5 高级优化-知道个名称就行了"></a>7.2.5 高级优化-知道个名称就行了</h4><p>在之前我们需要计算，代价函数 J(θ) 和 它的偏导数。这里有一些优化算法可以比梯度下降更快地完成工作。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0611/184507_80ece889_5550632.png" alt=""></p>
<p>它们是：</p>
<ol>
<li>共轭梯度法(Gradient descent)</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ol>
<p>它们都比梯度下降收敛速度快，而且可以自动设置学习率。</p>
<p>由于这些严重超纲，这里只做简单的介绍，而不再深入讨论了。</p>
<h3 id="7-3-多元分类问题"><a href="#7-3-多元分类问题" class="headerlink" title="7.3 多元分类问题"></a>7.3 多元分类问题</h3><p>先说场景：</p>
<p>区分邮件是如下几类中的一个：</p>
<ol>
<li>家人</li>
<li>公司</li>
<li>同学</li>
<li>朋友</li>
</ol>
<p>区分鼻塞是如下类中的一个：</p>
<ol>
<li>感冒</li>
<li>着凉</li>
<li>流感</li>
</ol>
<p>区分天气是如下类中的一个：</p>
<ol>
<li>晴天</li>
<li>多云</li>
<li>下雨</li>
<li>大风</li>
</ol>
<p>我们的类别由之前的2类，上升到了多类问题，但是其本质还是<strong>监督机器学习</strong>，那么它们的区别就会在训练集上有所体现：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0611/190546_68646bab_5550632.png" alt=""></p>
<p>我们可以这样处理这个数据集，将它分成三块：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0611/190708_66fbbd4a_5550632.png" alt=""></p>
<blockquote>
<ol>
<li>Class 1 和 其它的类分开</li>
<li>Class 2 和 其他的类分开</li>
<li>Class 3 和其它的类分开</li>
</ol>
</blockquote>
<p>在第一种情况中我们把Class 1看作正类，我们会有如下的假设函数：<br>$$<br>h^{(1)}(x)<br>$$<br><img src="https://images.gitee.com/uploads/images/2020/0611/191013_7be127aa_5550632.png" alt=""><br>$$<br>h^{(i)}(x)=P(y=i|x,\theta),(y = 1,2,3)<br>$$<br>当 i = 1时，它代表了当特征向量为x，模型参数为θ时，$\color{red}{为Class 1 的概率}$。同理我们可以划分其它两个类，最终它们长这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0611/191221_794cc28a_5550632.png" alt=""></p>
<p>那我们最终怎么确定是哪一类呢？<br>$$<br>i=max_i(h^{(i)}(x))<br>$$<br>咱们 n 个类都算一算，哪个类的概率最大，最后不就是它吗。</p>
<br>

<h2 id="Part-8-过拟合-Overfitting"><a href="#Part-8-过拟合-Overfitting" class="headerlink" title="Part 8 过拟合 (Overfitting)"></a>Part 8 过拟合 (Overfitting)</h2><h3 id="8-1-什么是过拟合"><a href="#8-1-什么是过拟合" class="headerlink" title="8.1 什么是过拟合"></a>8.1 什么是过拟合</h3><p>我们用回归问题和分类问题来描述什么是过拟合</p>
<p>先看一张图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0612/194523_ecf1e02b_5550632.png" alt=""></p>
<p>这是房价预测的例子，从左到右我们可以看到三张图，它们的假设函数次数越来越高。</p>
<p>左面第一张图，只是通过一条直线来拟合数据集，这个结果并不是很好，它忽略了一些训练数据，随着面积的增加训练集应该是一个平滑的趋势，但是它并没有做到这一点。对于这样的机器学习模型，我们称它为<strong>欠拟合 (underfit)</strong>。</p>
<p>中间这张图，用二次函数来拟合数据集效果不错，这是我们想要的结果。</p>
<p>右面的这张图，这个假设函数<strong>过分地让自己拟合数据集</strong>，导致<strong>泛化能力很差</strong>。（泛化能力就是这个模型迁移到新问题上的能力）。</p>
<p>我们再看一个分类问题的例子：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0612/195149_adf51773_5550632.png" alt=""></p>
<p>从左向右，依次是，<strong>欠拟合</strong>，合适的，<strong>过拟合</strong>的图像。</p>
<p>由于采用了高阶多项式去拟合函数，使得假设函数本身过度依赖训练集。</p>
<p>当特征值少的时候，我们还可以通过画假设函数图像的方式进行判断是否发生了过拟合或者欠拟合的现象。然而现实中的问题很复杂，我们不可能通过极少的特征来描述问题，对于一个 房价预测问题它的特征值有可能是这样的：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0612/195717_b114d489_5550632.png" alt=""></p>
<p>试问 1个100 元函数，计算机要花多长时间进行绘图？</p>
<p>如何判断过拟合和欠拟和的问题我们以后会学习，现在要解决的是我们有哪些手段可以就解决过拟合的问题呢？</p>
<h3 id="8-2-解决过拟合的途径"><a href="#8-2-解决过拟合的途径" class="headerlink" title="8.2 解决过拟合的途径"></a>8.2 解决过拟合的途径</h3><p><img src="https://images.gitee.com/uploads/images/2020/0612/200001_33be60d2_5550632.png" alt=""></p>
<ol>
<li>减少特征的数量<ol>
<li>人工选择保留哪些特征</li>
<li>通过选择算法自动选择特征</li>
</ol>
</li>
<li>正则化：保留所有特征，通过减小模型参数，保证模型的准确性。</li>
</ol>
<h3 id="8-3-正则化-Regularization"><a href="#8-3-正则化-Regularization" class="headerlink" title="8.3 正则化 (Regularization)"></a>8.3 正则化 (Regularization)</h3><p>正则化的目的就是<strong>在不舍弃特征值的前提下，让代价函数计算出最合适的模型参数θ</strong>，如何做到这一点呢？我们来看个简单的引入例子。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0613/195812_5b0347c6_5550632.png" alt=""></p>
<p>我们对θ<del>3</del>，θ<del>4</del>这两个模型参数设置<strong>惩罚项</strong>，简单来说就是故意让它们变大，由于要求代价函数的最小值，在惩罚项不变的前提下，快速地能让整个式子变小，只能通过缩小θ<del>3</del>，θ<del>4</del>这两个参数的值，算到最后，它们就基本缩小到零了。然后原来的高次多项式就会被简化成如下紫红色曲线的样子：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0613/204738_bc778e83_5550632.png" alt=""></p>
<p>如果我们想让它变成一个通用的规则我们可以这样写我们的代价函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0613/205101_874f23b1_5550632.png" alt=""></p>
<p>我们对每个参数都添加惩罚项，最终最小化的结果就是我们正则化之后的结果。</p>
<p>这里我们要注意两个地方：</p>
<ol>
<li><strong>正则化参数λ</strong></li>
<li>习惯上我们不对θ<del>0</del> 进行正则化，即便带上它也不影响最终结果</li>
</ol>
<p><img src="https://images.gitee.com/uploads/images/2020/0613/205523_3315ed35_5550632.png" alt=""></p>
<p>正则化参数λ是干什么的呢？它有两个目标：</p>
<ol>
<li>对于前半部分(原来的代价函数)，让它更好的去拟合训练集</li>
<li>对于后半部分，尽量让模型参数变小</li>
</ol>
<p>它起到了一个平衡的作用，如果它选的太大，导致惩罚代价太大，使得所有模型参数都会趋向于零，也就是说在假设函数里就剩下x<del>0</del>了，即，一条直线，显然这是不合适的。</p>
<h3 id="8-4-线性回归的正则化"><a href="#8-4-线性回归的正则化" class="headerlink" title="8.4  线性回归的正则化"></a>8.4  线性回归的正则化</h3><p><img src="https://images.gitee.com/uploads/images/2020/0617/205419_89407319_5550632.png" alt=""></p>
<p>这是我们新的代价函数，我们对它做完梯度下降的结果为：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0617/205644_0e6805c4_5550632.png" alt=""></p>
<p>前项推导之前做过了，咱们只对新加的惩罚项求偏导数即可。我们把θ<del>j</del>给提出来。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0617/210015_72996c2e_5550632.png" alt=""></p>
<p>你会发现后项不变，前项变成了一个很有意思的项：<br>$$<br>1-\alpha\frac{λ}{m}<br>$$<br>由于学习率通常很小而且数据集通常也很大，这就导致这个系数很接近于1，你可以把它当作0.999，这样的结果就带来了：<strong>θ<del>j</del>距离0变小了，或者说它的平方范数小了（向量变短了）</strong>。</p>
<p>我们之前还学过正规方程的方式来求模型参数。</p>
<p>原来的正规方程长这样：<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p>
<p>正则化的正规方程法长这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0617/210632_73355f8c_5550632.png" alt=""></p>
<p>PS：当 λ &gt; 0时，矩阵是一定可逆的。</p>
<h3 id="8-5-Logistic-回归的正则化"><a href="#8-5-Logistic-回归的正则化" class="headerlink" title="8.5 Logistic 回归的正则化"></a>8.5 Logistic 回归的正则化</h3><p>再讲Logistic 回归的正则化之前，先看一下Logistic回归的假设函数模型：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0606/111059_66ef2190_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0619/184328_907290ae_5550632.png" alt=""></p>
<p>在处理分类问题时，我们有可能会采用高阶多项式作为sigmoid函数的参数导致过拟合的出现，如今，我们同样可以借鉴线性回归的方式，给分类问题的代价函数后面加一个惩罚项，然后对新的代价函数进行梯度下降。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0619/184858_e5dd9465_5550632.png" alt=""></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0619/184956_0fd99a49_5550632.png" alt=""></p>
<p>梯度下降结果如上图所示。此处和线性回归的形式一样，<strong>但是，它们不是一个东西</strong>，$\color{red}{线性回归和Logistic回归的根本区别在于假设函数}$。</p>
<p>写成Octave函数的模式如下图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0619/185416_a254dffd_5550632.png" alt=""></p>
<br>

<h2 id="Part-9-机器学习实战-房价预测"><a href="#Part-9-机器学习实战-房价预测" class="headerlink" title="Part 9 机器学习实战-房价预测"></a>Part 9 机器学习实战-房价预测</h2><p>为什么之前讲的那么快，因为我已经按捺不住做实验的心了。纸上得来终觉浅，绝知此事要躬行。以下是，我针对<strong>线性回归模型</strong>的一系列实验。不感兴趣的小伙伴可跳过这一章。</p>
<h3 id="9-1-数据集来源以及数据集说明"><a href="#9-1-数据集来源以及数据集说明" class="headerlink" title="9.1 数据集来源以及数据集说明"></a>9.1 数据集来源以及数据集说明</h3><h4 id="0x00-数据集的介绍"><a href="#0x00-数据集的介绍" class="headerlink" title="0x00 数据集的介绍"></a>0x00 数据集的介绍</h4><p><strong>波士顿房价数据集（Boston House Price Dataset）（下载地址：<a href="http://t.cn/RfHTAgY）" target="_blank" rel="noopener">http://t.cn/RfHTAgY）</a></strong></p>
<p>CRIM：城镇人均犯罪率。</p>
<p>ZN：住宅用地超过 25000 sq.ft. 的比例。</p>
<p>INDUS：城镇非零售商用土地的比例。</p>
<p>CHAS：查理斯河空变量（如果边界是河流，则为1；否则为0）。</p>
<p>NOX：一氧化氮浓度。</p>
<p>RM：住宅平均房间数。</p>
<p>AGE：1940 年之前建成的自用房屋比例。</p>
<p>DIS：到波士顿五个中心区域的加权距离。</p>
<p>RAD：辐射性公路的接近指数。</p>
<p>TAX：每 10000 美元的全值财产税率。</p>
<p>PTRATIO：城镇师生比例。</p>
<p>B：1000（Bk-0.63）^ 2，其中 Bk 指代城镇中黑人的比例。</p>
<p>LSTAT：人口中地位低下者的比例。</p>
<p>MEDV：自住房的平均房价，以千美元计。</p>
<br>

<h4 id="0x01-数据集的类别和规模"><a href="#0x01-数据集的类别和规模" class="headerlink" title="0x01 数据集的类别和规模"></a>0x01 数据集的类别和规模</h4><p>正常数据集大小（506，13）</p>
<p>经过pandas数据处理后得到的训练集和测试集的大小为，均多加一列，x_0 = 1</p>
<p>训练集：（404，14）占总测试集的80%</p>
<p>测试集：（103，14）占总测试集的20%</p>
<br>

<h3 id="9-2-实验设计"><a href="#9-2-实验设计" class="headerlink" title="9.2 实验设计"></a>9.2 实验设计</h3><h4 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h4><p>使用之前学过的所有关于线性回归的算法</p>
<blockquote>
<ol>
<li>正规方程法预测</li>
<li>无参数归一化的梯度下降预测</li>
<li>采用z-score 特征归一化之后的梯度下降预测</li>
<li>采用 max-min-scaling 特征归一化之后的梯度下降预测</li>
</ol>
</blockquote>
<p>2，3，4 来比较，学习率和迭代次数是否因<strong>有无归一化而变化</strong>。</p>
<p>3，4来比较，<strong>不同归一化方法对同一训练集有无影响</strong>。</p>
<p>输出：各种处理的结果图，真实值是离散的散点，结果是预测的函数。</p>
<h4 id="0x01-源代码"><a href="#0x01-源代码" class="headerlink" title="0x01 源代码"></a>0x01 源代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DATA_SET_PATH = <span class="string">r"D:\ProgramLearning\Python\MachineLearning\dataset\housing.csv"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_frame</span><span class="params">(file_path = DATA_SET_PATH)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pd.read_csv(file_path,sep = <span class="string">','</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_set</span><span class="params">(data_frame)</span>:</span></span><br><span class="line">    feature_columns_list = [<span class="string">"CRIM"</span>, <span class="string">"ZN"</span>, <span class="string">"INDUS"</span>, <span class="string">"CHAS"</span>, <span class="string">"NOX"</span>, <span class="string">"RM"</span>, <span class="string">"AGE"</span>, <span class="string">"DIS"</span>, <span class="string">"RAD"</span>, <span class="string">"TAX"</span>, <span class="string">"PTRATIO"</span>, <span class="string">"B"</span>,</span><br><span class="line">                            <span class="string">"LSTAT"</span>]</span><br><span class="line">    value_columns_list = [<span class="string">"MEDV"</span>]</span><br><span class="line">    train_data_frame = data_frame[<span class="number">1</span>:int(data_frame.shape[<span class="number">0</span>] * <span class="number">0.8</span>)]</span><br><span class="line">    train_data_frame_x = pd.DataFrame(train_data_frame, columns=feature_columns_list)</span><br><span class="line">    train_data_frame_y = pd.DataFrame(train_data_frame, columns=value_columns_list)</span><br><span class="line">    train_data_frame_x.to_csv(<span class="string">r"..\dataset\FeatureX.csv"</span>, index=<span class="literal">None</span>)</span><br><span class="line">    train_data_frame_y.to_csv(<span class="string">r"..\dataset\ValueY.csv"</span>, index=<span class="literal">None</span>)</span><br><span class="line">    print(<span class="string">"Generate training set successfully!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_set</span><span class="params">(data_frame)</span>:</span></span><br><span class="line">    feature_columns_list = [<span class="string">"CRIM"</span>, <span class="string">"ZN"</span>, <span class="string">"INDUS"</span>, <span class="string">"CHAS"</span>, <span class="string">"NOX"</span>, <span class="string">"RM"</span>, <span class="string">"AGE"</span>, <span class="string">"DIS"</span>, <span class="string">"RAD"</span>, <span class="string">"TAX"</span>, <span class="string">"PTRATIO"</span>, <span class="string">"B"</span>,</span><br><span class="line">                            <span class="string">"LSTAT"</span>]</span><br><span class="line">    value_columns_list = [<span class="string">"MEDV"</span>]</span><br><span class="line">    test_data_frame = data_frame[int(data_frame.shape[<span class="number">0</span>]*<span class="number">0.8</span>)+<span class="number">1</span>:]</span><br><span class="line">    test_data_frame_x = pd.DataFrame(test_data_frame,columns=feature_columns_list)</span><br><span class="line">    test_data_frame_y = pd.DataFrame(test_data_frame,columns=value_columns_list)</span><br><span class="line">    test_data_frame_x.to_csv(<span class="string">r"..\dataset\testFeatureX.csv"</span>,index=<span class="literal">None</span>)</span><br><span class="line">    test_data_frame_y.to_csv(<span class="string">r"..\dataset\testValueY.csv"</span>,index = <span class="literal">None</span>)</span><br><span class="line">    print(<span class="string">"Generate test set successfully!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_matrix_x</span><span class="params">(data_frame)</span>:</span></span><br><span class="line">    matrix = np.array(data_frame)</span><br><span class="line">    ones_b = np.ones(data_frame.shape[<span class="number">0</span>])</span><br><span class="line">    new_matrix = np.c_[ones_b,matrix]</span><br><span class="line">    <span class="keyword">return</span> np.mat(new_matrix)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vector_y</span><span class="params">(data_frame)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(data_frame)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_set_info</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># CRIM：城镇人均犯罪率。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># ZN：住宅用地超过 25000 sq.ft. 的比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># INDUS：城镇非零售商用土地的比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># CHAS：查理斯河空变量（如果边界是河流，则为1；否则为0）。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># NOX：一氧化氮浓度。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># RM：住宅平均房间数。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># AGE：1940 年之前建成的自用房屋比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># DIS：到波士顿五个中心区域的加权距离。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># RAD：辐射性公路的接近指数。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># TAX：每 10000 美元的全值财产税率。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># PTRATIO：城镇师生比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># B：1000（Bk-0.63）^ 2，其中 Bk 指代城镇中黑人的比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># LSTAT：人口中地位低下者的比例。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># MEDV：自住房的平均房价，以千美元计。</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">一共有3种方式进行预测，</span></span><br><span class="line"><span class="string">第一种：正规方程法</span></span><br><span class="line"><span class="string">第二种：无归一化的梯度下降法，学习率:1e-14,迭代次数：150000</span></span><br><span class="line"><span class="string">第三种：归一化的梯度下降法，学习率：，迭代次数</span></span><br><span class="line"><span class="string">监督学习必要事项： 1.假设函数 hypothesis_function()</span></span><br><span class="line"><span class="string">                  2.代价函数 cost_function()</span></span><br><span class="line"><span class="string">                  3.初始化的模型参数 θ generate_init_theta() [梯度下降时用]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(feature_x,theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    h(x) = theta_0*x_0 + theta_1*x_1 +...+theta_n*x_n</span></span><br><span class="line"><span class="string">    x_0 = 1</span></span><br><span class="line"><span class="string">    向量化后，predict_vector_y = feature_x . theta</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :return: numpy.ndarray , n维列向量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    predict_vector_y = feature_x.dot(theta)</span><br><span class="line">    <span class="keyword">return</span> predict_vector_y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(feature_x,vector_y,theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    J(θ) = (1/(2*m)) * ∑(i=1,m)(h(x) - y)²</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :param vector_y: </span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :return: 一个浮点数，值越小模型结果和真实值越贴近</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = feature_x.shape[<span class="number">0</span>]</span><br><span class="line">    sum_value = np.sum((hypothesis_function(feature_x,theta) - vector_y)**<span class="number">2</span>)</span><br><span class="line">    cost_value = (<span class="number">1</span>/(<span class="number">2</span>*m)) * sum_value</span><br><span class="line">    <span class="keyword">return</span> cost_value</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_init_theta</span><span class="params">(feature_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    生成初始的模型参数</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :return: 类型为numpy.ndarray的值为 1 的 n维列向量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    theta = np.ones(feature_x.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equation</span><span class="params">(feature_x,vector_y)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    计算: theta = (X^TX)^&#123;-1&#125;X^Ty</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :param vector_y: </span></span><br><span class="line"><span class="string">    :return: 一个 n维的列向量，类型 numpy.ndarray  </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#求 X^T . X</span></span><br><span class="line">    matrix_temp = np.transpose(feature_x).dot(feature_x)</span><br><span class="line">    <span class="comment">#求 matrix_temp的伪逆</span></span><br><span class="line">    matrix_temp_pinv = np.linalg.pinv(matrix_temp)</span><br><span class="line">    <span class="comment"># 求 X^T . y</span></span><br><span class="line">    matrix_temp = np.transpose(feature_x).dot(vector_y)</span><br><span class="line">    theta = matrix_temp_pinv.dot(matrix_temp)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(feature_x,vector_y,theta,alpha = <span class="number">1e-7</span>,iteration_times = <span class="number">550000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    没有向量化的梯度下降算法：</span></span><br><span class="line"><span class="string">    theta_x := theta_x - α * 1/m *Σ(i=1,m)[h(x^i) - y^i]*x^i</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :param vector_y: </span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :param alpha: 学习率，默认 1e-7</span></span><br><span class="line"><span class="string">    :param iteration_times: 迭代次数，默认 210000</span></span><br><span class="line"><span class="string">    :return: theta(n,1)</span></span><br><span class="line"><span class="string">    设:矩阵feature_x_t = (feature_x^T),向量 vector_temp = (h(x) - y)，</span></span><br><span class="line"><span class="string">    系数coefficient = alpha - (1/m)</span></span><br><span class="line"><span class="string">    vector_temp: (m,1)</span></span><br><span class="line"><span class="string">    feature_x:  (m,n)</span></span><br><span class="line"><span class="string">    feature_x_t: (n,m) </span></span><br><span class="line"><span class="string">    向量化后的：theta := theta - coefficient * (feature_x_t.dot(vector_temp))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = feature_x.shape[<span class="number">0</span>]</span><br><span class="line">    theta = theta.reshape((feature_x.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#迭代 iteration_times 次进行计算</span></span><br><span class="line">    <span class="keyword">for</span> cnt <span class="keyword">in</span> range(iteration_times):</span><br><span class="line">        <span class="keyword">if</span> cnt % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"第%s次迭代的代价函数值为:%s"</span>%(cnt,cost_function(feature_x,vector_y,theta)))</span><br><span class="line">        <span class="comment"># 系数 coefficient = alpha * (1/m)</span></span><br><span class="line">        coefficient = alpha * (<span class="number">1</span> / m)</span><br><span class="line">        <span class="comment"># 向量 vector_temp = (h(x) - y)</span></span><br><span class="line">        vector_temp = hypothesis_function(feature_x, theta).reshape((feature_x.shape[<span class="number">0</span>], <span class="number">1</span>)) \</span><br><span class="line">                      - vector_y</span><br><span class="line">        <span class="comment"># 矩阵 feature_x_t = (feature_x^T)</span></span><br><span class="line">        feature_x_t = np.transpose(feature_x)</span><br><span class="line">        theta -= (coefficient * (feature_x_t.dot(vector_temp)))</span><br><span class="line">    print(<span class="string">"迭代%s次后，最终的代价函数值为：%s"</span>%</span><br><span class="line">          (iteration_times,cost_function(feature_x,vector_y,theta)))</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="comment">#归一化</span></span><br><span class="line"><span class="comment"># z-score 归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">z_score</span><span class="params">(feature_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    当原始训练数据可近似为正态分布时，效果最好</span></span><br><span class="line"><span class="string">    对feature_x和vector_y做z_score均一化</span></span><br><span class="line"><span class="string">    X = (X-μ)/σ</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :return: feature_x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    col_avg = np.average(feature_x, axis=<span class="number">0</span>)</span><br><span class="line">    col_standard_deviation = np.std(feature_x, axis=<span class="number">0</span>)</span><br><span class="line">    col_avg = col_avg.reshape((feature_x.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    col_standard_deviation = col_standard_deviation.reshape((feature_x.shape[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,feature_x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> col_standard_deviation[j] == <span class="number">0</span>:</span><br><span class="line">            col_standard_deviation[j] = <span class="number">1e-30</span></span><br><span class="line">        feature_x[:,j] = (feature_x[:,j] - col_avg[j])/col_standard_deviation[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_x</span><br><span class="line"></span><br><span class="line"><span class="comment"># max-min-scaling</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_min_scaling</span><span class="params">(feature_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    X = (X-min(X)) / (max(X) - min(X))</span></span><br><span class="line"><span class="string">    :param feature_x: </span></span><br><span class="line"><span class="string">    :return: feature_x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    feature_x_min = np.min(feature_x,axis=<span class="number">0</span>)</span><br><span class="line">    feature_x_max_min_range = np.max(feature_x,axis = <span class="number">0</span>) - feature_x_min</span><br><span class="line">    feature_x_min = feature_x_min.reshape((feature_x.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    feature_x_max_min_range = feature_x_max_min_range.reshape((feature_x.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,feature_x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> feature_x_max_min_range[j] == <span class="number">0</span>:</span><br><span class="line">            feature_x_max_min_range[j] = <span class="number">1e-30</span></span><br><span class="line">        feature_x[:, j] = (feature_x[:, j] - feature_x_min[j]) / feature_x_max_min_range[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_picture</span><span class="params">(test_set_x,test_set_y,ne_predict_y,gd_predict_y,z_score_gd_predict_y,mm_scaling_gd_predict_y)</span>:</span></span><br><span class="line"></span><br><span class="line">    plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">    plt.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span></span><br><span class="line">    house_id_x = []</span><br><span class="line">    <span class="keyword">for</span> house_id <span class="keyword">in</span> range(<span class="number">1</span>,test_set_x.shape[<span class="number">0</span>]+<span class="number">1</span>):</span><br><span class="line">        house_id_x.append(house_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#形状转换</span></span><br><span class="line">    ne_predict_y = ne_predict_y.tolist()</span><br><span class="line">    gd_predict_y = gd_predict_y.tolist()</span><br><span class="line">    z_score_gd_predict_y = z_score_gd_predict_y.tolist()</span><br><span class="line">    mm_scaling_gd_predict_y = mm_scaling_gd_predict_y.tolist()</span><br><span class="line"></span><br><span class="line">    fig,axes = plt.subplots(nrows = <span class="number">2</span>,ncols = <span class="number">2</span>)</span><br><span class="line">    axes[<span class="number">0</span>,<span class="number">0</span>].scatter(house_id_x, test_set_y, label=<span class="string">"真实值"</span>,color=<span class="string">"red"</span>,s = <span class="number">13</span>)</span><br><span class="line">    axes[<span class="number">0</span>,<span class="number">0</span>].set(title=<span class="string">"正规方程组法"</span>)</span><br><span class="line">    axes[<span class="number">0</span>,<span class="number">0</span>].plot(house_id_x, ne_predict_y, label=<span class="string">"正规方程法"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    axes[<span class="number">0</span>,<span class="number">1</span>].set(title=<span class="string">"无归一化的梯度下降法"</span>)</span><br><span class="line">    axes[<span class="number">0</span>, <span class="number">1</span>].scatter(house_id_x, test_set_y, label=<span class="string">"真实值"</span>, color=<span class="string">"red"</span>,s = <span class="number">13</span>)</span><br><span class="line">    axes[<span class="number">0</span>,<span class="number">1</span>].plot(house_id_x,gd_predict_y,label = <span class="string">"梯度下降法"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].set(title=<span class="string">"z-score的梯度下降法"</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].scatter(house_id_x, test_set_y, label=<span class="string">"真实值"</span>, color=<span class="string">"red"</span>,s = <span class="number">13</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">0</span>].plot(house_id_x, z_score_gd_predict_y, label=<span class="string">"z-score梯度下降法"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].set(title=<span class="string">"max-min-scaling的梯度下降法"</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].scatter(house_id_x, test_set_y, label=<span class="string">"真实值"</span>, color=<span class="string">"red"</span>,s = <span class="number">13</span>)</span><br><span class="line">    axes[<span class="number">1</span>, <span class="number">1</span>].plot(house_id_x, mm_scaling_gd_predict_y, label=<span class="string">"max-min-scaling梯度下降法"</span>)</span><br><span class="line"></span><br><span class="line">    plt.xlabel(<span class="string">"房屋编号"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"预测价格1000$"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#生成训练集</span></span><br><span class="line">    feature_x = get_matrix_x(get_data_frame(<span class="string">r"..\dataset\FeatureX.csv"</span>))</span><br><span class="line">    feature_x = feature_x.A <span class="comment">#把特征值统一成ndarray类型方便处理</span></span><br><span class="line">    vector_y = get_vector_y(get_data_frame(<span class="string">r"..\dataset\ValueY.csv"</span>))</span><br><span class="line">    <span class="comment">#生成测试集</span></span><br><span class="line">    test_set_x = get_matrix_x(get_data_frame(<span class="string">r"..\dataset\testFeatureX.csv"</span>))</span><br><span class="line">    test_set_y = get_vector_y(get_data_frame(<span class="string">r"..\dataset\testValueY.csv"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    feature_x_copy = feature_x</span><br><span class="line"></span><br><span class="line">    <span class="comment">#梯度下降法(无归一化)的模型参数</span></span><br><span class="line">    theta_gd = gradient_decent(feature_x,vector_y,generate_init_theta(feature_x))</span><br><span class="line">    print(<span class="string">"梯度下降法(无归一化)的模型参数:"</span>+str(theta_gd.tolist()))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#正规方程组法的模型参数</span></span><br><span class="line">    theta_ne = normal_equation(feature_x,vector_y)</span><br><span class="line">    print(<span class="string">"正规方程组法的模型参数:"</span> + str(theta_ne.tolist()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># #均值归一化后的特征矩阵和向量 z-score法</span></span><br><span class="line">    z_score_feature_x= z_score(feature_x_copy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 均值归一化后的特征矩阵和向量 max-min-scaling法</span></span><br><span class="line">    mm_scaling_feature_x= max_min_scaling(feature_x)</span><br><span class="line">    <span class="comment">#梯度下降法(归一化后)的模型参数 max-min-scaling法</span></span><br><span class="line">    mm_scaling_theta_gd = gradient_decent(mm_scaling_feature_x, vector_y, generate_init_theta(feature_x),</span><br><span class="line">                                       alpha=<span class="number">1e-3</span>, iteration_times=<span class="number">125000</span>)</span><br><span class="line">    print(<span class="string">"梯度下降法(归一化后)的模型参数:(max-min-scaling法)"</span> + str(mm_scaling_theta_gd.tolist()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># #梯度下降法(归一化后)的模型参数 z-score法</span></span><br><span class="line">    z_score_theta_gd = gradient_decent(z_score_feature_x, vector_y, generate_init_theta(feature_x),</span><br><span class="line">                                        alpha=<span class="number">1e-3</span>, iteration_times=<span class="number">125000</span>)</span><br><span class="line">    <span class="comment"># print("梯度下降法(归一化后)的模型参数:(z-score法)" + str(z_score_theta_gd.tolist()))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#正规方程组的预测值</span></span><br><span class="line">    ne_predict_y = hypothesis_function(test_set_x,theta_ne)</span><br><span class="line">    <span class="comment">#梯度下降法(无归一化)的预测值</span></span><br><span class="line">    gd_predict_y = hypothesis_function(test_set_x,theta_gd)</span><br><span class="line">    <span class="comment">#梯度下降法(归一化后)的预测值 z-score法</span></span><br><span class="line">    z_score_gd_predict_y = hypothesis_function(z_score(test_set_x),z_score_theta_gd)</span><br><span class="line">    <span class="comment"># 梯度下降法(归一化后)的预测值 max-min-scaling法</span></span><br><span class="line">    mm_scaling_gd_predict_y = hypothesis_function(max_min_scaling(test_set_x), mm_scaling_theta_gd)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    draw_picture(test_set_x,test_set_y,ne_predict_y,gd_predict_y,z_score_gd_predict_y,mm_scaling_gd_predict_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>



<h4 id="0x02-实验结果及可视化展示"><a href="#0x02-实验结果及可视化展示" class="headerlink" title="0x02 实验结果及可视化展示"></a>0x02 实验结果及可视化展示</h4><p><img src="https://images.gitee.com/uploads/images/2020/0724/160141_e15ceb7a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>据网上的资料显示，当我的样本服从正态分布的时候 z-score归一化的效果是最好的。从上面代码中可以看出<strong>归一化对机器学习算法起着重要的作用</strong></p>
<p>普通的不进行任何归一化的梯度下降算法的参数：</p>
<p>alpha = 1e-7,iteration_times = 550000</p>
<p>两种归一化后的梯度下降算法的参数：</p>
<p>alpha=1e-3, iteration_times=125000</p>
<p>这些参数我还只是粗调，具体差异可能会更大。</p>
<p>这也证实了我的两个实验目的，<strong>归一化对算法的影响明显</strong>，<strong>不同归一化算法对同一数据集效果不同</strong></p>
<br>

<h3 id="9-3-踩到的坑"><a href="#9-3-踩到的坑" class="headerlink" title="9.3 踩到的坑"></a>9.3 踩到的坑</h3><h4 id="0x00-多元梯度下降的向量化"><a href="#0x00-多元梯度下降的向量化" class="headerlink" title="0x00 多元梯度下降的向量化"></a>0x00 多元梯度下降的向量化</h4><p>当时鄙人为了给多元梯度下降向量化真是煞费苦心。</p>
<p>推导过程 (字很难看，请见谅)：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0724/160407_f4d0bedd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h4 id="0x01-均值归一化"><a href="#0x01-均值归一化" class="headerlink" title="0x01 均值归一化"></a>0x01 均值归一化</h4><p>我一共犯了2个错误。</p>
<p>第一个错误，把真实值（标签）当作特征，将特征矩阵和真实值向量同时进行归一化，导致出错。</p>
<p>第二个错误，进行测试时，没有进行归一化就开始预测结果。我们归一化后训练出的模型参数，是基于归一化后的特征值训练出来的，而且特征值的取值范围是被约束的。而我不把测试集进行归一化，那么这个预测结果当然是不对的。</p>
<br>

<h2 id="Part-10-神经网络-新的征程"><a href="#Part-10-神经网络-新的征程" class="headerlink" title="Part 10 神经网络-新的征程"></a>Part 10 神经网络-新的征程</h2><h3 id="10-1-神经网络的引出"><a href="#10-1-神经网络的引出" class="headerlink" title="10.1 神经网络的引出"></a>10.1 神经网络的引出</h3><p>我们之前学习的机器学习算法，都是一般采用的线性函数作为假设函数，为了有一个直观的体验，我们来看一个分类的例子</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/111659_d12cff34_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们采用高阶的线性多项式代入sigmoid函数，以求得到好的分类效果</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/112410_374589d4_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当整个算法只有2个特征值的时候，我们这样做高阶多项式看起来还不是很麻烦，但我们假设，这个分类问题是分出房子的类型（别墅，平房，贫民窟，……）那我们需要的特征值可能有：</p>
<blockquote>
<p>房屋面积</p>
<p>卧室个数</p>
<p>卫生间数量</p>
<p>供水平均时间</p>
<p>……</p>
</blockquote>
<p>如果，它有100个特征值，那么我们做出二次方的排列组合就是O(n^2^)量级，三次方就是O(n^3^)，从个数上看二次方的排列组合大概有5000种，三次方的排列组合大概有17000项。这种规模的数据计算机能不能算的出来暂且不论。我们看一个计算机视觉的例子。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/113301_7b77bbe3_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>计算机视觉的主要工作就是教电脑识别图像，比如，我们给电脑提供的是一个 50 × 50 像素点的灰度图像（0，255），这个算法需要根据提供的像素亮度矩阵，告诉我们这是不是<strong>一个车的门把手</strong></p>
<p>了解了大致原理之后，现在让我们看一个汽车识别的例子</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/113652_3ba93e1f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们首先有一个带标签的数据集，用来辨别它是不是一辆车，然后还有一个训练集，我们最后需要根据训练出的模型来判断输入图片是不是一辆车。</p>
<p>假设我们取2个50×50像素的灰度图像最为特征值</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/114130_41d3f5fd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/114203_ed8dbdc6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们的目的就是训练出一条曲线来拟合这个图像</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0725/114237_cff5cccf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>一个特征向量 x 的维数在灰度图像的表示就是2500</p>
<p>像x<del>i</del> 和 x<del>j</del> 的二次排列组合大概，需要3百万种吧……这电脑CPU肯定算不过来，而神经网络就是为解决这种问题诞生的。</p>
<h3 id="10-2-神经元与大脑"><a href="#10-2-神经元与大脑" class="headerlink" title="10.2 神经元与大脑"></a>10.2 神经元与大脑</h3><p>程序拟真的终极目标就是做出人工大脑，实现真正意义上的人工智能。在目前的计算机科学中，我们如何模拟脑组织结构呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/103340_5721abc5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>上图是脑组织中的神经元，它有输入用的树突，和输出用的轴突。神经元之间通过神经递质，造成电位差进而进行信息传递【我的生物水平仅停留在高中的生物课的时期】</p>
<p>计算机中的神经元的逻辑模型：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/103747_08fcca36_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>黄色颜色圆圈左侧是代表输入，黄色圆圈右侧为输出</p>
<p>当然了，上图中没有化出x<del>0</del>,因为它恒为1，在神经网络中我们称它为<strong>偏置单元（Bias Unit）</strong></p>
<p>这一个系统也可以统称成<strong>感知机 （Perceptron）</strong></p>
<p>总结一下就是，感知机（人工神经元）接收多个输入，输出1个信号，输入值会经过一个<strong>非线性函数【激活函数（activation function）】</strong>,最终决定其输出值。</p>
<p>在次之后，关于参数theta的模型也可能会称之为<strong>权重(weights)</strong>为theta的模型。</p>
<h3 id="10-3-神经网络结构"><a href="#10-3-神经网络结构" class="headerlink" title="10.3 神经网络结构"></a>10.3 神经网络结构</h3><p>神经网络就是把多个感知机按层叠加，连接在一起的集合</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/105015_5152e362_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>也可以画出x<del>0</del></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/105129_c00cb1f7_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>在神经网络中，第一层叫<strong>输入层(Input Layer)</strong>，最后一层为<strong>输出层(Output Layer)</strong>，中间的层为<strong>隐藏层(Hidden Layer)</strong>。</p>
<p>输入层输入特征列向量（1xn）,隐藏层计算权重矩阵theta，输出层输出结果。</p>
<p>我们在来看一下下图一个简单的神经网络中的计算过程，即神经网络在干什么：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/111952_e8b05490_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先明确术语及符号：<br>$$<br>a_i^{(j)}: 代表第 j 层的第i个感知机(人工神经元)<br>\Θ^{(j)}:为从第j层到第j+1层的权重矩阵(模型参数矩阵Θ)<br>$$<br>层数是从左向右，从一开始看的，<strong>原来的模型参数从向量变成了矩阵</strong></p>
<p><img src="C:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200731115700887.png" alt="image-20200731115700887"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/114323_8dae70c5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>以上就是对上述神经网络的计算流程，看不懂吧，第一眼看不懂就对了，我来给大家掰扯掰扯。</p>
<p>首先，毋庸置疑的，x<del>0</del> = 1</p>
<p>其次，我们的模型参数由<strong>向量变成了一个矩阵</strong></p>
<p>看输入层，我们输入的就是一个(1x4)的特征列向量，对于第二层的第一个神经元来说，它需要计算<strong>从第1层到第2层(这里直接是输出层)权重矩阵的第一行的线性运算再带入Sigmoid函数</strong></p>
<p>说人话就是，<strong>把第一层的权重矩阵的第一行拿出来，和每个特征值做线性运算，再带入激活函数Sigmoid函数中</strong></p>
<p>这样易得，我们的模型参数是一个(4x3)的矩阵【4个特征值，1层神经网有3个感知机】</p>
<p>那么第一层输出的就是一个(1x3)的列向量，再输入到输出层，列向量通过输出函数输出一个值</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/120352_01633a67_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>然后我们还有一个简单的数量关系，即：</p>
<p>$\color{red}{如果网络中有 s_j 个神经元在j层，s_{j+1} 个神经元在j + 1层，那么Θ^{(j)}的维度为 s_{j+1} × (s_j+1)}$</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0731/121145_6845c449_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>比如，上图中有输出层(第0层)有3个特征向量(不包含x<del>0</del>)，第2层有4个神经元，那么Θ^（1）^的维数就是 4 × (3+1) = 4 × 4</p>
<h3 id="10-4-神经网络向量化"><a href="#10-4-神经网络向量化" class="headerlink" title="10.4 神经网络向量化"></a>10.4 神经网络向量化</h3><p><img src="https://images.gitee.com/uploads/images/2020/0805/143329_dc614068_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>为了高效地进行计算，我们要对上面这个式子进行向量化<br>$$<br>设:z^{(2)}<em>1 = Θ^{(1)}</em>{10}x_0 + Θ^{(1)}<em>{11}x_1 +Θ^{(1)}</em>{12}x_2 +Θ^{(1)}<em>{13}x_3\<br>z^{(2)} = \left[<br>\begin{array}{lcr}<br>z_1^{(2)}\<br>z_2^{(2)}\<br>z_3^{(2)}<br>\end{array}<br>\right]<br>\同时将：Θ^{(1)}</em>{10}x_0 + Θ^{(1)}<em>{11}x_1 +Θ^{(1)}</em>{12}x_2 +Θ^{(1)}<em>{13}x_3和之后的若干项看作:<br>\<br>x = \left[<br>\begin{array}{lcr}<br>x_0\<br>x_1\<br>x_2\<br>x_3<br>\end{array}<br>\right]<br>Θ^{(1)} = \left[<br>\begin{array}{lcr}<br>Θ^{(1)}</em>{10}\quad Θ^{(1)}<em>{11}\quadΘ^{(1)}</em>{12}\quadΘ^{(1)}<em>{13}\<br>Θ^{(1)}</em>{20}\quad Θ^{(1)}<em>{21}\quadΘ^{(1)}</em>{22}\quadΘ^{(1)}<em>{23}\<br>Θ^{(1)}</em>{30}\quad Θ^{(1)}<em>{31}\quadΘ^{(1)}</em>{32}\quadΘ^{(1)}_{33}\<br>\end{array}<br>\right]<br>\<br>Θ^{(1)}x<br>$$<br>综上我们可以得出：<br>$$<br>z^{(2)} =Θ^{(1)}x\<br>a^{(2)} = g(z^{(2)})<br>$$</p>
<p>把第二层的神经元统一看成一个向量，它是由第一层的输入决定的，为了方便理解，这里可以把<br>$$<br>x 当作 a^{(1)}<br>$$</p>
<p>那么上述的整个计算流程可以用下图表示：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0805/145912_4532e951_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>从输入层开始到最后的输出层，我们是一层一层向后迭代的，所以把这个过程称为<strong>正向传播（forward propagation）</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0805/150153_f66c7674_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>单看这一层的输出，它就是一个单纯的逻辑回归，只不过它的特征矩阵不再是我们给它的了，而是由它的上一层计算得到的</p>
<p>整个神经网络看上去就是一个大的逻辑回归，但是所有的参数都是有它自己学习得出的。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0805/150456_5d5c3ebc_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>通过这样的学习可以使计算机跨过由于底层表示和现实世界的数据维度差距过大所导致的<strong>语义鸿沟</strong>。</p>
<p>最后讲一个有关神经网络<strong>架构(architecture)</strong>的概念</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0805/150900_fe1ae2b9_5550632.png" alt=""></p>
<p>神经网络的架构指的是不同层神经元之间的连接方式。</p>
<h5 id="通过例子进行深入理解"><a href="#通过例子进行深入理解" class="headerlink" title="通过例子进行深入理解"></a>通过例子进行深入理解</h5><h6 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h6><p>在举例之前我们重温一下Sigmoid函数的图像:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/150350_c9fcd06b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<p><strong>使用单一神经元模拟 AND</strong></p>
<p>我们的需求是使用神经元来模拟 AND 信号。真值表如下:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/150824_62a67bea_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>假设参数为20，20。偏置单元为 -30，如下图，就可以将这些值看成参数矩阵(本例中为向量)</p>
<p>$$<br>-30 =&gt; \theta^{(1)}<em>{10} \<br>20 =&gt; \theta^{(1)}</em>{11} \<br>20 =&gt; \theta^{(1)}_{12}<br>$$</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/150550_ca96f98e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当参数去上述值时，我们假设函数的输出和AND运算的结果是近似相同的。</p>
<br>

<p><strong>使用单一神经元模拟 OR</strong></p>
<p>同理，OR的真值表如下:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/151444_1aba2222_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>设计参数和偏置的值为：<strong>20，20，-10</strong></p>
<p>假设函数内部的值：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/151821_e47a3ed6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<p><strong>使用单一神经元模拟 NOT</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/152041_6fea7511_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<p><strong>使用单一神经元模拟 (NOT x_1) AND (NOT x_2)</strong></p>
<p>真值表如下:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/152438_3376c567_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>设计参数和偏置:<strong>-30,-30,20</strong></p>
<p>最终结果:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/152725_9b82655a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<p><strong>综上实现 XNOR</strong></p>
<p>首先将上述三种神经元画出来（参数的选择情况不止一种）:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/152929_cea72ca6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>XNOR的真值表:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/153123_bf7a7763_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/154540_b4af671e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>计算过程:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/154927_e7020f4e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/155011_1b346198_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h3 id="10-5-多元分类问题"><a href="#10-5-多元分类问题" class="headerlink" title="10.5 多元分类问题"></a>10.5 多元分类问题</h3><p>接下来我们将举出一个计算机视觉的例子，来学习多元分类问题</p>
<h4 id="10-5-1-问题概述"><a href="#10-5-1-问题概述" class="headerlink" title="10.5.1 问题概述"></a>10.5.1 问题概述</h4><p><img src="https://images.gitee.com/uploads/images/2020/0923/155554_99f22659_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>给出一组图片，让计算机去判别该图片中是<strong>行人，汽车，摩托车，卡车</strong>中的哪一类。很显然这是一个4元分类问题，所以我们的输出层会有4个单元。输出的结果是一个4维向量，我们可以做出如下假设:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0923/155912_7a748835_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当训练集图片为行人时，标记为 [1 0 0 0]^T^，训练集的标签由原来的实数，变成了向量。</p>
<h4 id="10-5-2-代价函数"><a href="#10-5-2-代价函数" class="headerlink" title="10.5.2 代价函数"></a>10.5.2 代价函数</h4><p>在讲多元分类问题的代价函数之前，我们需要明确一下需求。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0929/094130_24a25676_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先我们的训练集记作:</p>
<p>$$<br>{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}),……,(x^{(m)},y^{(m)})}<br>$$<br>L :代表了神经网络的总层数</p>
<p>s<del>l</del> : 第 l 层的神经元个数</p>
<p>在原来的二分类问题中，我们可以把它看作输出层只有一个神经元的神经网络，所以我们可以将它的代价函数写为:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0929/100620_e35d3265_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>$$<br>K=1\<br>J(\theta)=-\frac{1}{m}[\sum_{i=1}^m\sum_{k=1}^Ky^{(i)}<em>klog;h_\theta(x^{(i)})_k+(1-y^{(i)}_k)log;h_\theta(1-x^{(i)})_k] + \frac{\lambda}{2m}\sum</em>{l=1}^L\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta^{(l)}_{ji})^2<br>$$</p>
<p>当K&gt;1时:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/0929/101026_875cc28c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<h4 id="10-5-3-反向传播算法"><a href="#10-5-3-反向传播算法" class="headerlink" title="10.5.3 反向传播算法"></a>10.5.3 反向传播算法</h4><p>现在，我们拥有了假设函数，代价函数，那么只需要对代价函数进行梯度下降，我们就可以求出每层神经网之间的权重矩阵了。然而，梯度下降中有关于参数θ的偏导数。我们在神经网络中应该如何计算这些偏导数呢？</p>
<p>我们先假设一个简单的例子，看看计算过程：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1029/192044_736c4a32_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>假设我们的数据集中就一组数据(x,y)，应用前向传播的计算流程为上图所示。接着为了能算出每层之间的参数矩阵Θ，我们需要对代价函数进行梯度下降，其中所计算的偏导项就要使用反向传播算法了。</p>
<h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p><img src="https://images.gitee.com/uploads/images/2020/1029/192522_86c642b6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>定义：δ<del>j</del>^(l)^ 为第 l 层中第 j 个结点的误差。举个例子：</p>
<p>对于最后一层，我们将其向量化后可以看作 δ^(4)^ = a^(4)^ - y，那么我们就可以接着向前推导<br>$$<br>\delta^{(3)} = (Θ^{(3)})^T\delta^{(4)}.dot(g^{,}(z^{(3)}))\<br>\delta^{(2)} = (Θ^{(2)})^T\delta^{(3)}.dot(g^{,}(z^{(2)}))<br>$$<br>其中z^j^ 为第j层神经网络的输出值，由于从输入层进入是我们所观测到的数据所以只计算到δ^(2)^</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1029/192911_30d6594c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最终经过及其复杂的推导证明我们得出了这样一个结论：<br>$$<br>\left.\frac{\partial J(\Theta)}{\partial \Theta_{ij}^{(l)}}\right. = a^{(l)}_j*\delta^{(l+1)}_i<br>$$</p>
<h5 id="深入理解反向传播算法"><a href="#深入理解反向传播算法" class="headerlink" title="深入理解反向传播算法"></a>深入理解反向传播算法</h5><p>反向传播算法在机器学习的学习过程中是一个坎儿，接下来我要做的工作就是，将前向传播，反向传播算法细化推演，争取在宏观的角度给大家带来启发。我们首先来看前向传播算法。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1105/193658_df3785ec_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>假设有一个如上图的神经网络，将其故意画成椭圆形以便进行标注。那它的前向传播算法其实如下：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1105/195632_8b13ece3_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>所以，我们不难看出每个神经元的输入部分为，前一项的值和前一层的权重矩阵的线性组合，而神经元的输出部分则是，对该线性组合的结果的sigmoid函数值。这就是前向传播的计算流程。</p>
<p>想进行前向传播，那就必须知道<strong>每一层神经网之间的权重矩阵</strong>，在线性回归中，我们直接对代价函数进行梯度下降，然后得出模型参数。类比的，我们要对神经网络的<strong>代价函数进行梯度下降</strong>，神经网络中的代价函数长这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1105/200216_aab79a1d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>它的作用和线性回归中的均方根误差公式的作用是一样的。而反向传播算法就是在计算<strong>每一层神经网络的每个激活函数输出值的偏差</strong>，我们来详细地看一下它的计算流程：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1105/201756_f7b01eac_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>在数学上的含义如下：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1105/201902_af4d2a8a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h4 id="10-5-4-梯度检验-Gradient-Checking"><a href="#10-5-4-梯度检验-Gradient-Checking" class="headerlink" title="10.5.4 梯度检验 (Gradient Checking)"></a>10.5.4 梯度检验 (Gradient Checking)</h4><p>梯度检验用于评估在运行繁复的反向传播算法后<strong>是否得到了正确的偏导数值</strong>。有可能当你辛辛苦苦训练完一个神经网络之后，却发现由于<strong>反向传播算法的实现出现了失误</strong>，导致发生了难以察觉的细小Bug，导致最终的模型结果不准确。为了能够尽早地发现问题，我们来介绍一种梯度检验的方法，用于评估使用反向传播算法运算结果所得到的<strong>偏导数值</strong>。</p>
<p>我们是如下进行导数估计的</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/144608_d28be25c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>函数在点θ处的切线斜率为导数的几何意义，我们取距离θ较近的对称位置θ-e，θ+e，当e足够小时，该红色直线的斜率就是θ点处的导数值。一般我们取e为10e-4，此时θ的导数如下式:</p>
<p>$$<br>\frac{d}{d_\theta}J(\theta) ≈ \frac{J(\theta+e)-J(\theta-e)}{2e}<br>$$<br>(双侧差分)</p>
<p>当θ展开变成向量时我们可以如下计算：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/145714_bc96224b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>附上Octave的代码：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/145857_34c34a14_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>注意：我们在训练模型时（应用反向传播算法时），一定要关闭掉梯度检测算法，否则电脑会占用大量计算资源。</p>
<h4 id="10-5-5-随机初始化"><a href="#10-5-5-随机初始化" class="headerlink" title="10.5.5 随机初始化"></a>10.5.5 随机初始化</h4><p>我们在之前进行线性回归的实验时选择的是向量全为1 的初始化方式，而在逻辑回归中我们可以使用全零的初始化方式，但是这种方式应用在神经网络中就显得没什么意义了，原因如下：</p>
<p><img src="C:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201130152931236.png" alt="image-20201130152931236"></p>
<p>如果参数矩阵的值都相等，就会导致最终所计算出的激活函数值是相同的，它们再经过反向传播算法后得到的的偏导数也相同，梯度下降之后，所得到的参数矩阵的值虽然不是零，但是依旧相等。相当于整个神经网络就是在计算一个相同的特征，不仅浪费性能，而且还无法得到准确的结果。此时我们就要对初始化的参数矩阵做<strong>随机</strong>处理，如下所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/153526_5c4a6c7a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>rand(10,11)可以生成一个Matrix(10*11),基于上图中的公式，我们就可以将所有的θ矩阵初始化到一个[-a,a]的范围中了。</p>
<h4 id="10-5-6-组合到一起"><a href="#10-5-6-组合到一起" class="headerlink" title="10.5.6 组合到一起"></a>10.5.6 组合到一起</h4><p>本节来综合上述所讲的有关于神经网路的知识点，同时说明自己训练模型时的过程。</p>
<p><strong>Step 1选择神经网络的架构</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/173336_6da22807_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这一步主要要明确的是，<strong>输入的特征向量维度</strong>和最终神经网络<strong>输出的输出单元</strong>，同时选择隐藏层的数量和神经元的个数。当存在多层隐藏层时，在目前阶段我们推荐选择的神经元个数都和前一个隐藏层相同。（通常情况下，隐藏层越多越好）</p>
<p><strong>Step 2 随机初始化参数</strong></p>
<p>如同其字面意思，针对之前所选择的神经网络架构，随机初始化每层神经网间的权重矩阵。</p>
<p><strong>Step 3 实现前向传播对于每个x^(i)^得到其假设函数h_θ_(x)</strong></p>
<p><strong>Step 4 实现代价函数 J(θ) 的代码</strong></p>
<p><strong>Step 5 实现反向传播算法，计算偏导数</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/174945_08d481b1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>大佬强烈建议在初次实现前向和反向传播算法时，使用for循环来实现</p>
<p><strong>Step 6 使用梯度检测算法来验证第一次用反向传播算法计算出的偏导数值</strong></p>
<p>之后，$\color{red}{停止梯度检测程序}$</p>
<p><strong>Step 7 使用梯度下降或其它高级优化算法来对 J(θ)[关于参数θ的函数] 做最小化</strong></p>
<h5 id="直观理解梯度下降在神经网络中的作用"><a href="#直观理解梯度下降在神经网络中的作用" class="headerlink" title="直观理解梯度下降在神经网络中的作用"></a>直观理解梯度下降在神经网络中的作用</h5><p><img src="https://images.gitee.com/uploads/images/2020/1130/175734_008ec2e6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>上图为<em>代价函数在第 l 层神经网络中，从权重矩阵中仅选择两个参数的可视化假设图</em>，说人话就是，我们画了一个代价函数在不同参数取值时的函数值。由于我们只能理解3维图像，这里就取了两个模型参数作为函数的输入，实际上它是n维的。图像中的每个点代表<strong>一组参数所对应的代价函数值</strong>，反向传播的作用相当于<strong>选择向哪个方向做梯度下降</strong>，梯度下降的作用就是<strong>负责在这个选定方向上下降多少</strong>，最终我们可以得到一个局部最小值。使得：</p>
<p>$$<br>h_\theta(x^{(i)}) ≈ y^{(i)}<br>$$</p>
<p>只不过我们此时的假设函数的表示形式为神经网络中每一层的线性组合。</p>
<br>

<h2 id="Part-11-机器学习诊断法-Machine-Learning-diagnostic"><a href="#Part-11-机器学习诊断法-Machine-Learning-diagnostic" class="headerlink" title="Part 11 机器学习诊断法 (Machine Learning diagnostic)"></a>Part 11 机器学习诊断法 (Machine Learning diagnostic)</h2><p>在完成一个机器学习项目时，如何评估算法的性能好坏，如果发现效果不好，我们应当如何改进自己的算法模型，以房价预测为例，我们有如下的处理办法：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/181648_eacc35bf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<blockquote>
<p>获得更大的数据集</p>
<p>使用更少的特征值</p>
<p>试图增加新的特征值</p>
<p>试图在假设函数中加入多项式</p>
<p>增加λ</p>
<p>减小λ</p>
</blockquote>
<p>有没有一种简单有效的方法来选择上述优化方法从而避免浪费不必要的时间，避免不必要的努力呢？这章我们要讲的内容就是这个，在此之后我会通过这一套方法来实现基于神经网络的项目（又到了喜闻乐见手撸机器学习源码的时候呢）。</p>
<h3 id="11-1-评估假设函数"><a href="#11-1-评估假设函数" class="headerlink" title="11.1 评估假设函数"></a>11.1 评估假设函数</h3><p>在对假设函数进行评估前，我们一般采用一种标准化的流程，实际上，我在之前的房价预测案例中已经无形的使用了这种方式。</p>
<blockquote>
<ol>
<li>分割数据集</li>
<li>对测试集运行代价函数，以便评估</li>
</ol>
</blockquote>
<p>一般我们习惯将数据集分为<strong>70%训练集，30%测试集</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/183132_1e224cfc_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>像图中红色方框内的数据就是测试集的第一组数据，并记下标test。如果数据集中的数据不是随机排列的，那么建议我们随机选择70%的数据作为训练集。之后，就是将使用测试集训练出的参数代到代价函数中（如果是线性回归是这样的），计算<strong>测试集的代价函数值</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/183526_0b9575b8_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如果是逻辑回归，则同理：</p>
<p>$$<br>J_{test}(\theta)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}y_{test}^{(i)}log;h_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)})log;h_\theta(1-x_{test}^{(i)})<br>$$<br>如果是0-1分类问题，我们就设计一个误差函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1130/184216_dc1468b6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>它的含义就是：当假设函数输出值大于0.5 ，但真实值为负类，或者当分类为负类时实际为正类时，我们将其置为1（就是分类分错的时候）</p>
<p>分类分正确了，函数值为 0。</p>
<p>最后，我们对测试集的误差估计就是:</p>
<p>$$<br>Err_{test} = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_{\theta{test}}(x^{(i)}),y_{test}^{(i)})<br>$$</p>
<h3 id="11-2-模型选择问题"><a href="#11-2-模型选择问题" class="headerlink" title="11.2 模型选择问题"></a>11.2 模型选择问题</h3><p>我们现在依旧假设有一个房价预测的问题，它有如下10个假设函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/132645_6ee08e5e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>d为多项式的最高次方。</p>
<p>按照我们之前的做法，我们会将数据集一分为二，用训练集训练模型，用测试集测试模型，<strong>并选出代价函数值最小的那个多项式为假设函数</strong>。但其实这样做是错误的，这样算出来的代价函数值不能代表该模型在其它数据集上的<strong>泛化能力</strong>。什么是泛化能力？其实就是模型由于过拟合导致无法适配其它数据的现象。下图为上述方法选择模型的可视化流程：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/133019_1fba0da8_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>计算代价函数值本身没错，只不过是数据选择的问题，所以最终我们决定采用如下的数据集分配方式：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/133242_f9da934a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们将数据集分为<strong>60%训练集，20%交叉验证集(Cross Validation Set)，20%测试集</strong></p>
<p>我们依旧采用相同的记法记录数据</p>
<p>$$<br>x_{cv}^{(i)},y_{cv}^{(i)}<br>$$<br>随后我们用<strong>交叉验证集的代价函数值来评估模型的好坏</strong>，选择最小的模型之后，我们使用测试集<strong>最终测试数据集的泛化能力</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/133537_ded7cb9b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/133737_a411e9a5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>还是之前的场景，假设四次多项式的交叉测试集的代价函数值最小，我们选择<em>d= 4</em>的模型，并评估其在测试集上的代价函数值<strong>(泛化误差值)</strong>。</p>
<br>

<h3 id="11-3-诊断偏差与方差"><a href="#11-3-诊断偏差与方差" class="headerlink" title="11.3 诊断偏差与方差"></a>11.3 诊断偏差与方差</h3><h4 id="11-3-1-偏差与方差"><a href="#11-3-1-偏差与方差" class="headerlink" title="11.3.1 偏差与方差"></a>11.3.1 偏差与方差</h4><p>在机器学习算法中，一个模型只会出现两种情况，<strong>过拟合和欠拟合</strong>的问题，那我们如何来评估一个机器学习算法是过拟合还是欠拟合呢？一般来说$\color{red}{欠拟合就是出现了偏差问题，而过拟合就是出现了方差问题}$。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/135423_d959e62b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这张图我们可熟悉了，左侧第一张就是由于采用了低次函数拟合数据集导致发生了<strong>欠拟合</strong>的现象，最右侧的图像就是我们熟悉的<strong>过拟合</strong>图像了。我们来画一画</p>
<p>$$<br>J_{train}(\theta)\<br>J_{cv/test}(\theta)<br>$$<br>这两个关于误差的图像吧。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/135804_a344ce8a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>$\color{purple}{粉紫色}$：代表训练集的代价函数值（函数值越小，误差越小）</p>
<p>$\color{red}{红色：}$代表交叉验证集或者测试集的代价函数值</p>
<p>随着多项式的次数增加当然训练集的代价函数值会<strong>减小</strong>，甚至会到 0 ，但是由于过拟合数据集，会导致无法很好的拟合交叉验证集的数据，就会导致<strong>交叉验证集的代价函数值飙升</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1201/140513_3ee999e1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>一般而言，当</p>
<p>$$<br>J_{train}(\theta);很大; \<br>;J_{cv}(\theta)≈J_{train}(\theta)<br>$$</p>
<p>时，我们认为算法出现了偏差问题，即欠拟合问题，<strong>需要调高多项式的最高次数</strong></p>
<p>当</p>
<p>$$<br>J_{train}(\theta);很小; \<br>;J_{cv}(\theta)&gt;&gt;J_{train}(\theta)<br>$$<br>时，我们认为算法出现了方差问题，即过拟合问题，<strong>需要降低多项式的最高次数</strong></p>
<h4 id="11-3-2-正则化和偏差与方差的关系"><a href="#11-3-2-正则化和偏差与方差的关系" class="headerlink" title="11.3.2 正则化和偏差与方差的关系"></a>11.3.2 正则化和偏差与方差的关系</h4><p>可能大家已经忘记了什么是正则化，正则化就是用于<strong>梯度下降时所添加的那个惩罚项</strong>。如同下述公式中的那个小“尾巴”</p>
<p>$$<br>J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 + \frac{λ}{2m}\sum_{j=1}^{m}\theta_j^2<br>$$</p>
<p><strong>※注意区分概念，代价函数$\color{red}{用于评估模型拟合结果的好坏}$，带有正则化项的代价函数则是$\color{red}{用来训练模型，作为优化对象应用于梯度下降算法的(梯度下降时用带正则化项的代价函数，评估误差时去掉正则项)}$</strong>。</p>
<p>我们看一看，正则化参数λ过大过小会引发哪些问题：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1202/185041_fa58f10a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如上图所示，<strong>当λ过大</strong>，导致惩罚过大，梯度下降时值减的太厉害，导致所有的模型参数均较小，最后的假设函数就像是一条常函数。其结果就是<strong>欠拟合</strong>。</p>
<p>如果<strong>λ过小</strong>，导致梯度下降时，梯度下降得很慢，最终模型过拟合数据集。只有挑选一个适中的λ，模型效果才是最优的。在训练模型时，我们选择带有正则项的代价函数做梯度下降，在<strong>挑选，评估模型</strong>时，我们计算的训练集的代价函数，交叉验证集的代价函数以及训练集的代价函数均<strong>不带正则项</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1202/185620_6d555777_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>随后，我们假设若干λ的值，对 J(θ) 进行梯度下降，得到一系列模型参数，如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1202/185928_433dcc25_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>随后我们用交叉验证集测试，找出代价函数最小的λ参数，假设这里我们找到的是<strong>λ = 0.08</strong>，最后我们用测试集测试这个模型，看看测试集下的代价函数值。如果将λ和代价函数值一起可视化，可以得到如下图像（理想状况）：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1202/190654_7c83f6b2_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当λ取值较小时，蓝色的训练集代价函数值会很小，因为很可能出现<strong>方差问题，导致过拟合</strong>，当λ较大时，对每个模型参数的惩罚都相对较大，很可能出现<strong>偏差问题，导致欠拟合</strong>。而交叉验证集则相反，当λ很小时，模型过拟合，导致效果不好，代价函数值大，当λ很大时，模型几乎不拟合数据集，效果不好，出现偏差问题，代价函数值大。</p>
<h3 id="11-4-学习曲线-Learning-curves"><a href="#11-4-学习曲线-Learning-curves" class="headerlink" title="11.4 学习曲线 (Learning curves)"></a>11.4 学习曲线 (Learning curves)</h3><p>通过综合上面所学，我们可以得到一种判断模型处于哪种问题下的方法，它就是<strong>学习曲线</strong>。我们先看一下学习曲线是怎么来的：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1203/181130_4745e453_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>在选定一个模型后</strong>，我们记<strong>实际所使用的数据集数据条数为m</strong>，如上图，我们选定的假设函数为</p>
<p>$$<br>h_\theta(x) = \theta_0+\theta_1x+\theta_2x^2<br>$$<br>这样的二次函数，用一条数据训练模型，当然能很好地拟合数据，就算使用正则化，那最后训练集上的代价函数值也不会很大，随后我们增加训练模型的数据数目，会发现随着数据量的增大，假设函数越来越难来拟合数据集，那么训练集的代价函数值会逐步上升。</p>
<p>我们反观，交叉验证集或者是测试集的代价函数值。由于刚开始我们使用一条数据进行训练，当然模型的泛化能力差，代价函数值高。随着训练集规模的增加，假设函数拟合的相对较好，交叉验证的代价函数值就会缓慢下降。</p>
<p>换句话说，学习曲线可以<strong>评估训练集规模对同一模型的两种不同代价函数值(训练集下的代价函数值，交叉验证集下的代价函数值)</strong>，进而可以用于评估模型是出现了偏差问题，还是方差问题。</p>
<p>我们先看看在高偏差情况下的学习曲线：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1203/182215_5f2676ee_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们选取一次函数作为假设函数（一条直线）。在训练集较小时，如m=1,2时，当然用直线拟合没有问题，所以应用训练集的模型的代价函数很小，随着训练集的增加，直线还是直线，无论再怎么扩大数据集，基本上代价函数值都不会发生改变了，如同上图紫色图像。同理，应用交叉验证集的模型随着数据量的增大，会逐步减小。最后就给大家呈现出了一种<strong>J<del>cv</del>(θ) 和 J<del>train</del>(θ)最终持平的效果</strong>，</p>
<p>同时我们会发现，在高偏差情况下，<strong>增加数据集的数目是没有帮助的</strong>。我们再看看高方差下的学习曲线：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1203/182952_d893c33c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>由于采用高阶多项式拟合数据集，最终会过拟合，随着数据集的增加，当然 J_train(θ)会增加，因为拟合的难度会增加，而且由于过拟合，导致模型泛化能力差，J_cv(θ)也居高不下，图像开始会呈现出一个<strong>差距gap</strong>，<strong>然后随着训练集数据的增加而有所改善，最终两条曲线趋于平稳</strong>。</p>
<p>那么一个比较理想的学习曲线就应该是<strong>低方差，低偏差，收敛且误差值小</strong>，学习曲线如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/161936_637454fd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那么我们回到之前所说的房价预测例子上：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/132133_0977ea56_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>之前我们给出了若干种解决方案，如果我们画出了学习曲线，我们就能明确地选择出对应问题的解决方法。</p>
<p>如果是<strong>高方差问题 (high variance)</strong>，我们可以：</p>
<ol>
<li>增加训练集的规模</li>
<li>减少特征向量数目</li>
<li>增加正则化参数λ(λ越小，惩罚代价越小越容易出现过拟合的现象)</li>
</ol>
<p>如果是<strong>高偏差问题 (high bias)</strong>,我们可以：</p>
<ol>
<li>添加新的特征向量</li>
<li>使用更高阶的多项式</li>
<li>减小正则化参数λ</li>
</ol>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/132918_b7d9d2d7_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>一般而言那种“小型”的神经网络（参数和隐藏层数目都少），参数更少，更可能出现欠拟合的问题，但是胜在计算量小</p>
<p>而那种“大型”的神经网络，往往性能更好，但是可能会出现过拟合的情况，我们一般可以采用调整正则化参数的方式来改善模型质量。</p>
<p>当我们不知道应该使用多少层隐藏层时，我们可以通过<strong>计算不同数目隐藏层下的 J<del>cv</del>(θ) 来评估选择什么样的模型</strong>。</p>
<br>

<h2 id="Part-12-机器学习系统设计"><a href="#Part-12-机器学习系统设计" class="headerlink" title="Part 12 机器学习系统设计"></a>Part 12 机器学习系统设计</h2><h3 id="12-1-情景引入"><a href="#12-1-情景引入" class="headerlink" title="12.1 情景引入"></a>12.1 情景引入</h3><p>假设现在我们要做一个垃圾邮件分类器：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/134642_9b2b2bb9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>左子图是一封垃圾邮件，右面的不是。我们如何训练一个分类器来进行邮件分类呢？</p>
<p>我们第一步当然是要取<strong>选择特征向量</strong>了</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/134826_f8062df6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如上图，我们可以这样简单地建立特征向量。</p>
<p>首先，选出垃圾邮件中可能出现的单词，比如：<em>deal</em>,<em>buy</em>,<em>discount</em>,<em>now</em>,……</p>
<p>然后，对于一封邮件，我们将找到该单词的位置置为 1，未找到就置为 0</p>
<p>$$<br>x_j = 1 第j个位置的单词出现了\<br>x_j = 0 第j个位置的单词未出现<br>$$</p>
<p>通过这种方法，我们就得到了一个简单的特征向量。当然了，我们也可以按照下面列出的思路去设计更复杂的特征向量：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/135254_a86a6e66_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<ol>
<li>收集更多的数据</li>
<li>设计可以包含邮件发送人的特征向量</li>
<li>同一字母的不同大小写</li>
<li>字母内部穿插数字</li>
</ol>
<p>……</p>
<p>所以，大佬推荐的开发过程是：</p>
<ol>
<li>快速地实现一个简单粗暴的模型</li>
<li>应用学习曲线来决定，进行何种优化</li>
<li>误差分析</li>
</ol>
<p>所以，这种开发模式不像是我们所想的像造大坝那样的庞大的预测式项目，而像是<strong>基于快速原型模型开发的项目</strong>。举一个具体例子</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/141911_2de6b837_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们先实现一个邮件分类器，使用交叉验证集检验模型，发现算法在500个验证数据中错误地分了100封邮件，那么接下来我们就要<strong>人工地</strong>对这些数据进行分析，比如我们发现：</p>
<blockquote>
<p>医药广告：12条</p>
<p>虚假消息：4条</p>
<p>钓鱼邮件：53条</p>
<p>其它：31条</p>
</blockquote>
<p>在这些邮件中，我们发现它们可能会出现以下情况：</p>
<blockquote>
<p>错误拼写：5条</p>
<p>特殊的邮件路径：16条</p>
<p>特殊的标点符号：32条</p>
</blockquote>
<p>此时，我们就可以针对钓鱼邮件和标点符号设计更复杂的特征向量。那么我们再细节一点，如果单词中出现<em>discount,discounting，discounted，discounts……</em>这些词，我们是将它们看成一个特征好，还是分开看好呢？单词的大小写区分是不是会影响到模型的准确度呢？此时，我们迫切地希望可以存在一种<strong>量化指标来评估模型的效果</strong>，这就是接下来要讲的误差分析。</p>
<h3 id="12-2-误差分析"><a href="#12-2-误差分析" class="headerlink" title="12.2 误差分析"></a>12.2 误差分析</h3><h4 id="12-2-1-偏斜类问题-Skewed-classes"><a href="#12-2-1-偏斜类问题-Skewed-classes" class="headerlink" title="12.2.1 偏斜类问题(Skewed classes)"></a>12.2.1 偏斜类问题(Skewed classes)</h4><p>机器学习分类模型存在一种情况叫<strong>偏斜类</strong>。偏斜类简单理解就是在训练模型时由于正样本和负样本数目之间的严重不平衡，导致模型最后检测全部都是1或者全部都是0，我们有如下一个肿瘤分类问题：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1204/152038_b70c15e7_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>假如我们发现我们的模型正确率为99.2%，误差为0.8%，但是我们在测试集中只有<strong>0.5%的患者</strong>，此时，如果我们单纯地返回0，只会造成0.5%的误差，所产生的效果甚至优于机器学习模型的效果，“模型正确率”为99.5%。所以我们能够得出一个结论，<strong>简单地使用正确率，错误率来检验模型误差是不合适的</strong>，因此我们引出两个指标用于评估模型的误差。我们先画出如下的一张图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1207/175342_9bc70054_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h5 id="查准率-（Precision）"><a href="#查准率-（Precision）" class="headerlink" title="查准率 （Precision）"></a>查准率 （Precision）</h5><p><strong>查准率/准确率</strong>是针对我们<strong>预测结果</strong>而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是<br>$$<br>Precision = \frac{TP}{TP+FP}<br>$$</p>
<p>举个例子就是，<strong>在我们预测为阳性的患者中，真正的阳性的比例为多少</strong>。</p>
<h5 id="召回率-（Recall）"><a href="#召回率-（Recall）" class="headerlink" title="召回率 （Recall）"></a>召回率 （Recall）</h5><p>而<strong>召回率</strong>是针对我们原来的<strong>样本</strong>而言的，它表示的是样本中的正类有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。<br>$$<br>Recall=\frac{TP}{TP+FN}<br>$$</p>
<p>举个例子就是，<strong>在样本中，有多少比例的患者被正确被确诊为阳性</strong>。</p>
<p>比如对于一个只是返回0的模型，它的召回率就是0（它不存在真阳性的情况），那它就不是一个好模型。</p>
<p>我们通常将<strong>很少出现的情况置为正类（y = 1），在本例中恶性肿瘤为正类，良性肿瘤为负类</strong>。</p>
<br>

<h4 id="12-2-2-查准率和召回率的权衡"><a href="#12-2-2-查准率和召回率的权衡" class="headerlink" title="12.2.2 查准率和召回率的权衡"></a>12.2.2 查准率和召回率的权衡</h4><p>准确率和召回率都是越高越好吗？实则不然。我们接着讨论癌症分类的问题，我们记患癌症 y = 1，否则 y = 0。</p>
<p>我们的假设函数会输出一个[0,1]间的数，为<strong>患癌症的概率</strong>。如果我们将h(x)的输出阈值设为0.9，即 h(x) &gt; 0.9时，y = 1。那么此时，我们会拥有很高的查准率，较低的召回率。较高的阈值，<strong>我们预测病人为真阳性的概率会很高，但是会出现由于阈值设置过高而忽略的假阴性</strong>。而较低的阈值，<strong>我们确实可以找出很多阳性的患者，但是对于是否为真阳性这种情况，我们的准确度会降低</strong>。</p>
<p>总的来说就是，<strong>高查准率带来了较低的召回率，反之亦然，鱼和熊掌不可兼得</strong>，如何选择出效果最优的模型，就是本节要讲述的内容。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/195909_48fe752a_5550632.png" alt="输入图片说明" title="1.png"></p>
<p>上图是什么意思呢？上图表示，准确率和召回率会随着采用的模型不同而发生改变，<strong>不同模型之间的查准率和召回率间没有直接联系</strong>，算法1的0.1查准率和算法2的0.1查准率的含义是不同的，因为它们的图像不同。那么我们如何寻找一个指标来评估呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/195940_b70d294a_5550632.png" alt="输入图片说明" title="2.png"></p>
<p>如上图，我们不采用平均值作为评估指标，因为当其中一个值极大时，可能会拉高整体均值，如算法3。所以，我们转而使用 f （或者f1） 作为评审指标，它越大，模型的效果就越好。</p>
<p>F1计算方式为：<br>$$<br>P为查准率,R为召回率<br>\F_1Score = 2\frac{PR}{P+R}<br>$$</p>
<br>

<h3 id="12-3-数据集的规模与机器学习算法"><a href="#12-3-数据集的规模与机器学习算法" class="headerlink" title="12.3 数据集的规模与机器学习算法"></a>12.3 数据集的规模与机器学习算法</h3><p>在之前的学习中，我们知道了机器学习算法会出现高偏差或高方差的问题，为了获得一个高性能的机器学习模型，我们一般采用<strong>模型参数多的，使用大量数据训练</strong>的方法来获得模型。模型参数多而复杂可以避免偏差问题，训练的数据量大可以避免方差问题。</p>
<br>



<h2 id="Part-13-支持向量机-Support-Vector-Machine"><a href="#Part-13-支持向量机-Support-Vector-Machine" class="headerlink" title="Part 13 支持向量机 (Support Vector Machine)"></a>Part 13 支持向量机 (Support Vector Machine)</h2><h3 id="13-1-支持向量机的引出"><a href="#13-1-支持向量机的引出" class="headerlink" title="13.1 支持向量机的引出"></a>13.1 支持向量机的引出</h3><p>支持向量机是监督学习中的一种二元分类器，具体数学原理我们之后再讨论，先看看支持向量机是怎么来的。</p>
<p>以二分类问题为例，我们有如下的假设函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/202518_087ebad1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们记</p>
<p>$$<br>z = \theta^Tx<br>$$</p>
<p>那么当y=1,正类时，h(x) ≈ 1，z &gt;&gt; 0</p>
<p>那么当y=0,负类时，h(x) ≈ 0，z &lt;&lt; 0</p>
<p>接着，我们就要对代价函数进行最小化，使用逻辑回归算法时，一条数据的损失函数值的计算公式如下：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/202857_56ed962d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这里把负号移到括号内部了。</p>
<p>我们以y=1为例画出函数图像：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203040_3d098fbd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>支持向量机做的就是用下图这条紫色的函数来代替原来的函数，当y=0时也同理：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203204_832b4290_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203354_26b2ce74_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>在这里插一句，此时我们计算的是<strong>单个样本的代价值</strong>，当数据为正类时，我们的z会远大于零，自然h(x)≈1，代价值小。如果预测错了，即z&lt;&lt;0,h(θ)≈0，那么代价就会变大。然后，我们给它们分别命名:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203711_7d0729fd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>cost_0(z)代表代价函数中 y=0 时的函数.</p>
<p>cost_1(z)为代价函数中 y=1 时的函数.</p>
<p>接着我们看看新的代价函数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203955_43428945_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们用新函数代替log的复合函数，同时去掉常数项1/m。其实去掉常数项不影响最小化的结果，如上图红字示例所示（对原来的式子进行了适当的放大，对原来的代价函数进行了线性近似）</p>
<p>得到：</p>
<p>$$<br>min_\theta;\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)}) + \frac{λ}{2}\sum_{j=1}^{m}\theta_j^2<br>$$</p>
<p>在逻辑回归中，我们可以把代价函数拆成如下两个部分：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/205340_d4db8738_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>$$<br>J(\theta)=A+λB<br>$$<br>它可以通过参数来调整是A项重要还是B项重要（当λ很小时，正则项的权重就更大）。在支持向量机中，我们换了一个新的表达形式：</p>
<p>$$<br>J(\theta) = CA+B\<br>如果当;C=\frac{1}{λ}时，支持向量机和逻辑回归所得到的模型参数应该相同<br>$$</p>
<p>最终，我们通过最小化如下函数，就得到使用SVM的模型参数：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/205828_1964c0aa_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>支持向量机输出的不是概率，而是最终的分类结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/205956_4d06fb09_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h3 id="13-2-对SVM的直观理解"><a href="#13-2-对SVM的直观理解" class="headerlink" title="13.2 对SVM的直观理解"></a>13.2 对SVM的直观理解</h3><p>我们来回顾一下支持向量机的代价函数（优化目标）：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/155344_60de69f6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>对于这样的一个函数，<strong>它如何才能取到最小值呢？</strong></p>
<p>当 y = 1 时，θ^T^x^(i)^ ≥ 1 ，cost<del>1</del>(θ^T^x^(i)^)的值最小，(1-y^(i)^)cost<del>0</del>(θ^T^x^(i)^)为零。</p>
<p>当 y = 0 时，θ^T^x^(i)^ ≤ -1 ，cost<del>0</del>(θ^T^x^(i)^)的值最小，y^(i)^cost<del>1</del>(θ^T^x^(i)^)为零。</p>
<p>接下来，我们考虑一种情况，<strong>当 C 很大，如C=100000时，如何让SVM的代价函数尽量小呢？</strong>当然是让C后面乘的项尽可能地小，为零最好。那也就意味着，支持向量机只剩下了最后一项。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/160219_fb9ec68a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>因为我们知道<strong>前面那一堆的最小值就是0</strong>，那么最终我们需要最小化的函数就是：<br>$$<br>min_\theta ; \frac{1}{2}\sum_{j=1}^{n}\theta_j^2<br>$$<br>因此，我们会得到一个很有趣的<strong>决策边界</strong>，如下图所示:</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/161411_71d87bf1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>支持向量机做出的决策边界和其它的决策边界不同，它能使正负类之间的“间隔”尽可能地大。下图展示的是SVM产生的决策边界和其它决策边界的区别，其中黑色的为SVM的决策边界，可以看出它是最大化支持向量的边界的：</p>
<img src="https://images.gitee.com/uploads/images/2020/1217/161717_9186cf65_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:67%;" />

<p>最后一点，支持向量机对噪声很敏感，尤其是当C很大的时候。下图是两种不同情况下的决策边界示意图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/162413_71405c0b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<h3 id="13-3-大间隔分类器的数学原理-初步"><a href="#13-3-大间隔分类器的数学原理-初步" class="headerlink" title="13.3 大间隔分类器的数学原理-初步"></a>13.3 大间隔分类器的数学原理-初步</h3><p>既然支持向量机又叫大间隔分类器，从上节中我们已经看到了“间隔”的直观表现形式，那么它是通过何种数学原理呈现的呢？</p>
<p>首先，我们先复习一下向量的点乘，向量点乘有两种定义方式，代数方式和几何方式<br>$$<br>v=[v_1,v_2];u=[u_1,u_2]<br>\u^Tv=?<br>\代数定义:u_1<em>v_1+u_2*v_2<br>\几何定义:||v||</em>||u||*cos\theta ;其中||v|| = \sqrt{v_1^2+v_2^2}<br>$$<br><img src="https://images.gitee.com/uploads/images/2020/1214/143604_f71db46b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>由上图，我们就得出了一个推论：<br>$$<br>u^Tv=u_1<em>v_1+u_2*v_2= P</em>||u||(P为v在u上的投影长度)<br>$$<br>由于P的正负取决于夹角θ，若θ&gt;90°，cos θ &lt; 0。</p>
<p>接下来，我们来规定支持向量机的假设函数，代价函数。</p>
<p>假设函数：<br>$$<br>h_\theta(x)=\left{<br>\begin{aligned}<br>1 &amp; &amp;\theta^Tx≥0 \<br>0 &amp; &amp; otherwise\<br>\end{aligned}<br>\right.<br>$$<br>代价函数:<br>$$<br>s.t.<br>; \theta^Tx^{(i)}≥1 ;if ;y^{(i)} = 1 \<br>\theta^Tx^{(i)}≤-1 ;if ;y^{(i)} = 0<br>\<br>J(\theta) =\frac{1}{2}\sum_{j=1}^{n}\theta_j^2<br>$$<br>我们使每次为正类样本时的向量内积≥1，反之≤-1。由于之前我们画出了支持向量机的图像，所以在此就直接使用该函数图像的结果。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1208/203711_7d0729fd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当 n=2 时，我们可以有如下推论：<br>$$<br>\frac{1}{2}\sum_{j=1}^{2}\theta_j^2 = \frac{1}{2}(\sqrt{(\theta_1^2+\theta_2^2)})^2\<br>=\frac{1}{2}||\theta||^2<br>$$<br>那么将θ与x的向量点乘类比到u,v上，我们就可以得到如下结论：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/145927_d7b77f5d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那么就可以这么书写：<br>$$<br>\theta^Tx^{(i)} = P^{(i)}||\theta||<br>$$</p>
<p>$$<br>s.t.<br>;  P^{(i)}||\theta||≥1 ;if ;y^{(i)} = 1 \<br> P^{(i)}||\theta||≤-1 ;if ;y^{(i)} = 0\<br> min_\theta ; \frac{1}{2}\sum_{j=1}^{n}\theta_j^2=\frac{1}{2}||\theta||^2<br>$$</p>
<p>假设有这样一条决策边界（绿色）：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/150819_2406eb79_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>正类为红色的图案，负类为蓝色的图案。<br>$$<br>∵x^{(1)}是正类\<br>∴p^{(1)}<em>||\theta|| ≥ 1且p^{(1)}为定值<br>\∴||\theta||值会大，与最小化的目标冲突\<br>同理,x^{(2)}为负类,p^{(2)}</em>||\theta|| ≤ -1且p^{(2)}为定值\||\theta||值会大，与最小化的目标冲突<br>$$<br>所以上面这条分割线其实不是最终SVM找到的那条。实际上，支持向量机会寻找类似下图的分类边界，找出<strong>支持向量投影长度尽量长，但参数向量θ范数尽量小的分界线</strong>，这也就是大间隔的由来。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/151634_453dda9f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>顺带提一下为什么<strong>决策边界和参数向量θ正交</strong>，以下图为例，我们规定了<br>$$<br>\theta^Tx^{(i)}≥1 ;if ;y^{(i)} = 1 \<br>\theta^Tx^{(i)}≤-1 ;if ;y^{(i)} = 0<br>$$</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/152323_a4339890_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>$$<br>当-1&lt;\theta^Tx^{(i)}&lt;1时，其实就是决策边界的图像范围，我们不妨设决策边界(绿线)为:\\theta^Tx^{(i)}=0\<br>\theta_1<em>x_1 + \theta_2</em>x_2 = 0,K_{决策边界} = -\frac{\theta_1}{\theta_2}\<br>θ向量(蓝线)由于\theta_0 = 0始终过原点，斜率K_{theta} = \frac{\theta_2}{\theta_1}\<br>两斜率乘积为-1，相互垂直，所以两向量正交<br>$$</p>
<h3 id="13-4-核函数"><a href="#13-4-核函数" class="headerlink" title="13.4 核函数"></a>13.4 核函数</h3><p>我们希望能对复杂的高阶多项式做分类器，按照之前的做法很难实现，所以我们来介绍一个用于非线性函数边界确定的方法。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/180852_ad135a7f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如上图，我们拥有一个高阶非线性函数，我们记每项特征的组合为 f<del>1</del>,f<del>2</del>,……<br>$$<br>g(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+……<br>\<br>f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,……<br>$$<br>接着我们<strong>手动地</strong>在n为特征向量构成的空间中，选出标记点 l</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/181428_ce81599d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>上图中所选取的特征向量为二维特征向量，它们构成了一个平面，我们在这个平面上寻找出3个<strong>标记点</strong>，接着，我们来<strong>计算空间中的点距离标记点的相似度</strong>。<br>$$<br>给定空间上任意一点x\<br>f_1 = Similarity(l^{(1)},x) = e^{-\frac{||x-l^{(1)}||^2}{2 σ^2}} &lt;=&gt;exp(-\frac{||x-l^{(1)}||^2}{2 σ^2}))\<br>f_2 = Similarity(l^{(2)},x) = exp(-\frac{||x-l^{(2)}||^2}{2 σ^2})\<br>f_3 = Similarity(l^{(3)},x) = exp(-\frac{||x-l^{(3)}||^2}{2 σ^2})\<br>$$</p>
<p>上述所说的<strong>相似度函数</strong>，就是<strong>核函数(kernel function)</strong>，这里我们使用的是高斯核函数。我们会将<strong>Similarity(a,b)写为k(a,b)</strong></p>
<p>举个例子：<br>$$<br>If  \quad x ≈ l^{(1)}\<br>f_1 ≈ k(l^{(1)},x) ≈ exp(-\frac{0}{2 σ^2})≈1/e^0 ≈ 1\<br>If \quad x 远离l^{(1)},假设为+∞\<br>f_1 ≈ k(l^{(1)},x) ≈ exp(-\frac{+∞}{2 σ^2})≈\frac{1}{e^{+∞}} ≈ 0\<br>$$</p>
<p>我们接着看一看核函数在几何上的体现。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/183434_c6b67682_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如上图，我们人工选择了一个标记点 l^(1)^ = [3,5]，如果选取特征x = [3,5]，它们重合，相似度此时最高为 1，如果更改σ的值，我们会得到下图中的结果。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/184158_2d33d131_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们不难分析出，<strong>σ影响了核函数的相似范围</strong>，σ越大，相似的范围越宽泛，σ越小，相似的条件越严苛。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1214/184712_97caff6b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们通过<strong>设定标记点，计算核函数的方式来替换了原来的特征</strong>，如上图，如果我们有一个品红色的x，假设模型参数已知，那么对于品红色的x就有：<br>$$<br>\theta_0 = -0.5,\theta_1 = 1,\theta_2 = 1,\theta_3 = 0\<br>f_1 ≈ 1,f_2≈0,f_3≈0\<br>品红色x的值为0.5&gt;0所以预测为正类\<br>蓝色的点同理<br>$$</p>
<h3 id="13-5-核函数与SVM"><a href="#13-5-核函数与SVM" class="headerlink" title="13.5 核函数与SVM"></a>13.5 核函数与SVM</h3><p>在讲核函数时，我们是人工地选取若干点作为标记点，但是在一个机器学习训练任务中，我们应该如何选择呢？</p>
<p>事实上我们直接将<strong>训练样本点设置为标记点</strong>，如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/150853_d48404b9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最终得到 x^(m)^ 对应的 l^(m)^ .</p>
<p>那么带有核函数的SVM就可以归结为一下过程：</p>
<ol>
<li><p>选定训练集中的点作为标记点。</p>
</li>
<li><p>对于一个从训练集或交叉验证集中选出的点，我们做<strong>特征映射</strong></p>
<blockquote>
<p>f<del>1</del> = similarity（l^(1)^,x^(i)^）</p>
<p>f<del>2</del> = similarity（l^(2)^,x^(i)^）</p>
<p>…</p>
<p>f<del>i</del> = similarity（l^(i)^,x^(i)^） = 1</p>
<p>…</p>
<p>f<del>m</del> = similarity（l^(m)^,x^(i)^）</p>
</blockquote>
<p>得到的这个新向量，作为新的特征向量，即：<br>$$<br>f = [f_0,f_1,…,f_m] ;;f_0 = 1\<br>h_\theta(x) = \theta^Tf \<br>if ;;\theta^Tf ≥ 0; predict ;1<br>$$<br>想得到参数向量θ，我们只需要对于下列函数做最小化即可：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0125/172540_e92682f9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
</li>
</ol>
<p>对于最后一项，将其向量化后可以看成：<br>$$<br>\frac{1}{2}\sum_{j=1}^m{\theta_j}^2 = \frac{1}{2}\theta^T\theta<br>$$<br>接着，我们来看一看关于参数选择与方差，偏差问题之间的关系。</p>
<p>由于我们是从Logistics Regression出发，推导的SVM，那么对于逻辑回归，我们可以将其代价函数看做：<br>$$<br>A+\lambda B<br>$$<br>正则化参数λ越大，对参数向量的惩罚越大，模型欠拟合导致高偏差问题。反之，造成高方差问题。</p>
<p>此时SVM可以看作：<br>$$<br>CA+B;(当C=\frac{1}{\lambda}时,A+\lambda B = CA+B)<br>$$<br>此时，C越大(λ越小)，造成高方差，低偏差的问题，C越小(λ越大)，造成高偏差，低方差的问题（欠拟合）。</p>
<p>对于核函数中的σ^2^，则有下列结论：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0125/173945_4dc708c3_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>σ^2^过大，映射特征f更光滑，模型对变化更不敏感，高偏差，低方差。</p>
<p>反之，σ^2^过小，映射特征f 更陡峭，模型对变化更敏感，低偏差，高方差。</p>
<h3 id="13-6-SVM的使用"><a href="#13-6-SVM的使用" class="headerlink" title="13.6 SVM的使用"></a>13.6 SVM的使用</h3><p>根据大佬的指示，对于SVM我们不必从头到尾自己实现。毕竟在之后的练习中，大佬也只是自己写了个模型让我们调用。我好奇看了看源码，其中用到了一个叫SMO的优化算法来实现SVM。其原理，不是现在的我能攻克的。</p>
<p>对于调库，这工作就轻松多了，首先我们要确定SVM中的<strong>参数C</strong>，然后如果使用高斯核函数的话那就再指定一下<strong>σ^2^</strong>,如果不用核函数，即线性核函数的话，那就不用管了。</p>
<p>至于什么时候使用SVM，Logistics Regression和神经网络，大佬给了如下建议：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0125/203715_55b1cc1a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>总的来讲就是，当特征值很多（相对与训练样本数量）时，我们可以使用逻辑回归或者无核的SVM</p>
<p>如果，特征值少，训练数量适中，选择带高斯核函数的SVM。</p>
<p>特征值少，训练样本很多，考虑添加更多的特征，使用逻辑回归或者无核的SVM</p>
<p>神经网络无论什么情况都适用。</p>
<p>具体的数值参考范围就看图片吧，这里我就不赘述了。</p>
<br>

<h2 id="Part-14-无监督学习-Unsupervised-Learning"><a href="#Part-14-无监督学习-Unsupervised-Learning" class="headerlink" title="Part 14 无监督学习 (Unsupervised Learning)"></a>Part 14 无监督学习 (Unsupervised Learning)</h2><h3 id="14-1-启航"><a href="#14-1-启航" class="headerlink" title="14.1 启航"></a>14.1 启航</h3><p>就在此刻，我们迈入了机器学习的第二扇大门-无监督学习。在本博客开篇我们简单地介绍了一下无监督学习。现在，让我们重新地来认识一下它。</p>
<p>在监督学习中，我们的数据集是有标签的，此时我们通过假设函数来拟合数据集达成我们的目标，如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0125/205321_28963647_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>但是在无监督学习中，我们希望计算机能够自动地分别出这些数据，这些相似的数据就是一个<strong>簇（cluster）</strong>，此时我们的训练集长这样，已经没有了标签，通过分簇算法将它们自行归类：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0125/205445_c4b9944a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>无监督学习能做的事情有很多，比如分析社交网络，用户群体，探索银河系……</p>
<h3 id="14-2-K-means算法"><a href="#14-2-K-means算法" class="headerlink" title="14.2 K-means算法"></a>14.2 K-means算法</h3><h4 id="14-2-1-算法流程概述"><a href="#14-2-1-算法流程概述" class="headerlink" title="14.2.1 算法流程概述"></a>14.2.1 算法流程概述</h4><p>K-means算法是一个迭代算法，主要会进行两项工作，<strong>簇分配</strong>和<strong>移动簇中心</strong>，我们用图片来表示：</p>
<p>开始时，对于下图，我们随机出两个<strong>簇中心</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/160802_9e52c3a2_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>接着我们开始簇分配，其实很简单就是遍历整个数据集<strong>计算样本点到若干簇的距离</strong>，取最小的距离的那个簇，标为一类，如同用颜色分类一样。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/161014_4bab46ab_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>如上图，我们就将样本点按簇中心分为这两类了，接着我们要将<strong>簇中心进行移动</strong>。步骤很简单，计算现在红色点和蓝色点的均值，生成新的坐标点，然后让簇中心移动到该点处。如下图：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/161325_b459805b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>然后重新进行簇分配，进行簇移动。</p>
<p>第二次簇分配和簇移动：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/161417_0f2035cf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>第三次簇分配和簇移动：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/161505_1d03dbcd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>所以对于K-means算法而言，只需要准备训练数据和K的大小就可以把数据分成K个簇了。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0126/161701_b72b02b5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们指定 K 个簇，K-means就会随机初始化 K 个簇中心。</p>
<p>接着开始迭代循环，在循环中要做两件事情：</p>
<ol>
<li><p>进行簇分配，从第1个样本遍历到第m个样本，给它们分配簇中心<br>$$<br>c^{(i)} = min_k(||x^{(i)}- \mu_k||^{2})<br>$$</p>
</li>
<li><p>移动簇中心，举个例子：<br>$$<br>若;c^{(3)} = 3,c^{(1)} = 3,c^{(5)} = 3,c^{(8)} = 3\<br>x^{(3)},x^{(1)},x^{(5)},x^{(8)}均为簇3的点\<br>新的簇中心;\mu_3=\frac{1}{4}[x^{(3)}+x^{(1)}+x^{(5)}+x^{(8)}]<br>$$</p>
</li>
</ol>
<p>所以我把K-means算法理解为<strong>分K个簇的，簇中心平均值坐标算法</strong></p>
<h4 id="14-2-2-K-means-的代价函数（优化目标）"><a href="#14-2-2-K-means-的代价函数（优化目标）" class="headerlink" title="14.2.2 K-means 的代价函数（优化目标）"></a>14.2.2 K-means 的代价函数（优化目标）</h4><p>在开始说明K-means的代价函数（失真函数）前，首先说明一下若干变量。<br>$$<br>c^{(i)}=x^{(i)}样本点被分配的簇中心索引号\<br>\mu_k = 第k个簇中心\<br>\mu_{c^{(i)}} = x^{(i)}从属的簇中心编号<br>$$<br>这样，我们来看一下K-means算法的代价函数：<br>$$<br>J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_K)= \frac{1}{m}\sum_{i=1}^{m}{||x^{(i)}-\mu_{c^{(i)}}||}^2\<br>min_{c^{(1)},…,c^{(m)}\\mu_1,…,\mu_K}J(c^{(1)},…,c^{(m)},\mu_1,…,\mu_K)<br>$$<br>通过这个公式，我们就能找到距离簇中心最近的点的集合和最优的簇中心。</p>
<h4 id="14-2-3-簇的随机初始化"><a href="#14-2-3-簇的随机初始化" class="headerlink" title="14.2.3 簇的随机初始化"></a>14.2.3 簇的随机初始化</h4><p>现在，我们来介绍簇的随机初始化过程，下图总述了K-means的过程，那我们如何进行随机初始化呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/143618_59057d44_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们采取的方式是<strong>随机选择样本点</strong>，来继续簇中心的初始化，举个例子：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/143730_ea09ef8b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先，当然分的簇的个数不能大于样本点的数量（这不是废话吗(＠_＠;)），这里我们假设分两个簇。</p>
<p>然后就如同上图的右侧部分一样，如果脸好，随机的结果比较好，我们就可能得到图片右上方的结果。要是脸黑，我们就随机成右下方的结果。鉴于这种情况，一般而言在分簇数量在<strong>2~10</strong>的时候采用重复随机的形式来计算代价函数，取出最小的代价函数值，得到聚类结果。我们看图说话：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/144332_daf77f64_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>显然对于这个数据集，我们最好的聚类结果应该是最上面的那张子图。但由于随机初始化，我们就可能得到下面这两种图像，为了避免这个情况发生，我们会这样处理：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/144549_7d78a8b3_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>随机初始化k-means 50-1000次，每次都计算一个 代价函数J的值，最后选最小的函数值，那其中所有的参数（参数包括了每个点所分配的簇中心），就是最优的聚类结果。</p>
<h4 id="14-2-4-分簇的数量"><a href="#14-2-4-分簇的数量" class="headerlink" title="14.2.4 分簇的数量"></a>14.2.4 分簇的数量</h4><p>大佬说，现在最常用的方式还是得到数据集的可视化视图，进行人工辨别得出分簇的数量。不过有时很难用肉眼观测出结果，如下图：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/145840_1a8ecca9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>有人觉得它是可以分两个簇，有人觉得它是4个簇，甚至有人认为它是三个簇。由此可见有时肉眼不见得能很好地观察出来聚类结果。那么我们还有另外一种方式，叫做“肘部法则”，如下图：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/150109_dc833963_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们通过<strong>增加聚类数量的方式来计算每一次增加时的代价函数值</strong>，我们期望得到左侧图像这样的结果，这样我们就可以直接<strong>取拐点</strong>作为我们的分簇数量了。但是一般而言，我们得到的图像都是类似右侧的图像，图像很光滑，不太好找到那个“肘”。</p>
<p>通常，聚类结果是用于具体场景下的，比如卖T恤衫，我到底是将尺码分成（S，M，L）三类好，还是（XS，S，M，L，XL）五类好，这个得看实际的市场需要和顾客的满意程度，没有固定的答案。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0127/150626_a590fced_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h3 id="14-3-第二类无监督学习-降维-Dimensionality-Reduction"><a href="#14-3-第二类无监督学习-降维-Dimensionality-Reduction" class="headerlink" title="14.3 第二类无监督学习-降维(Dimensionality Reduction)"></a>14.3 第二类无监督学习-降维(Dimensionality Reduction)</h3><p>降维，这个词听起来很高大上，其实很好理解。我们举个例子说明一下，假设我们有两维数据，其中一维数据用来描述物体长度单位厘米，另一维数据也用来描述物品长度，单位英尺。很显然，这两组数据<strong>高度耦合</strong>，本质上都是描述物体的相同信息-长度。我们画个图，表示一下上述文字：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/145509_b786998e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们要做的就是用一个一维特征来替代这个二维的特征向量。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/145918_bb0e1790_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最简单的做法就是把原函数直接投影到一条直线上。那如果是n维特征值，想降成n-1维特征值，何解？其实也是做投影，比如将3维数据投影到一个二维平面上。如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/150023_2ca76dd4_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>通过降维的方式使耦合数据压缩，进而我们可以减少存储空间的使用，同时提高学习算法的时间效率。不过，我有一些小猜测想留在这里。</p>
<ol>
<li>降维的过程像不像从一个坐标系变换到另一个坐标系的过程呢？或者说由一组基换成另一组基。</li>
<li>降维的过程中势必会造成误差，这个误差能算出来吗？就像重积分中坐标转换一样会不会出现一个形如雅可比行列式这样的东西呢？</li>
<li>是不是在特征高度耦合的时候，采用降维这个手段是最好的呢？</li>
</ol>
<h4 id="14-3-1-降维的应用-数据可视化"><a href="#14-3-1-降维的应用-数据可视化" class="headerlink" title="14.3.1 降维的应用-数据可视化"></a>14.3.1 降维的应用-数据可视化</h4><p>对于下面这些关于经济的数据，貌似直接使用画图模块是没法画的，毕竟这是一个50维的数据，为了能将其呈现在画布上。我们需要将它将成2维数据。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/151355_f303935f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/151429_c434d55c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>将50维的数据压缩成2维，我们就会得到如上图这样的二维数据，进而我们可以进行画图。则数据z1可能主要表示国家总体GDP情况，z2主要表示人均GDP情况。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0128/151649_462050c4_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>接下来，我们就要学习这种名为<strong>主成分分析PCA(Principal Component Analysis)</strong>的算法来进行数据降维和数据压缩。</p>
<h4 id="14-3-2-PCA-Principal-Component-Analysis"><a href="#14-3-2-PCA-Principal-Component-Analysis" class="headerlink" title="14.3.2 PCA (Principal Component Analysis)"></a>14.3.2 PCA (Principal Component Analysis)</h4><h5 id="PCA的概述"><a href="#PCA的概述" class="headerlink" title="PCA的概述"></a>PCA的概述</h5><p>假设我们有一个分布在二维空间上的数据集，此时我们像将其投影到一个一维直线上。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0129/154351_100a4d24_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们要做的事情就是，找到一条向量，将点投影到该向量上，<strong>以期许找到投影长度最短的那条向量</strong>。再举一个三维空间上的例子，如下图，我们拥有一个分布在三维空间中的数据集。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0129/154620_1988d197_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们要做的就是找到两个向量用来表示二维平面，使得3维空间的点投影至二维平面上的投影误差最小。同时，线性回归和PCA的工作是完全不同的。这主要是由于它们两个计算的目标不同。线性回归评价的是<strong>预测值到真实值之间的误差</strong>。而PCA是<strong>投影距离尽量小</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0129/155347_60c9b240_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那么，我们如何使得投影最小化呢？如何进行数据降维呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/170841_091cc1ae_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先第一件事就是特征的<strong>均值归一化</strong>，如上图所示。对于PCA，接下来要做的事情就是寻找到两个重要的内容。</p>
<ol>
<li>一组比原特征空间<strong>低维的一组基</strong></li>
<li>新特征的数值映射值</li>
</ol>
<p>就是下图的u向量和z映射：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/171436_5a8b4b07_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>至于如何得到这组向量涉及到复杂的数学推导公式，未来我可能会补上，但是现在能力有限，请允许我先忽略过去。首先引入<strong>协方差矩阵</strong>这一概念，计算公式如下:<br>$$<br>\sum=\frac{1}{m}\sum_{i=1}^m(x^{(i)}){(x^{(i)})}^T<br>$$<br>这个矩阵是一个n<em>n的矩阵((n*1)\</em>(1*n))，然后计算该矩阵的特征向量，想降几维，就选几列向量即可。对协方差矩阵的分析操作在Octave中使用的是SVD函数，声明如下：（SVD又称为奇异值分解 Singular Value Decomposition）<br>$$<br>[U,S,V] = svd(Sigma)<br>$$<br>我们取U这个返回值，U的结果如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/172722_612743cb_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>取到了一组基，我们现在就开始做数值映射。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/173501_20b41804_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>将n维数据压缩到k维，选取前k个向量后，我们给它起个新名字叫<strong>U_reduce</strong>,我们一个新的特征向量就是：<br>$$<br>Z = {U_{reduce}}^Tx ;(k<em>n)</em>(n<em>1)-&gt;(k</em>1)<br>$$<br>总结一下如何实现PCA。</p>
<ol>
<li>进行均值归一化</li>
<li>计算协方差矩阵Σ</li>
<li>调用svd奇异值分解函数</li>
<li>选择U<del>reduce</del> = U(：,1:k)</li>
<li>计算z=U<del>reduce</del>^T^ * x</li>
</ol>
<p>挖个坑(PCA是如何最小化平方误差投影的)</p>
<h5 id="如何选择主要成分数量"><a href="#如何选择主要成分数量" class="headerlink" title="如何选择主要成分数量"></a>如何选择主要成分数量</h5><p>在之前所计算出来的 U 矩阵中，你所选择的数量就是PCA的主要成分的数量，本小节是一些有关选择数量的建议。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/175128_134f7222_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>其中 x_approx 是这样计算的<br>$$<br>x_{approx}^{(i)} = U_{reduce}^T<em>z^{(i)}<br>$$<br>分式上方的算式计算的是平均投影误差，下方算式用来计算特征据坐标轴的举例，我们*</em>尝试不同的k值使得满足上述不等式**。不等式右侧常用常数值为0.01，不过0.05也是可以的。接着我们如何写脚本实现上述过程呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0204/180224_e7b70f95_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>一个迭代算法，取k=1计算若干参数，再应用不等式直到不等式成立，算法结束。迭代看起来很慢很傻，幸运的是，应用SVD我们能很快地计算出不等式的结果。<strong>SVD第二个返回参数S是一个对角矩阵</strong>，原不等式可以等价为<br>$$<br>1-\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} ≤0.01<br>$$<br><img src="https://images.gitee.com/uploads/images/2021/0204/180907_72f5a9e1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>所以我们只需要执行一次SVD，获得S这个对角矩阵然后不断计算这个不等式就行了。（当降维幅度较大时我们可以计算 算式≥0.99）</p>
<h5 id="压缩重现-（Reconstruction）"><a href="#压缩重现-（Reconstruction）" class="headerlink" title="压缩重现 （Reconstruction）"></a>压缩重现 （Reconstruction）</h5><p>通过PCA我们可以得到降维的数据，那么我们怎么将降维的数据还原成原来的维度呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0208/151507_5457b0c9_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>$$<br>z=U_{reduce}^Tx\<br>x≈ x_{approx} = U_{reduce}z<br>$$<br>U<del>reduce</del>=(n,k)转置之后为(k,n),x为(n,n),那么z就是(k,n)</p>
<p>那么将其逆运算回去得到的x<del>approx</del>就是（n,n）近似为降维前的数据。</p>
<h5 id="PCA应用时的小建议"><a href="#PCA应用时的小建议" class="headerlink" title="PCA应用时的小建议"></a>PCA应用时的小建议</h5><p>我们可以将PCA应用到监督学习的场景下，以减少运算量。假设我们正在做一个计算机视觉的项目，得到了一个(100*100)的图片，这就包含了至少10000的特征值。我们现在想通过PCA降低模型的计算量，何解？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0208/153246_80235eb5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先我们对训练集做PCA，得到n维到k维的映射关系，然后将新特征z^(i)^代入学习算法中，上图是Logistics Regression。计算模型参数。当从测试集或交叉检验集中来的x^(i)^时，应用<strong>从训练集得出的U<del>reduce</del></strong>进行降维，扔到学习算法中去。</p>
<p><strong>最好不要用PCA去尝试解决过拟合问题</strong>，老师严正声明，这是因为PCA不使用标签y去降维，而且降维本身会带来误差，所以不要这样去做，如果过拟合，咱们为啥不用<strong>调整正则化参数</strong>，这种方法呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0208/154231_7e69d0f9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最后，还是要嘱咐一句话<strong>请不要滥用PCA算法，它很好用，不过请在应用它之前问一问自己，如果没有PCA学习算法会怎样呢？</strong></p>
<br>

<h3 id="14-4-异常检测-Anomaly-Detection"><a href="#14-4-异常检测-Anomaly-Detection" class="headerlink" title="14.4 异常检测 (Anomaly Detection )"></a>14.4 异常检测 (Anomaly Detection )</h3><h4 id="14-4-1-问题背景"><a href="#14-4-1-问题背景" class="headerlink" title="14.4.1 问题背景"></a>14.4.1 问题背景</h4><p>什么是异常检测，我觉得异常检测其实就是离群点检测。举个实际的例子，假设你是一个飞机引擎制造商，在引擎出厂时需要进行质量检测，会测量一系列特征参数，如震动频率啊，温度啊，什么的，我们要做的事情就是<strong>从所有引擎中挑选出有问题的引擎</strong>，详细展开说明其实就是对所有样本构造一个<strong>概率分布模型</strong>，P(x)&lt;ε，所有小于阈值e的样本均为<strong>异常样本</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0209/153401_73d368bf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h4 id="14-4-2-高斯分布（Gaussian-distribution）"><a href="#14-4-2-高斯分布（Gaussian-distribution）" class="headerlink" title="14.4.2 高斯分布（Gaussian distribution）"></a>14.4.2 高斯分布（Gaussian distribution）</h4><p>嗯，正态分布，主要就说说图像，参数和他们的含义吧</p>
<p><img src="https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3530606917,2044115981&fm=26&gp=0.jpg" alt=""></p>
<p>这是一个标准的正态分布图像，其概率密度函数写在了图像下方，我们记x~N(μ,σ^2^)为样本x<strong>服从</strong>平均值为μ，标准差为σ的正态分布。图像的<strong>对称轴为x=μ，σ决定了图形的胖瘦</strong>，如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0209/155004_5442bdca_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>σ越小，函数图像“越瘦”，越大越胖</strong>，同时，如果你对这个图形做积分，其结果一定是1，因为总体概率是1.</p>
<h5 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h5><p>在我们的场景下，是已知观测的样本数据，假设它们服从正态分布，<strong>估算出概率模型的参数，即μ和σ</strong>，怎么算？（极大似然估计）</p>
<p>概率论内容，不想赘述，公式如下：<br>$$<br>\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}\<br>\sigma^2=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-\mu)^2<br>$$<br>事实上，对于方差的计算在机器学习领域中取m和m-1关系不大。</p>
<h4 id="14-4-3-异常检测算法"><a href="#14-4-3-异常检测算法" class="headerlink" title="14.4.3 异常检测算法"></a>14.4.3 异常检测算法</h4><p>我们假设数据集中的样本的<strong>每个特征值都独立同分布的</strong>，并服从高斯分布。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0210/172823_ff193627_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那么对于每一个样本X,就会存在如下的公式：<br>$$<br>P(x) = P(x_1;\mu_1;\sigma_1^2)P(x_2;\mu_2;\sigma_2^2)P(x_3;\mu_3;\sigma_3^2)…\<br>P(x)=\prod_{j=1}^nP(x_j;\mu_j;\sigma_j^2)<br>$$<br>所以对于异常检测算法它的工作流程是这样的：</p>
<ol>
<li><p>选取一些你觉得可能是异常的特征值x<del>i</del></p>
</li>
<li><p>计算均值与方差<br>$$<br>\mu_j=\frac{1}{m}\sum_{i=1}^mx^{(i)}<em>j\<br>\sigma^2_j=\frac{1}{m}\sum</em>{i=1}^m(x^{(i)}_j-\mu_j)^2<br>$$</p>
</li>
<li><p>给定一个新的样本x，计算P(x),如果P(x)&lt;ε,则为异常点.</p>
</li>
</ol>
<p>$$<br>P(x)=\prod_{j=1}^nP(x_j;\mu_j;\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})<br>$$</p>
<p>举个例子，看一看怎么进行异常检测。如下图，我们算出了特征值x<del>1</del>,x<del>2</del>的期望和标准差，它们的图像在右侧，其连乘的图像为左下角的3D图像。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0210/174155_97150835_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>对于$\color{green}{绿色待测试的x_{test}^{(1)},x_{test}^{(2)}}$,我们计算出它们的概率值，然后与阈值进行比对，最后发现第一个样本是正常的，第二个样本是异常的。同时不难观测出，P(x)的3D图像上平面区域是异常区域。</p>
<h4 id="14-4-4-使用异常检测解决实际问题"><a href="#14-4-4-使用异常检测解决实际问题" class="headerlink" title="14.4.4 使用异常检测解决实际问题"></a>14.4.4 使用异常检测解决实际问题</h4><p>当我们在试图选取一个特征值来决策是否将其放入异常检测算法中时，我们总能希望能找到一个指标值（实数值）来评估一个算法的好坏。所以，我们引入带有标签的异常检测算法。以飞机引擎检测为例，现在我们的每个样本自带一个标签，表示正常或异常。假设我们现在拥有<strong>10000个正常飞机引擎的样本，20个异常的飞机引擎样本</strong>。第一件事情就是<strong>重新设计数据集</strong>，新数据集的结构如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0215/163647_32a416bd_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>然后我们使用训练集来拟合出P(x),$P(x)=\prod_{j=1}^nP(x_j;\mu_j;\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$,其中$\mu_j=\frac{1}{m}\sum_{i=1}^mx^{(i)}<em>j;<br>\sigma^2_j=\frac{1}{m}\sum</em>{i=1}^m(x^{(i)}_j-\mu_j)^2$。</p>
<p>接着，我们使用交叉验证集，来预测样本x</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0215/164053_32253790_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>此时，我们是不是就会得到一个预测结果的混淆矩阵啊，是不是忘了混淆矩阵长啥样了呢？我把它拿出来，复习复习。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1207/175342_9bc70054_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>回忆一下，之前我们是如何权衡查准率和召回率之间的关系的，是不是使用一个叫F<del>1</del>Score的函数啊，它是怎么算的来着？<br>$$<br>P为查准率,R为召回率<br>\F_1Score = 2\frac{PR}{P+R}<br>$$<br>这个值越大，不就代表模型效果越好吗，经过反复不断地在交叉验证集上选取不同的特征，不同的ε，使得F1函数值到达最大，最后在测试集上测试出所有数据。（在CV中做决策，在Test中得到最终结果）</p>
<h4 id="14-4-5-异常检测与监督学习的比较"><a href="#14-4-5-异常检测与监督学习的比较" class="headerlink" title="14.4.5 异常检测与监督学习的比较"></a>14.4.5 异常检测与监督学习的比较</h4><p>异常检测做的工作是选择适当的特征值构造一个高斯函数，然后决策一个样本是否是异常点。而监督学习看起来做的是相同问题，无论是应用Logistics Regression还是SVM亦或是神经网络都可以做到分类的工作。那么我们什么时候使用异常检测又在什么时候使用监督学习算法呢？<strong>简单来说，当数据集中正样本极多，负样本极少时，采用异常检测。正负样本数量都差不多大的时候，使用监督学习算法</strong>。</p>
<p>异常检测主要应用在，<em>小网站的欺诈检测，飞机引擎的异常检测，计算机集群的主机检测，……</em>。</p>
<p>倾斜类时用异常检测，非倾斜类时用监督学习。</p>
<h4 id="14-4-6-异常检测的特征选择"><a href="#14-4-6-异常检测的特征选择" class="headerlink" title="14.4.6 异常检测的特征选择"></a>14.4.6 异常检测的特征选择</h4><p>首先，我们在选择特征之前，最好把该特征的结果可视化一下。这一步的目的是看一看该特征是否<strong>满足高斯分布</strong>。如果不满足最好对它进行一下变换（当然不做这步对算法的影响不大）</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0216/192658_9f78c5fa_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>变换的方式有很多，我们可以取对数，可以开根号。此时请化作调参侠，将一个特征值捏成高斯分布吧。第二步就是特征选取，很遗憾在这一步没有可以量化的指标来进行评估。不过有一个很浅显的道理，<strong>特征选取的越多，所包含的信息就越多，在一个维度上检测不出来的问题，会在另一个维度上出现异常</strong>，如下图：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0216/193139_ceab531e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>图像的高度为近似的概率值。我们选取了x<del>1</del>这个特征作为算法的一部分，假设$\color{green}{绿色线条对应的是一个异常样本的特征值}$，此时如果仅看x<del>1</del>，算法是检测不出来异常的，因为此时的概率值并不小，如果我们如左图叠加一个新的特征值x<del>2</del>，分布在圈里的就是正常样本，而此时绿色样本的异常就被检测出来了。</p>
<p>换句话说，<strong>异常检测的特征选择是取决于实际业务场景的。</strong>如果想检测一个数据中心的计算机集群中是否有计算机出现异常，我们认为<strong>CPU利用率，网络使用率为特征值x<del>1</del>,x<del>2</del></strong>。且它们是线性关系的，因为可能像在双十一购物节时，一台服务器需要为很多用户服务，导致这两个特征值都很高，此时为正常情况。但是，如果一个计算机出现代码的死循环，那么x<del>1</del>会很高，x<del>2</del>很低，此时我们就需要捏出一个新的特征值$x_3 = \frac{x_1^2}{x_2}$或者其它的特征。来表示这种异常情况，此时算法就会在一个新的维度上找到该异常主机。</p>
<h4 id="14-4-7-多元高斯分布"><a href="#14-4-7-多元高斯分布" class="headerlink" title="14.4.7 多元高斯分布"></a>14.4.7 多元高斯分布</h4><p>普通的高斯分布是不蕴含<strong>特征之间的关系的</strong>，就像上节处理的那样，需要人工创造新的特征值，来显示地描述。但是，多元高斯分布却可以描述特征之间的关系，举个例子。</p>
<p>我们这有一个数据中心监控计算机异常的任务，其中两个重要评判指标是<strong>CPU使用率和内存占用率，x<del>1</del>,x<del>2</del></strong>，此二者本身线性关系假设它们两个特征的高斯分布图像长这样:</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/134056_1deaf769_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>现在有一个$\color{green}{绿色的异常点}$。分别看它的两个特征，概率都不小，于是之前我们采用连乘方式的单变量高斯分布模型会认为它是一个正常样例。而实际上它却是一个异常点。因此我们需要改变建模方式，<strong>不再对每一个特征值单独建立若干个高斯分布，而是对所有特征值建立一个高斯分布</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/134158_7602ff36_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>$$<br>\mu∈R^n,Σ∈R^{n×n}(协方差矩阵)<br>\P(x;\mu;Σ)=\frac{1}{(2\pi)^{\frac{n}{2}}|Σ|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^TΣ^{-1}(x-\mu))<br>$$<br>公式很复杂，不仅要求协方差矩阵的行列式，还需要求它的逆。那么我们此时化作调参侠，将这个函数可视化，看看都代表什么意思。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/135117_42e46d1f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>第一组参数，<strong>$\mu不动，改变协方差矩阵Σ$</strong>，当Σ=E为单位矩阵时，图像为最左边所示，为参照。当缩小x<del>1</del>对应的矩阵值时，图像变窄，放大，图像变宽。</p>
<p>第二组参数，协方差矩阵值缩小，图像变化瘦，尖</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/135449_8ad400c9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>第三组参数，取负值：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/135624_c67127df_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>第四组参数，移动μ：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/135652_b44a0edc_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>由于笔者能力有限，只能用图像的方式描述多元高斯分布，更多细节请参考其它博主的博文。</p>
<h4 id="14-4-8-使用多元高斯分布的异常检测"><a href="#14-4-8-使用多元高斯分布的异常检测" class="headerlink" title="14.4.8 使用多元高斯分布的异常检测"></a>14.4.8 使用多元高斯分布的异常检测</h4><p>上文中介绍了多元高斯分布，其中有两个重要参数：<strong>μ，Σ</strong>。如何估计这些参数呢？</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/140059_9e7da43b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>μ是一个n维向量，估计方法就是对样本求平均值，Σ，协方差矩阵和PCA中的求法一样，公式参照上图。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/141608_adb5f271_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>对于一个应用了多元高斯分布的异常检测模型，我们首先需要计算两个参数，μ和Σ。然后套公式计算即可。</p>
<p>那我们什么时候选择多元高斯分布的模型呢？如下表：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0217/141823_9d9d6f12_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>原始模型不会捕捉特征之间的关系，而多元高斯分布会。</p>
<p>原始模型，计算代价小，方便算。新模型代价大，毕竟要求协方差矩阵，求逆。</p>
<p>当数据量大于特征数量时，可以考虑选择多元高斯分布模型。</p>
<br>

<h2 id="Part-15-推荐系统-Recommender-System"><a href="#Part-15-推荐系统-Recommender-System" class="headerlink" title="Part 15  推荐系统 (Recommender System)"></a>Part 15  推荐系统 (Recommender System)</h2><h3 id="15-1-问题场景"><a href="#15-1-问题场景" class="headerlink" title="15 .1 问题场景"></a>15 .1 问题场景</h3><p>现在我们拥有一个电影推荐的项目，如下表，我们会有一些用户对电影的喜爱度评分。从0-5，那些用户没看过的就变成问号</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0219/144817_48ea3368_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>接着，我们来引入一些符号，来描述表格中的信息。<br>$$<br>n_u= ;no.users;客户数量\<br>n_m ;no.movies;电影数量\<br>r(i,j)=1;第i个用户是否给第j个电影评过分\<br>y^{(i,j)}=当r(i,j)=1时，用户的评分;y^{(1,1)}=5<br>$$<br>我们要做的就是去预测这些问号值，然后结合该用户的观影习惯，如果没有看过，那么我们就给他进行推荐。</p>
<h3 id="15-2-基于内容的推荐算法"><a href="#15-2-基于内容的推荐算法" class="headerlink" title="15.2 基于内容的推荐算法"></a>15.2 基于内容的推荐算法</h3><p>假设，我们现在能评估出一个内容的度量特征值，我们若是以电影举例的话就将其简单评估为是<strong>动作电影</strong>还是<strong>爱情电影</strong>。我们就会得到这样的一张表。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0219/150853_7000cd21_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>对于第一个电影，取爱情度量为0.9，动作度量为0.那么对于每一个电影，我们都能构造出一个特征向量。由于对于用户而言，他的评分是一个基于线性回归的预测问题，所以，我们要对每一个用户求出他对应的模型参数θ^(j)^,假设我们此时求出了Alice的模型参数。<br>$$<br>\theta^{(1)} = [0;5;0]^T\<br>x^{(3)}=[1;0.99;0]\<br>y^{(1,3)}=(\theta^{(1)})^T.dot(x^{(3)})=4.95<br>$$<br>如果我们把<strong>求解每个用户的模型参数</strong>提炼成问题的表达式，则有：<br>$$<br>r(i,j)=1;第i个用户是否给第j个电影评过分\<br>y^{(i,j)}=当r(i,j)=1时，用户的评分\<br>\theta^{(j)}=第j个用户的模型参数向量\<br>x^{(i)}=第i个电影的特征向量\<br>m^{(j)}=被第j个用户评过的电影数量\<br>对于每个用户j，电影i预测评分、(θ^{(j)})^T(x^{(i)})\<br>学习θ^{(j)}<br>$$</p>
<p>$$<br>min_{\theta^{(j)}} \frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})-y^{(i,j)})^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^n(\theta_k^{(j)})^2<br>$$</p>
<p>这个公式只是用来学习一个用户的模型参数的公式，如果我们要求出所有用户的模型参数，那么这个公式长成这样（此处删掉m是因为常数不影响函数最小化）<br>$$<br>J({\theta^{(1)},\theta^{(2)},…\theta^{(n_u)}})=min_{\theta^{(1)},\theta^{(2)},…\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2<br>$$<br>Okay,现在代价函数有了，我们推导其梯度下降公式:<br>$$<br>\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})-y^{(i,j)})x_k^{(i)}(k=0)<br>$$<br>$$<br>\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})-y^{(i,j)})x_k^{(i)}+\lambda\theta_k^{(i)})(k≠0)<br>$$</p>
<p>哦，看起来我们得到了我们最终想要的公式，但是，其中的物品（内容）的特征值我们都得是已知的，不过大多数情况下我们物品的特征值是未知的，下一讲介绍真正的基于物品的推荐算法。</p>
<h3 id="15-3-协同过滤-Collaborative-Filtering"><a href="#15-3-协同过滤-Collaborative-Filtering" class="headerlink" title="15.3 协同过滤 (Collaborative Filtering)"></a>15.3 协同过滤 (Collaborative Filtering)</h3><p>协同过滤算法可以自动学习特征。现在，我们的数据集是这样的，我们想把每部电影的特征值反向计算出来：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0219/160638_21d48851_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>但是，在用户新注册系统时，我们要对用户进行<strong>偏好设置</strong>，得到每个用户的特征θ，类似于刚才的方法，给定每个用户的偏好，学习每部电影的特征值。</p>
<p><br>，</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0219/160916_d17b4903_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>然后，我们就可以“套娃”了，随机初始化一组用户的偏好。然后….</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0219/161224_9c8c464e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这就是未改良的协同过滤推荐算法。</p>
<h3 id="15-4-组合到一起"><a href="#15-4-组合到一起" class="headerlink" title="15.4 组合到一起"></a>15.4 组合到一起</h3><p>将两个式子结合到一起，我们得到了协同过滤算法的优化目标：<br>$$<br>J({x^{(1)},x^{(2)},…,x^{(n_m)},\theta^{(1)},\theta^{(2)},…\theta^{(n_u)}})=\min_{x^{(1)},x^{(2)},…,x^{(n_m)}\theta^{(1)},\theta^{(2)},…\theta^{(n_u)}}\frac{1}{2}\sum_{(i,j):r(i,j)=1}((θ^{(j)})^T(x^{(i)})-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2<br>$$<br>在协同过滤算法中，我们不再强制捏出一个截距x<del>0</del>=1，所以特征向量是n维的，不是n+1维的。</p>
<p>所以，协同过滤算法的执行流程为：</p>
<ol>
<li><p>随机初始化模型参数x,θ</p>
</li>
<li><p>对上式进行梯度下降或者使用其它数值优化算法找到最小值</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0222/135316_10abee74_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
</li>
<li><p>根据用户的特征计算出$(\theta^{(j)})^T(x^{(i)})$(评分)</p>
</li>
</ol>
<h3 id="15-5-低秩矩阵分解-Low-rank-matrix-factorization"><a href="#15-5-低秩矩阵分解-Low-rank-matrix-factorization" class="headerlink" title="15.5 低秩矩阵分解 (Low rank matrix factorization)"></a>15.5 低秩矩阵分解 (Low rank matrix factorization)</h3><p>之前我们以评分为基本度量，建立了商品与用户之间的联系，<strong>评分越高，用户的喜爱程度越高。反之亦然。</strong>然后通过线性回归的方式，预测出每个用户与商品之间的评分，排除掉那些用户已经浏览或交互过的商品。最终得到的若干待推荐内容。但是，<strong>如何度量，某一内容和用户最近浏览内容的相似度呢？</strong></p>
<p>还是这个场景：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/152152_001175db_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先，要确定每个用户的评分，经过向量化之后，对评分的计算不难得到如下表达：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/152344_3da79f23_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>即，<strong>用户特征.dot(电影特征)</strong>。将上述内容拆成两个矩阵，<strong>电影特征矩阵 X 和 用户特征矩阵 Θ，其中按行表达每一个电影和用户</strong>。<br>$$<br>X = \left[<br>\begin{array}{lcr}<br>-(x^{(1)})^T-\<br>-(x^{(2)})^T-\<br>;;;;;;.\<br>;;;;;;.\<br>;;;;;;.\<br>-(x^{(n_m)})^T-\</p>
<p>\end{array}<br>\right]\<br>Θ= \left[<br>\begin{array}{lcr}<br>-(\theta^{(1)})^T-\<br>-(\theta^{(2)})^T-\<br>;;;;;;.\<br>;;;;;;.\<br>;;;;;;.\<br>-(\theta^{(n_u)})^T-\</p>
<p>\end{array}<br>\right]\<br>$$<br>对于评分的预测即可写作：<br>$$<br>R=XΘ^T<br>$$<br>评分矩阵都是稀疏的，也就是大部分的评分都是空的。我们在做推荐的时候，希望能够把空的评分位置填充上评分。这样就可以根据评分的高低，对用户提供推荐服务。所以上述算法又称<strong>低秩矩阵分解</strong>。</p>
<p>那么度量内容之间的相似度就相对容易了许多，得到了内容的特征向量（高维空间上的一个点），两点之间的距离，不就是最好的相似度度量方式吗？<br>$$<br>min ||x^{(i)}-x^{(j)}||<br>$$<br>现在，让我们再面对一个更让人头疼的问题，如果<strong>我们有一个新用户，注册了我们的系统，他没有给任何电影评过分数，想给他推荐电影，假设电影的特征向量的维度还是2，不采用k-means对用户本身做聚类，不采用其它估计方式，如何给这名新用户推荐内容呢？</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/155251_fa97b307_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>换句话说，该用户的初始特征向量里全是0，预测评分的结果也全是0，无法为用户进行推荐，何解？</p>
<p><strong>采用对评分进行均值归一化的方式解决</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/155513_7cd0e234_5550632.png" alt=""><br>$$<br>r(i,j)=1时，计算每个电影的平均评分值，然后Y-\mu得到新的评分矩阵<br>$$<br>对于用户 j 在电影 i 上的评分，即可表示为：<br>$$<br>(\theta^{(j)})(x^{(i)}) + \mu_i<br>$$<br>对于此时的用户 5而言，用户的特征值就不全是 0 了。</p>
<br>

<h2 id="Part-16-学习大规模数据集"><a href="#Part-16-学习大规模数据集" class="headerlink" title="Part 16 学习大规模数据集"></a>Part 16 学习大规模数据集</h2><p>数据集在机器学习算法中是十分重要的一部分，如果现在给你一个 m = 1 亿 这么多条记录的数据集，对其做线性回归，其中计算偏导数项的求和所耗费的时间代价可想而知。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0225/160905_12d15b53_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>第一点，我们可以反思，在项目开始之前能不能只用1000条数据去训练模型呢？</p>
<p>第二点，我们可以借助学习曲线，来评估一个算法的高偏差方差问题，进而在决定是否增添数据。如果是高偏差问题，<strong>不如再仔细设计一下特征向量，或者是神经网的隐藏层数。增加数据的做法，并不会改变现状。</strong><br>虽说如此，当我们如果真的要处理大规模数据集时，总得有方法来处理它，本章内容就是介绍一些处理大规模数据集的方法。它们是<strong>随机梯度下降(Stochastic gradient decent)</strong>和<strong>减少映射(Map Reduce)</strong>还有其它的梯度下降计算技巧。</p>
<h3 id="16-1-随机梯度下降-Stochastic-gradient-decent"><a href="#16-1-随机梯度下降-Stochastic-gradient-decent" class="headerlink" title="16.1 随机梯度下降(Stochastic gradient decent)"></a>16.1 随机梯度下降(Stochastic gradient decent)</h3><p>在学习随机梯度下降之前，我们先复习一下我们之前经常用的批量梯度下降(Batch gradient decent)。<br><img src="https://images.gitee.com/uploads/images/2021/0227/155209_0d338c60_5550632.png" alt="输入图片说明" title="屏幕截图.png"><br>这可是老熟人了，一个熟悉的线性回归的假设函数，熟悉的代价函数，以及熟悉的梯度下降公式和它的可视化。我们考虑一下,当m = 5 亿时，这个偏导数计算是这样的（假设 n &gt;1000）：</p>
<ol>
<li><p>将 5 亿条记录全部读入内存中</p>
</li>
<li><p>此时我们特征值构成了一个 (5亿×n)的矩阵，计算 $x^T.dot((x.dot(theta)-y))$</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(theta,x,y,alpha=<span class="number">0.01</span>,iteration_cnt=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    m = theta.shape[<span class="number">0</span>] <span class="comment">#m = 5亿</span></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(iteration_cnt):</span><br><span class="line">        S = (<span class="number">1</span>/m)*(x.T.dot((x.dot(theta)-y)))<span class="comment">#向量化后计算偏导数</span></span><br><span class="line">        theta = theta - alpha*S</span><br><span class="line">        print(cost_function(x,y,theta))</span><br><span class="line">        <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>

<p>   生成一个 5亿维的向量，然后更新模型参数（迭代 1 次）</p>
</li>
<li><p>重复步骤 2，直到完成迭代次数。</p>
</li>
</ol>
<p>好家伙，内存爆不爆炸另说，光这矩阵转置和乘法就要命了。那么此时，我们想找到一种函数优化算法<strong>能不能每次只取一个样本做梯度下降，这即节省了内存（不用一次性全读取进来），又可以少算很多项</strong>。随机梯度下降就是干这个的，我们来看看它是咋玩的。<br>$$<br>cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\<br>J_{train}(\theta) = \frac{1}{m}\sum_{i=1}^mcost(\theta,(x^{(i)},y^{(i)}))<br>$$</p>
<p>随机梯度下降的算法流程如下:</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/151448_57775065_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>它是先对第一个样本做梯度下降，得到一组 θ<del>j</del> (1,2,…,n)，再对第二个样本做梯度下降更新 模型参数，….。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0227/164448_df3cf800_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>与批量梯度下降不同的是，随机梯度下降在更新模型参数时，是来一个样本点更新一次模型参数 θ，直到取完 5 亿个样本</strong>。而不是一次性计算 5亿个样本的求和值。画出随机梯度下降和批量梯度下降时的迭代过程示意图：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0227/165041_7cea3966_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>$\color{red}{红色}$为批量梯度下降的迭代路线，$\color{purple}{品红色}$为随机梯度下降的迭代路线。所以对于随机梯度下降而言，执行一次迭代基本上就可以找到模型参数了，如果不放心可以在[1,10]之间选择总迭代次数。</p>
<p>了解了随机梯度下降的原理和过程后。那么此时，新的问题诞生了，我们<strong>如何确保随机梯度下降算法收敛，学习率α如何确定？</strong></p>
<p>首先，回忆一下，在梯度下降中，我们<strong>每做完一次迭代后</strong>，会计算代价函数值J(θ)，来评估算法是否在收敛，或者算法由于学习率过大而直接导致错过最小值。回忆一下，代价函数的计算公式：<br>$$<br>J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}x^{(i)}-y^{(i)})^2<br>$$</p>
<p>对于大样本量的梯度下降，我们显然不希望每次迭代后，等待一段时间去计算这个式子。所以我们换一种思路。，对于每个样本点，我们还是具有如下公式：<br>$$<br>cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_{\theta}x^{(i)}-y^{(i)})^2<br>$$<br>然后，注意了，<strong>我们在每次参数列表更新前计算代价函数值</strong>。每隔1000次，记录一下之前计算代价函数值的平均值，记录下来以便画图使用。写成伪代码就类似于（c初始值为0）：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/152650_6962bd24_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那么将代价函数可视化之后，我们会得到若干种图像。</p>
<p><strong>第一种，正常收敛，蓝线为基础学习率，红线学习率比蓝线小</strong>：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/152904_9fa4c2ff_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>由于噪声的干扰，图像看上去歪歪扭扭，不过大体趋势是下降，证明算法在慢慢收敛。红线由于学习率小，最终收敛在最小值附近的结果相对准确。</p>
<p><strong>第二种，收集间隔比较大，蓝线为1000次收集一次代价函数值，红线为5000次收集一次代价函数值画图</strong>：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/153211_115a7ddb_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>由于红线每次取值较多，平均掉了噪声带来的影响，图像显得光滑了很多。从上图看，这也是梯度下降正常收敛的结果。</p>
<p><strong>第三种，反复横跳，蓝线为1000次采样，红线为5000次采样，酚酞粉为5000次采样</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/153529_b426f907_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>蓝线在1000次的采样频率下，蹦蹦跳跳。<strong>第一感觉是算法没有收敛，此时不妨将采样间隔变大一些，如红线所示</strong>，可以看出趋势还是在缓慢下降的，<strong>至少可以说明算法没有问题</strong>。但是，如果采样间隔扩大了，但是<strong>图像像酚酞粉一样，代价函数值基本不变，那么大概率是学习算法没有进行有效的学习，可以重新设计特征值和算法了</strong>。</p>
<p><strong>最后一种，火箭升空</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/153902_8f6bf457_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这种情况大多由于<strong>设置了过大的学习率</strong>，导致算法刀片超车，直接错过全局最小值。解决方案，调低学习率再试试。</p>
<p>有关学习率的设置方式，一般而言和梯度下降一样，取个（0.1~0.001）的数试一试，效果好的话再微调。全局保持不动。但也有另外一种做法，设学习率为：<br>$$<br>\alpha=\frac{const;1}{iteration_{count}+coust;2}<br>$$<br>易得，<strong>学习率随着迭代次数的增多而减小</strong>，普通的随机梯度下降可能会在全局最小值附近徘徊震荡，误差相对较大。而采样这种学习率的更新方式，就会让它<strong>越走越精准（离终点越近，迈的步子越小）</strong>。最终会呈现出这样的效果：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/154607_efb7fce5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>而不是这样的：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0301/154639_30538903_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>准确的代价是复杂，至于这两个常量如何确定，只能化身调参侠，慢慢去尝试。</p>
<h3 id="16-2-Mini-Batch-gradient-decent"><a href="#16-2-Mini-Batch-gradient-decent" class="headerlink" title="16.2 Mini-Batch gradient decent"></a>16.2 Mini-Batch gradient decent</h3><p>批量梯度下降算法每次使用m个样本做一次迭代，随机梯度下降算法每次使用一个样本做一次迭代。而mini-batch一次选择<strong>b个样本做一次迭代</strong>，这个b就是，算法所包含的参数。b的值取2~100之间。若取b=10,则梯度下降会变为：<br>$$<br>\theta_j:=\theta_j-\alpha*\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}\<br>i:=i+10<br>$$<br><img src="https://images.gitee.com/uploads/images/2021/0301/144409_e741abcf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>这样看起来，感觉mini-batch也没什么，不就是取了b组数据来做梯度下降吗？这不就比随机梯度下降多取了几条样本吗。<strong>其实，Mini-Batch是为向量化服务的。</strong></p>
<p>由于随机梯度下降每次只取一个样本，导致向量化之后，会形成一个(1×n)的矩阵（向量），矩阵的行数太小，向量化的意义不大。而如果用批量梯度下降，又会形成一个(m×n)的矩阵，行数太大，计算量过高。</p>
<p><strong>通过选择b的方式，控制每次梯度下降时特征矩阵X的行数，减少矩阵转置的计算量，提高算法整体性能，多次少量地进行梯度下降</strong>。</p>
<h3 id="16-3-在线学习（Online-Learning）"><a href="#16-3-在线学习（Online-Learning）" class="headerlink" title="16.3 在线学习（Online Learning）"></a>16.3 在线学习（Online Learning）</h3><p>在线学习适用于<strong>用户量大，并发数高的项目</strong>，不过，我更愿意把它称之为<strong>大数据杀熟算法</strong>。在线学习和普通的机器学习算法比，差距只是在<strong>数据集的使用上</strong>。我们首先例举出一个生活中的场景，假如你在某宝或某电商平台上购物，你现在想买一本《算法导论》，发现居然需要90块钱，感觉太贵，不买了。又过了7天，当你攒够了钱，再次打开网站的时候，发现同样的商品降价到了79块钱。这是为啥呢？其实在背后，这些平台上运行着一种<strong>分类算法</strong>，<strong>它会预测出用户是否购买某商品的概率，根据这个概率，电商平台再适当地提高或降低商品的价格。</strong>有的时候用户期望低，不想太多花钱，那就降价，少赚点。有时用户期望高，愿意花钱，那就多赚点。</p>
<p>我们假设这个算法是逻辑回归模型，最终我们要的就是它：$P(y=1;x,θ)$。</p>
<p>它是这样训练的：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0302/152947_eaa307f3_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>可以看出，该网站是来一个用户就收集一下用户地行为信息，不再采用线下的训练方式，故称在线学习。它的第二个应用就是<strong>点击量预测（Click Through Rate CTR）</strong>。什么是点击量预测呢？假设你是一个未来科技无限责任公司的电商平台部长，专门卖手机，现在你有1000种不同型号的手机，如果用户查询<strong>128T内存+50兆亿像素主摄+86核光量子CPU</strong>的手机，如何选择<strong>10台</strong>，放在首页上？根据手机的配置和用户搜索的条件，我们可以构造出一款手机的特征向量，依旧是计算$P(y=1;x,θ)$。y 代表拥有是否点击链接进入商品信息主页。那么每当有新用户登录平台，你就会获取到十条数据记录，用于算法的学习。</p>
<p>回到现实，在线学习还可以应用在舆情推荐，产品推荐等等方面。</p>
<p>说了这么多，其实就是一句话：<strong>在线学习应用在高用户高并发的项目中，不用这种方法也能做，核心算法长得一样。对偏好兴趣发生变化的场景更加适应。</strong></p>
<h3 id="16-4-减少映射-（Map-reduce）与数据并行（Data-parallel）"><a href="#16-4-减少映射-（Map-reduce）与数据并行（Data-parallel）" class="headerlink" title="16.4 减少映射 （Map-reduce）与数据并行（Data parallel）"></a>16.4 减少映射 （Map-reduce）与数据并行（Data parallel）</h3><p>减少映射是什么意思呢？</p>
<p>假设，现在我们有一个线性回归任务，有4亿条数据。<strong>我们显然不想只让一台电脑去计算，为了提高效率，我们采用归并的思想，将大问题化小，最后集中解决</strong>。看个图就都明白了：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0302/161653_79d023d7_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>当学习任务的梯度值可以求和的时候，我们才能拆成子集去做。</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0302/161849_28fcb274_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当然了，个人电脑的多核CPU间的并行计算是同样的道理，只是把电脑换成了CPU，而且没有了网络传输的延迟。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0302/162017_6166d5cb_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>一般而言，优秀的线性代数库都是自动采用并行计算的，所以这也就是为什么我们喜欢对学习问题向量化的原因。</p>
<br>

<h2 id="Part-17-机器学习实例：Photo-OCR"><a href="#Part-17-机器学习实例：Photo-OCR" class="headerlink" title="Part 17 机器学习实例：Photo OCR"></a>Part 17 机器学习实例：Photo OCR</h2><h3 id="17-1-Machine-Learning-Pipeline"><a href="#17-1-Machine-Learning-Pipeline" class="headerlink" title="17.1 Machine Learning Pipeline"></a>17.1 Machine Learning Pipeline</h3><p>在这一章，我们用一个机器学习实例项目来说明机器学习中的<strong>流水线思想（pipeline）</strong></p>
<p>Photo OCR全称<strong>Photo Optical Character Recognition</strong>，其目的在于，在图片中找到文本区，并将文本区中的文字提取出来。类似这样：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/152637_9b1f050a_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>那它需要几步才能做到呢？</p>
<ol>
<li>识别出文本区</li>
<li>将文本串分割成文字</li>
<li>进行文字分类，得到结果</li>
</ol>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/152842_85ada914_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>整个识别流程就是<strong>机器学习流水线</strong>。十分类似与<strong>面向过程的函数调用</strong>。</p>
<h5 id="0x00-滑动窗口"><a href="#0x00-滑动窗口" class="headerlink" title="0x00 滑动窗口"></a>0x00 滑动窗口</h5><p>先解决流水线中的第一个步骤，<strong>如何划分文本区？</strong>。从图片中识别出文本区和在图片中识别出行人，本质上是一样的，就是在<strong>一张大图片中，把识别结果用矩形框框出来即可。</strong>由于行人的矩形框无论远近都是可以等比例缩放的，我们先看看行人检测系统是如何训练的</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/153241_c444d52d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/153430_ec488dea_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>每一张图片都是（82*36）作为特征向量，一组行人照片为正类，另一组非行人照片为负类。</p>
<p>假设，我们依据数据集已经训练好了模型，下图为测试集中的一张照片，我们<strong>如何把人框出来？</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/153803_e0880845_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>我们可以<strong>任取一个矩形框（x*y）【多试几个不同大小的矩形】，取出的矩形按比例缩放至（82*36）,这个矩形就叫做滑动窗口</strong>。然后，我们从左向右遍历图片，每次走b个步长，单位为像素。不难看出，每次向右走一个像素是最准确的，也是计算代价最大的。</p>
<p>把整个图片遍历一遍。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/154808_f9c14b2f_5550632.gif" alt="输入图片说明" title="slide window.gif"></p>
<p>最终得到结果：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/155048_0291d57b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>文本识别同理，先训练让电脑知道哪里是文本区：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/155325_2963da1e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>虽说文本有长短，但是我们依旧用一个固定大小的矩形来当作滑动窗口。最终生成一个和原图片比例一模一样的副本，<strong>有字的地方为高亮，越亮为文字的概率越大</strong>。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/155546_1a5dd879_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>为了提高精度，我们使用放大算子，提高识别结果。具体来讲，就是一个像素一个像素遍历，如果周围10-15个像素为高亮，那么这一片区域均为高亮。最终效果如下：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/155909_109e266e_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最终我们丢弃掉那些奇奇怪怪的矩形（竖着写的文字怎么办？），并以此绘制矩形框。</p>
<p>第一步结束了，接着进行第二步，识别文本间隔，划分文本。训练方法同理：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/160156_aedb1aa7_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>依旧是使用一维的滑动窗口，对每个字符串进行分割：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/160322_0332a8d5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>最后一步，n元分类问题，在此就不赘述了。</p>
<h3 id="17-2-数据获取和人工数据合成"><a href="#17-2-数据获取和人工数据合成" class="headerlink" title="17.2 数据获取和人工数据合成"></a>17.2 数据获取和人工数据合成</h3><p>为了得到一个可靠的机器学习系统，我们喜欢<strong>构造一个低偏差的模型，用大量数据去训练模型</strong>，可是数据从哪来呢？</p>
<p>在机器学习中有个思想，叫做<strong>人工数据合成</strong>。没有数据，咱们就自己创造数据，当然得靠谱地创造数据，以Photo OCR为例，看看怎么造数据。下图是一个真实的数据集，我们的任务是<strong>识别出小正方形最中央的字母</strong>：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/161145_fd9bda32_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>为了扩大该数据集，我们可以应用不同的字体，或者旋转，剪切，等等操作。并将它们粘到不同的背景中去。然后我们得到了人造数据集：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/161348_ebd70649_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>上述方案，可以让你从零创造数据集，但是当你已经有了一个数据集时，我们也可以对其进行魔改，达到扩大数据集的效果：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/161627_5e7a0d4d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>当然了，在对数据做失真处理的时候，一定是合乎常理有意义的，即<strong>生活中存在的</strong>。</p>
<p>最后再次强调，<strong>于其花大代价创造数据集，不如先做出一个低偏差的模型来的实在。</strong></p>
<h3 id="17-3-上限分析（Ceiling-Analysis）"><a href="#17-3-上限分析（Ceiling-Analysis）" class="headerlink" title="17.3 上限分析（Ceiling Analysis）"></a>17.3 上限分析（Ceiling Analysis）</h3><p>通过上限分析，我们能够评估出在流水线中，哪些模块是最值得花时间改进的。以Photo OCP为例。</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/163301_6dfcffe1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>在这个例子中，对一个模型的好坏的评判标准之一就是，<strong>识别准确率</strong>。那么我们需要做如下的事情：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/163958_0938760b_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>首先整体跑一边，得到模型的准确率，上图中为72%。</p>
<p>然后，我们使用“人工智能”，在第一步的文本区检测中人工地进行划分，把正确地结果告诉算法，然后再进行剩余步骤，模型准确率为89%</p>
<p>同理，在第二步，我们直接将人工划分好的数据喂给模型，最后跑出来的准确率是90%。</p>
<p>最后，我们认为每次分类都是正确的，当然最终准确率为100%。</p>
<p>这样我们就能看出<strong>改善文本区识别模块可以让模型提高17%的准确率，空间最大，而改善字符分割的作用不大，就只能提高1%的准确率。</strong>至此，<strong>我们抓住了事务的主要矛盾</strong>。有了模型优化的方向。</p>
<p>再举一个例子，现在有一个人脸识别的任务（只是一个例子，实际上远比这个复杂的多），你想知道照片里的人你到底是不是你的朋友。我们设计如下的工作流：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/164836_96046c9d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<ol>
<li>去掉图片背景</li>
<li>找到人脸</li>
<li>找到五官特征</li>
<li>进入逻辑回归分类器</li>
<li>得到标签</li>
</ol>
<p>对于该任务做上限分析，得到的结果如下：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/165159_009ae726_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>不难看出，将精力投入在人脸检测上对整体效果影响最大，其次五官中，眼睛的分割和嘴巴的分割。</p>
<br>

<h2 id="Part-18-总结与感谢"><a href="#Part-18-总结与感谢" class="headerlink" title="Part 18 总结与感谢"></a>Part 18 总结与感谢</h2><p>在本次学习中我学到了以下内容：</p>
<ol>
<li>监督学习算法：线性回归，逻辑回归，神经网络，SVM</li>
<li>无监督学习算法：K-means，PCA，异常检测</li>
<li>特殊应用：推荐系统和用于大数据杀熟的在线学习</li>
<li>一些机器学习的经验方法：偏差/方差，正则化，如何评估机器学习算法的优劣（召回率，查准率，F<del>1</del>Score），学习曲线，误差分析，工作流。</li>
</ol>
<p><img src="https://images.gitee.com/uploads/images/2021/0303/170408_2fb9acad_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>至此，课程完结，感谢吴恩达老师的教诲，感谢大家能耐着性子把本篇博客看完。谢谢大家。</p>
<br>

<h2 id="Appendix-作业及解析"><a href="#Appendix-作业及解析" class="headerlink" title="Appendix 作业及解析"></a>Appendix 作业及解析</h2><p>附录中的作业均出自于：</p>
<p><a href="https://github.com/Ayatans/Machine-Learning-homework" target="_blank" rel="noopener">https://github.com/Ayatans/Machine-Learning-homework</a></p>
<p>中文解析包括在该项目内，以下内容均为练习参考使用，请酌情观看。</p>
<h3 id="Programming-Exercise-1-Linear-Regression"><a href="#Programming-Exercise-1-Linear-Regression" class="headerlink" title="Programming Exercise 1 Linear Regression"></a>Programming Exercise 1 Linear Regression</h3><p><strong>题目概述</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In this part of this exercise, you will implement linear regression with one</span><br><span class="line">variable to predict profits for a food truck. Suppose you are the CEO of a</span><br><span class="line">restaurant franchise and are considering different cities for opening a new</span><br><span class="line">outlet. The chain already has trucks in various cities and you have data for</span><br><span class="line">profits and populations from the cities.</span><br><span class="line">You would like to use this data to help you select which city to expand</span><br><span class="line">to next.</span><br><span class="line">The file ex1data1.txt contains the dataset for our linear regression prob-</span><br><span class="line">lem. The first column is the population of a city and the second column is</span><br><span class="line">the profit of a food truck in that city. A negative value for profit indicates a</span><br><span class="line">loss.</span><br><span class="line">The ex1.m script has already been set up to load this data for you.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参考翻译：</span><br><span class="line">在本次练习中，你将会实现一个单变量线性回归模型来预测一卡车食物的利润。</span><br><span class="line">假设你是一个特许经营餐厅的CEO，希望打通不同城市间的一条新销路。在不同城市的供应链上已经存在了很多卡车，同时你拥有每座城市的人口和利润信息。</span><br><span class="line">你想使用这些信息来帮助你决策下一个生意扩张的城市。</span><br><span class="line">ex1data1.txt 包含了线性回归模型所需要的数据集，第一列为人口，第二列为一卡车食物的利润，负值代表亏损。</span><br><span class="line">ex1.m 中的脚本已经帮助你加载了这些数据</span><br></pre></td></tr></table></figure>

<p>单变量线性回归模型：</p>
<blockquote>
<p>x0 = 1</p>
<p>x1 = 城市人口</p>
<p>y = 利润</p>
</blockquote>
<p><strong>Warmup</strong></p>
<p>生成一个五维单位矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">m = np.eye(<span class="number">5</span>)</span><br><span class="line">print(m)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[1. 0. 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. 0. 0.]</span><br><span class="line"> [0. 0. 1. 0. 0.]</span><br><span class="line"> [0. 0. 0. 1. 0.]</span><br><span class="line"> [0. 0. 0. 0. 1.]]</span><br></pre></td></tr></table></figure>

<p><strong>数据集可视化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_data_set</span><span class="params">()</span>:</span></span><br><span class="line">    data = np.loadtxt(FILE_PATH, delimiter=<span class="string">','</span>)</span><br><span class="line">    x = data[:,<span class="number">0</span>]</span><br><span class="line">    y = data[:,<span class="number">1</span>]</span><br><span class="line">    plt.scatter(x,y,marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"population"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"profit"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/1216/152718_d976c487_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>下图为作业中自带的参考答案：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1216/152751_cb2a27a9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>代价函数的实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    predict_value = hypothesis_function(theta,x)</span><br><span class="line">    sqrErrors = (predict_value - y)**<span class="number">2</span></span><br><span class="line">    cost = <span class="number">1</span> /(<span class="number">2</span>*m)*sum(sqrErrors)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">运行结果:</span><br><span class="line">32.072733877455654</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2020/1216/154920_6dbcd9cf_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>未实现均值归一化前的梯度下降实现</strong></p>
<p>要求：学习率为：0.01,迭代次数为：1500</p>
<p>我设置的参数为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha&#x3D;0.0001,iteration_cnt&#x3D;1000</span><br></pre></td></tr></table></figure>

<p>其中正规方程组法得到的参数为: [-3.89578088  1.19303364]<br>代价函数值为：4.476971375975179</p>
<p>梯度下降法得到的参数为：[-2.2379466   1.02648621]</p>
<p>代价函数值为：4.727185425388962</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1216/173525_dc98af68_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">FILE_PATH = <span class="string">r"ex1data1.txt"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_x</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param file_path: </span></span><br><span class="line"><span class="string">    :return: vectorX(97,2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    data[:,<span class="number">1</span>] = data[:,<span class="number">0</span>]</span><br><span class="line">    data[:,<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(theta,x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :param x: </span></span><br><span class="line"><span class="string">    :return: theta0*x0+theta1*x1(x0 = 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> x.dot(theta)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    predict_value = hypothesis_function(theta,x)</span><br><span class="line">    sqrErrors = (predict_value - y)**<span class="number">2</span></span><br><span class="line">    cost = <span class="number">1</span> /(<span class="number">2</span>*m)*sum(sqrErrors)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(theta,x,y,alpha=<span class="number">0.0001</span>,iteration_cnt=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    m = theta.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(iteration_cnt):</span><br><span class="line">        S = (<span class="number">1</span>/m)*(x.T.dot((x.dot(theta)-y)))</span><br><span class="line">        theta = theta - alpha*S</span><br><span class="line">    print(cost_function(x,y,theta))</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equation</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    res = np.linalg.pinv(x.T.dot(x)).dot(x.T).dot(y)</span><br><span class="line">    <span class="keyword">return</span>  res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    theta = np.array([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    x = get_x(FILE_PATH)</span><br><span class="line">    y = np.loadtxt(FILE_PATH,delimiter=<span class="string">','</span>)</span><br><span class="line">    y = y[:,<span class="number">1</span>]</span><br><span class="line">    theta = gradient_decent(theta,x,y)</span><br><span class="line">    print(theta)</span><br><span class="line">    theta_normal_equation = normal_equation(x,y)</span><br><span class="line">    print(theta_normal_equation)</span><br><span class="line">    print(cost_function(x,y,theta_normal_equation))</span><br><span class="line">    draw_data_set(x,theta)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_data_set</span><span class="params">(feature_x,theta)</span>:</span></span><br><span class="line">    data = np.loadtxt(FILE_PATH, delimiter=<span class="string">','</span>)</span><br><span class="line">    x = data[:,<span class="number">0</span>]</span><br><span class="line">    y = data[:,<span class="number">1</span>]</span><br><span class="line">    predict_y = feature_x.dot(theta)</span><br><span class="line">    plt.scatter(x,y,marker=<span class="string">'x'</span>,c=<span class="string">"red"</span>,label=<span class="string">"Training data"</span>)</span><br><span class="line">    plt.plot(x,predict_y,label=<span class="string">"Linear regression"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">"population"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"profit"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p><strong>实现均值归一化后的梯度下降实现</strong></p>
<p>我设置的参数为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alpha&#x3D;0.01,iteration_cnt&#x3D;1000</span><br></pre></td></tr></table></figure>

<p>其中正规方程组法得到的参数为: [-3.89578088  1.19303364]<br>代价函数值为：4.476971375975179</p>
<p>梯度下降法得到的参数为：[0.69726026 9.73491593]</p>
<p>代价函数值为：4.476971375975179</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1217/145642_5c59c1c1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">FILE_PATH = <span class="string">r"ex1data1.txt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_x</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param file_path: </span></span><br><span class="line"><span class="string">    :return: vectorX(97,2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    data[:,<span class="number">1</span>] = data[:,<span class="number">0</span>]</span><br><span class="line">    data[:,<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(theta,x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :param x: </span></span><br><span class="line"><span class="string">    :return: theta0*x0+theta1*x1(x0 = 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> x.dot(theta)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    predict_value = hypothesis_function(theta,x)</span><br><span class="line">    sqrErrors = (predict_value - y)**<span class="number">2</span></span><br><span class="line">    cost = <span class="number">1</span> /(<span class="number">2</span>*m)*sum(sqrErrors)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(theta,x,y,alpha=<span class="number">0.01</span>,iteration_cnt=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    m = theta.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(iteration_cnt):</span><br><span class="line">        S = (<span class="number">1</span>/m)*(x.T.dot((x.dot(theta)-y)))</span><br><span class="line">        theta = theta - alpha*S</span><br><span class="line">    print(cost_function(x,y,theta))</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_equation</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    res = np.linalg.pinv(x.T.dot(x)).dot(x.T).dot(y)</span><br><span class="line">    <span class="keyword">return</span>  res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">    sigma = np.mean(x,axis=<span class="number">0</span>)</span><br><span class="line">    mu = np.std(x,axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> sigma[j] == <span class="number">0</span>:</span><br><span class="line">            sigma[j] = <span class="number">1e-30</span></span><br><span class="line">        x[:,j] = (x[:,j] - mu[j])/sigma[j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    theta = np.array([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    x = get_x(FILE_PATH)</span><br><span class="line">    y = np.loadtxt(FILE_PATH,delimiter=<span class="string">','</span>)</span><br><span class="line">    y = y[:,<span class="number">1</span>]</span><br><span class="line">    new_x = feature_normalize(x)</span><br><span class="line">    new_theta = gradient_decent(np.array([<span class="number">0</span>,<span class="number">0</span>]),new_x,y)</span><br><span class="line">    print(<span class="string">"new theta: "</span> + str(new_theta))</span><br><span class="line">    print(<span class="string">"cost of feature normalization: %.2f"</span> % cost_function(x, y, new_theta))</span><br><span class="line"></span><br><span class="line">    draw_data_set(x,new_theta)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_data_set</span><span class="params">(feature_x,theta)</span>:</span></span><br><span class="line">    data = np.loadtxt(FILE_PATH, delimiter=<span class="string">','</span>)</span><br><span class="line">    x = data[:,<span class="number">0</span>]</span><br><span class="line">    y = data[:,<span class="number">1</span>]</span><br><span class="line">    predict_y = feature_x.dot(theta)</span><br><span class="line">    plt.scatter(x,y,marker=<span class="string">'x'</span>,c=<span class="string">"red"</span>,label=<span class="string">"Training data"</span>)</span><br><span class="line">    plt.plot(x,predict_y,label=<span class="string">"Linear regression"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">"population"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"profit"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h3 id="Programming-Exercise-2-Logistics-Regression"><a href="#Programming-Exercise-2-Logistics-Regression" class="headerlink" title="Programming Exercise 2 Logistics Regression"></a>Programming Exercise 2 Logistics Regression</h3><p><strong>题目概述</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In this part of the exercise, you will build a logistic regression model to</span><br><span class="line">predict whether a student gets admitted into a university.</span><br><span class="line">Suppose that you are the administrator of a university department and</span><br><span class="line">you want to determine each applicant&#39;s chance of admission based on their</span><br><span class="line">results on two exams. You have historical data from previous applicants</span><br><span class="line">that you can use as a training set for logistic regression. For each training</span><br><span class="line">example, you have the applicant&#39;s scores on two exams and the admissions</span><br><span class="line">decision.</span><br><span class="line">Your task is to build a classification model that estimates an applicant&#39;s</span><br><span class="line">probability of admission based the scores from those two exams. This outline</span><br><span class="line">and the framework code in ex2.m will guide you through the exercise.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">参考翻译：</span><br><span class="line">在本次练习中，你将会训练一个逻辑回归模型来预测一个学生是否能进入大学。</span><br><span class="line">假设你是一名大学招生办的管理员，你希望通过申请人的两项考试成绩来决定他们是否有升学的机会。</span><br><span class="line">你有可以用于进行逻辑回归模型训练的之前申请人的历史数据。</span><br><span class="line">对于每一个训练数据，它会提供申请人的两项考试成绩和决策结果。</span><br><span class="line">你的任务是训练一个分类模型以用于评估申请人基于两项考试成绩申请的通过可能性。</span><br><span class="line">大纲和框架代码将会在 ex2.m 中知道你完成此次练习。</span><br></pre></td></tr></table></figure>

<p><strong>数据可视化</strong></p>
<p>下面这句话是作业文档中的原话，我觉得很有价值，便抄录了下来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Before starting to implement any learning algorithm, it is always good to</span><br><span class="line">visualize the data if possible.</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">FILE_PATH = <span class="string">r"ex2data1.txt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    res = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">data = get_data(FILE_PATH)</span><br><span class="line">x = data[:,<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">y = data[:,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res1_x1 = []</span><br><span class="line">res1_x2 = []</span><br><span class="line"></span><br><span class="line">res2_x1 = []</span><br><span class="line">res2_x2 = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> y[i] == <span class="number">0</span>:</span><br><span class="line">        res1_x1.append(x[i,<span class="number">0</span>])</span><br><span class="line">        res1_x2.append(x[i,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">elif</span> y[i] == <span class="number">1</span>:</span><br><span class="line">        res2_x1.append(x[i, <span class="number">0</span>])</span><br><span class="line">        res2_x2.append(x[i, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(res1_x1,res1_x2,marker=<span class="string">'x'</span>,c=<span class="string">'r'</span>,label=<span class="string">'Not admitted'</span>)</span><br><span class="line">plt.scatter(res2_x1,res2_x2,marker=<span class="string">'o'</span>,c=<span class="string">'b'</span>,label=<span class="string">'Admitted'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Exam 1 score"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Exam 2 score"</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>数据集可视化效果图：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1218/110057_0f8ce02d_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>作业文档中的参考图像：</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1218/105850_f0038aa5_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>Warmup-实现sigmoid函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(z)</span>:</span></span><br><span class="line">    result = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(feature_x,theta)</span>:</span></span><br><span class="line">    z = feature_x.dot(theta)</span><br><span class="line">    <span class="keyword">return</span> g(z)</span><br><span class="line"></span><br><span class="line">print(g(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<p><strong>实现代价函数和梯度下降</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">FILE_PATH = <span class="string">r"ex2data1.txt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    res = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    result = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(feature_x,theta)</span>:</span></span><br><span class="line">    z = feature_x.dot(theta)</span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(x,y,theta)</span>:</span></span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 1/m*np.sum(-y.dot(np.log(h(x))) - (1-y).dot(np.log(1-h(x))))</span></span><br><span class="line">    cost = <span class="number">1</span>/m*((-y.dot(np.log(hypothesis_function(x,theta)))) - ((<span class="number">1</span>-y).dot(np.log(<span class="number">1</span>-hypothesis_function(x,theta)))))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = get_data(FILE_PATH)</span><br><span class="line">    x = data[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    y = data[:, <span class="number">2</span>]</span><br><span class="line">    theta =  np.zeros((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    cost = cost_function(x,y,theta)</span><br><span class="line">    print(cost)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p>初始代价函数值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.69314718</span><br></pre></td></tr></table></figure>

<p>接着我们按照要求应该进行目标函数的最小化，但是题目要求使用Octave中的<strong>fminunc</strong>，由于我使用的是Python实现，这里使用了<strong>scipy中的optimize</strong>来类似计算。最终计算结果和Octave的一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> op</span><br><span class="line">FILE_PATH = <span class="string">r"ex2data1.txt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    res = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    z = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    J = (-np.dot(y.T, np.log(sigmoid(x.dot(theta)))) -</span><br><span class="line">         np.dot((<span class="number">1</span> - y).T, np.log(<span class="number">1</span> - sigmoid(x.dot(theta))))) / m</span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,x,y)</span>:</span></span><br><span class="line">    m, n = x.shape</span><br><span class="line">    theta = theta.reshape((n, <span class="number">1</span>))</span><br><span class="line">    grad = np.dot(x.T, sigmoid(x.dot(theta)) - y) / m</span><br><span class="line">    <span class="keyword">return</span> grad.flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_normalize</span><span class="params">(x)</span>:</span></span><br><span class="line">    sigma = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    mu = np.std(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, x.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> sigma[j] == <span class="number">0</span>:</span><br><span class="line">            sigma[j] = <span class="number">1e-30</span></span><br><span class="line">        x[:, j] = (x[:, j] - mu[j]) / sigma[j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_y</span><span class="params">(x,theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1*0 +theta1*x1 + theta2*x2 = 0</span></span><br><span class="line"><span class="string">    x2 = -theta1*x1/theta2</span></span><br><span class="line"><span class="string">    :param x: </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> -theta[<span class="number">0</span>]*x/theta[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_pict</span><span class="params">(data,theta)</span>:</span></span><br><span class="line">    x = data[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    y = data[:, <span class="number">2</span>]</span><br><span class="line">    res1_x1 = []</span><br><span class="line">    res1_x2 = []</span><br><span class="line">    res2_x1 = []</span><br><span class="line">    res2_x2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> y[i] == <span class="number">0</span>:</span><br><span class="line">            res1_x1.append(x[i, <span class="number">0</span>])</span><br><span class="line">            res1_x2.append(x[i, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> y[i] == <span class="number">1</span>:</span><br><span class="line">            res2_x1.append(x[i, <span class="number">0</span>])</span><br><span class="line">            res2_x2.append(x[i, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    plt.scatter(res1_x1, res1_x2, marker=<span class="string">'x'</span>, c=<span class="string">'r'</span>, label=<span class="string">'Not admitted'</span>)</span><br><span class="line">    plt.scatter(res2_x1, res2_x2, marker=<span class="string">'o'</span>, c=<span class="string">'b'</span>, label=<span class="string">'Admitted'</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(x[:,<span class="number">0</span>],calc_y(x[:,<span class="number">0</span>],theta))</span><br><span class="line">    plt.xlabel(<span class="string">"Exam 1 score"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Exam 2 score"</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_data</span><span class="params">(x)</span>:</span></span><br><span class="line">    m,n = x.shape</span><br><span class="line">    initial_theta = np.zeros(n + <span class="number">1</span>)</span><br><span class="line">    vector_one =  np.ones((m,<span class="number">1</span>))</span><br><span class="line">    x = np.column_stack((vector_one,x))</span><br><span class="line">    <span class="keyword">return</span> x,initial_theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    m = np.size(data, <span class="number">0</span>)</span><br><span class="line">    x=data[:,<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    y=data[:,<span class="number">2</span>]</span><br><span class="line">    y=y.reshape((m,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data = get_data(FILE_PATH)</span><br><span class="line">    x,y = reshape_data(data)</span><br><span class="line">    x,initial_theta = init_data(x)</span><br><span class="line"></span><br><span class="line">    result = op.minimize(fun=cost_function, x0=initial_theta, args=(x, y), method=<span class="string">'TNC'</span>, jac=gradient)</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   fun: array([0.2034977])</span><br><span class="line">    jac: array([9.17727211e-09, 1.02335214e-07, 4.81971997e-07])</span><br><span class="line">message: &#39;Local minimum reached (|pg| ~&#x3D; 0)&#39;</span><br><span class="line">   nfev: 36</span><br><span class="line">    nit: 17</span><br><span class="line"> status: 0</span><br><span class="line">success: True</span><br><span class="line">      x: array([-25.16131857,   0.20623159,   0.20147149])</span><br></pre></td></tr></table></figure>

<p>第一行是代价函数值：0.203，</p>
<p>最后一行是参数向量theta =[-25.16131857,   0.20623159,   0.20147149]</p>
<p>随后，我们使用这个参数进行画图。</p>
<p><img src="https://images.gitee.com/uploads/images/2020/1219/172625_cb1541c9_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<h4 id="2-2-带有的正则化逻辑回归"><a href="#2-2-带有的正则化逻辑回归" class="headerlink" title="2.2 带有的正则化逻辑回归"></a>2.2 带有的正则化逻辑回归</h4><p><strong>题目概述</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In this part of the exercise, you will implement regularized logistic regression</span><br><span class="line">to predict whether microchips from a fabrication plant passes quality assur-</span><br><span class="line">ance (QA). During QA, each microchip goes through various tests to ensure</span><br><span class="line">it is functioning correctly.</span><br><span class="line">Suppose you are the product manager of the factory and you have the</span><br><span class="line">test results for some microchips on two different tests. From these two tests,</span><br><span class="line">you would like to determine whether the microchips should be accepted or</span><br><span class="line">rejected. To help you make the decision, you have a dataset of test results</span><br><span class="line">on past microchips, from which you can build a logistic regression model.</span><br><span class="line">You will use another script, ex2 reg.m to complete this portion of the</span><br><span class="line">exercise.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在本部分的练习中，你将实现一个带有正则化的逻辑回归模型，用于预测从制造工厂产出的微芯片是否能通过质量保证（QA）</span><br><span class="line">在QA的时候，每个微芯片都会进行一系列测试来保证其功能的正确性。</span><br><span class="line">假设你是工厂的产品经理，你有一些微芯片的两种不同测试的结果。通过这两项测试，你会决策出一个微芯片是否应当接受或拒绝出厂。</span><br><span class="line">为了帮助你进行选择，你有一个关于之前微芯片的测试数据集，以便于构建逻辑回归模型。</span><br><span class="line">你将使用另外一个脚本，ex2 reg.m 来完成这一部分的练习。</span><br></pre></td></tr></table></figure>

<p><strong>数据集可视化</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1222/132416_3d3f5fbe_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>特征映射</strong></p>
<p>为了能够拟合这个分散的数据集，普通的直线分类已经不适用，所以我们要把<strong>2维原始特征映射成一个28维特征</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2020/1222/132738_3ed9f71f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_map</span><span class="params">(X,exponent=<span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    特征映射,将二维特征向量映射为 28 维特征向量</span></span><br><span class="line"><span class="string">    :param X: </span></span><br><span class="line"><span class="string">    :return: res_feature</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x1 = X[:,<span class="number">0</span>]</span><br><span class="line">    x2 = X[:,<span class="number">1</span>]</span><br><span class="line">    res_feature = np.ones((X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,exponent+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,i+<span class="number">1</span>):</span><br><span class="line">            new_feature = (x1**(i-j))*(x2**j)</span><br><span class="line">            res_feature = np.column_stack((res_feature,new_feature))</span><br><span class="line">    <span class="keyword">return</span> res_feature</span><br></pre></td></tr></table></figure>

<p><strong>代价函数和梯度</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> op</span><br><span class="line"></span><br><span class="line">FILE_PATH = <span class="string">r"ex2data2.txt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    data = np.loadtxt(file_path,delimiter=<span class="string">','</span>)</span><br><span class="line">    X = data[:,<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    y = data[:,<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> X,y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_map</span><span class="params">(X,exponent=<span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    特征映射,将二维特征向量映射为 28 维特征向量</span></span><br><span class="line"><span class="string">    :param X: </span></span><br><span class="line"><span class="string">    :param exponent: </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x1 = X[:,<span class="number">0</span>]</span><br><span class="line">    x2 = X[:,<span class="number">1</span>]</span><br><span class="line">    res_feature = np.ones((X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,exponent+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,i+<span class="number">1</span>):</span><br><span class="line">            new_feature = (x1**(i-j))*(x2**j)</span><br><span class="line">            res_feature = np.column_stack((res_feature,new_feature))</span><br><span class="line">    <span class="keyword">return</span> res_feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(theta,X,y,my_lambda = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算代价函数值</span></span><br><span class="line"><span class="string">    :param X: 训练集</span></span><br><span class="line"><span class="string">    :param y: 标签</span></span><br><span class="line"><span class="string">    :param theta: 模型参数</span></span><br><span class="line"><span class="string">    :param my_lambda: 正则化参数</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    regularization = (my_lambda/<span class="number">2</span>*m)*theta.T.dot(theta)</span><br><span class="line">    left = np.dot((-y).T,np.log(hypothesis_function(theta,X)))</span><br><span class="line">    right = np.dot((<span class="number">1</span>-y).T,np.log(<span class="number">1</span> - hypothesis_function(theta,X)))</span><br><span class="line">    cost = (left-right)/m+regularization</span><br><span class="line">    <span class="comment">#print(cost)</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(theta,X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param theta: (28, 1)</span></span><br><span class="line"><span class="string">    :param X: (118, 28)</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># print(X.shape)</span></span><br><span class="line">    <span class="comment"># print(theta.shape)</span></span><br><span class="line">    z = X.dot(theta)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_theta</span><span class="params">(m)</span>:</span></span><br><span class="line">    res_theta = np.zeros((m,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> res_theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,X,y,my_lambda = <span class="number">1</span>)</span>:</span></span><br><span class="line">    x_0 = X[:, <span class="number">0</span>]</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    temp = hypothesis_function(theta,X).flatten()-y <span class="comment">#降维成118向量后对应做差</span></span><br><span class="line"></span><br><span class="line">    partial_derivative_theta_0 = (<span class="number">1</span>/m)*np.dot(temp,x_0)</span><br><span class="line">    x_remain = X[:,<span class="number">1</span>:].T</span><br><span class="line">    tail = (my_lambda/m)*theta[<span class="number">1</span>:]<span class="comment">#(27,1)</span></span><br><span class="line">    partial_derivative_theta_j = (<span class="number">1</span>/m)*np.dot(x_remain,temp)-tail.flatten() <span class="comment">#j&gt;=1 时的偏导数</span></span><br><span class="line">    res_partial_derivative = np.append(partial_derivative_theta_0,partial_derivative_theta_j)</span><br><span class="line">    <span class="keyword">return</span> res_partial_derivative</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    X,y = get_data(FILE_PATH)</span><br><span class="line">    new_X = feature_map(X,exponent=<span class="number">6</span>)</span><br><span class="line">    theta = initialize_theta(new_X.shape[<span class="number">1</span>])</span><br><span class="line">    theta = theta.flatten()</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    optimize.minimize(target_fun,init_val,method,jac,hess) </span></span><br><span class="line"><span class="string">    fun：待优化函数的表达式计算； </span></span><br><span class="line"><span class="string">    X0：初始值；</span></span><br><span class="line"><span class="string">    args:元组，给待优化函数的参数</span></span><br><span class="line"><span class="string">    method：最小化的算法； </span></span><br><span class="line"><span class="string">    jac：雅各比矩阵 </span></span><br><span class="line"><span class="string">    注意，在optimize.minimize函数中，优化函数和梯度函数的第一个参数必须是theta，且shape为(x,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    gradient(theta,new_X,y)</span><br><span class="line">    result = op.minimize(fun=cost_function, x0=theta, args=(new_X, y), method=<span class="string">'TNC'</span>, jac=gradient)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>

<p>注意：$\color{red}{在optimize.minimize函数中，优化函数和梯度函数的第一个参数必须是theta，且shape为(x,)}$</p>
<p>函数优化结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    fun: 0.6931386454411619</span><br><span class="line">    jac: array([8.47340040e-03, 1.87875076e-02, 7.88189819e-05, 5.03437256e-02,</span><br><span class="line">      1.15006836e-02, 3.76665340e-02, 1.83553987e-02, 7.32380104e-03,</span><br><span class="line">      8.19162502e-03, 2.34772285e-02, 3.93479496e-02, 2.23843278e-03,</span><br><span class="line">      1.28603353e-02, 3.09512924e-03, 3.93037093e-02, 1.99700945e-02,</span><br><span class="line">      4.32950572e-03, 3.38608692e-03, 5.83832901e-03, 4.47555784e-03,</span><br><span class="line">      3.10079314e-02, 3.10304053e-02, 1.09681899e-03, 6.31578628e-03,</span><br><span class="line">      4.08091516e-04, 7.26520038e-03, 1.37565475e-03, 3.87935191e-02])</span><br><span class="line">message: &#39;Linear search failed&#39;</span><br><span class="line">   nfev: 107</span><br><span class="line">    nit: 1</span><br><span class="line"> status: 4</span><br><span class="line">success: False</span><br><span class="line">      x: array([ 1.14616634e-04,  1.03955049e-04,  1.46947914e-04, -2.47073240e-04,</span><br><span class="line">      -2.76692230e-04, -2.36092801e-04,  1.18886735e-04, -1.04813843e-04,</span><br><span class="line">      -1.11375838e-04,  3.79573444e-05, -1.38756231e-04, -3.21762115e-05,</span><br><span class="line">      -1.32052923e-04, -8.67618336e-05, -1.12277115e-04,  8.79636146e-05,</span><br><span class="line">      -4.69546463e-05, -2.06384068e-05, -6.52112561e-05, -7.13623836e-05,</span><br><span class="line">       7.54429159e-05, -4.18552393e-05,  5.41398980e-06, -6.25800757e-05,</span><br><span class="line">      -7.69406082e-06, -6.48878220e-05, -3.39848194e-05,  1.71874064e-05])</span><br></pre></td></tr></table></figure>

<p>从上图可以看出，我们的代价函数值为  0.693，和答案所给的一致，我们忽略掉failed和False，直接用这组模型参数进行绘图。</p>
<p><strong>绘制不同正则化参数下的决策边界</strong></p>
<p>画图函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_result</span><span class="params">(path,opt_theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param path: 数据集路径</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    绘制数据集-Start</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data1 = np.loadtxt(path, delimiter=<span class="string">','</span>)</span><br><span class="line">    pos_index = np.where(data1[:, <span class="number">2</span>] == <span class="number">1</span>)</span><br><span class="line">    neg_index = np.where(data1[:, <span class="number">2</span>] == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(data1[pos_index, <span class="number">0</span>], data1[pos_index, <span class="number">1</span>], marker=<span class="string">'o'</span>,c=<span class="string">'b'</span>,label=<span class="string">'Accepted'</span>,s=<span class="number">5</span>)</span><br><span class="line">    plt.scatter(data1[neg_index, <span class="number">0</span>], data1[neg_index, <span class="number">1</span>], marker=<span class="string">'x'</span>,c=<span class="string">'r'</span>,label=<span class="string">'Rejected'</span>)</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    绘制数据集-End</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    绘制决策边界-Start</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    xx = np.linspace([<span class="number">-1</span>],[<span class="number">1.5</span>],<span class="number">30</span>)<span class="comment">#生成 from -1 to 1.5，中间有30项的等差数列</span></span><br><span class="line">    yy = np.linspace([<span class="number">-1</span>],[<span class="number">1.5</span>],<span class="number">30</span>)</span><br><span class="line">    <span class="comment"># 对x,y网格化，得到X,Y</span></span><br><span class="line">    X, Y = np.meshgrid(xx, yy)</span><br><span class="line">    z = np.zeros((xx.size, yy.size))<span class="comment">#等高线的高</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, xx.size):<span class="comment">#生成特征 1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, yy.size):<span class="comment">#生成特征 2</span></span><br><span class="line">            one_row = np.column_stack((xx[i], yy[j]))</span><br><span class="line">            value = np.dot(feature_map(one_row),opt_theta) <span class="comment">#预测结果</span></span><br><span class="line">            z[i][j] = value</span><br><span class="line">    z = z.T <span class="comment">#z矩阵的第一行对应数据集的第一列，所以要转置</span></span><br><span class="line">    plt.contour(X, Y, z, [<span class="number">0</span>], colors=<span class="string">'g'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    绘制决策边界-End</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    plt.xlabel(<span class="string">'Feature 1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Feature 2'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>正则化参数，λ = 1，代价函数 =  0.6931386454411619：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0315/114316_04fdcca2_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p> 正则化参数，λ = 0，代价函数值= 0.2598871533548602：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0315/120831_d84f5bfa_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<br>

<h3 id="Programming-Exercise-3-Multi-class-Classification-and-Neural-Networks"><a href="#Programming-Exercise-3-Multi-class-Classification-and-Neural-Networks" class="headerlink" title="Programming Exercise 3 Multi-class Classification and Neural Networks"></a>Programming Exercise 3 Multi-class Classification and Neural Networks</h3><p><strong>问题描述</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In this exercise, you will implement one-vs-all logistic regression and neural networks to recognize hand-written digits.In the first part of the exercise, you will extend your previous implemention of logistic regression and apply it to one-vs-all classification.You are given a data set in ex3data1.mat that contains 5000 training examples of handwritten digits.</span><br><span class="line">There are 5000 training examples in ex3data1.mat, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is &quot;unrolled&quot; into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This gives us a 5000 by 400 matrix X where every row is a training example for a handwritten digit image.The second part of the training set is a 5000-dimensional vector y thatcontains labels for the training set. To make things more compatible with Octave&#x2F;MATLAB indexing, where there is no zero index, we have mappedthe digit zero to the value ten. Therefore, a &quot;0&quot; digit is labeled as &quot;10&quot;, while the digits &quot;1&quot; to &quot;9&quot; are labeled as &quot;1&quot; to &quot;9&quot; in their natural order.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">题目大意：</span><br><span class="line">在本次练习中，你将实现一个一对多的逻辑回归模型和神经网络来识别手写数字。在第一部分的练习中，你将拓展先前的逻辑回归，使应用到一对多分类问题中。5000个手写数字的数据集在ex3data1.mat中给出。5000个训练样例是由20*20像素的灰度图片构成的，每一个像素都是一个浮点数，用于描述其在当前区域的灰度强度，这些像素被展开成一个400维的向量，因此我们拥有一个5000*400的特征矩阵X，每一行都是一张手写数字图片。同理我们拥有一个5000维的标签向量,为了适应Octave&#x2F;MATLAB，没有0索引，数字0的标签为10，1-9为1-9</span><br></pre></td></tr></table></figure>

<p><strong>数据可视化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FILE_PATH = <span class="string">r"ex3data1.mat"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param file_path: </span></span><br><span class="line"><span class="string">    :return:data-&gt;dict,key="X",value="y"</span></span><br><span class="line"><span class="string">     X(5000, 400),y(5000, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = io.loadmat(file_path)</span><br><span class="line">    <span class="comment"># print(data["X"].shape)</span></span><br><span class="line">    <span class="comment"># print(data["y"].shape)</span></span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">"X"</span>],data[<span class="string">"y"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_image</span><span class="params">(X,n=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    从手写数字特征矩阵中随机拿出n*n个数字进行可视化</span></span><br><span class="line"><span class="string">    :param X: 手写数字数据集</span></span><br><span class="line"><span class="string">    :param n: n*n个数字</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    images = np.zeros((<span class="number">1</span>,<span class="number">400</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n*n):</span><br><span class="line">        pict_sub = np.random.randint(X.shape[<span class="number">0</span>])</span><br><span class="line">        image = X[pict_sub]</span><br><span class="line">        images = np.row_stack((images,image))</span><br><span class="line">    images = images[<span class="number">1</span>:,:]</span><br><span class="line">    fig, axes = plt.subplots(nrows=n, ncols=n)</span><br><span class="line">    sub = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,n):</span><br><span class="line">            axes[i,j].imshow(images[sub].reshape(<span class="number">20</span>,<span class="number">20</span>).T)</span><br><span class="line">            sub+=<span class="number">1</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X,y=load_data(FILE_PATH)</span><br><span class="line"></span><br><span class="line">show_image(X)</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2021/0316/220725_2e684151_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>向量化逻辑回归模型</strong></p>
<p>因为我们这次要分10类，所以要训练10个分类器，由此可以看出<strong>向量化非常重要！！</strong></p>
<p>在这里我就直接复用上一节中的代码了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> op</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis_function</span><span class="params">(theta,X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param theta: </span></span><br><span class="line"><span class="string">    :param X: </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># print(X.shape)</span></span><br><span class="line">    <span class="comment"># print(theta.shape)</span></span><br><span class="line">    z = X.dot(theta)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(theta,X,y,my_lambda = <span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算代价函数值</span></span><br><span class="line"><span class="string">    :param X: 训练集</span></span><br><span class="line"><span class="string">    :param y: 标签</span></span><br><span class="line"><span class="string">    :param theta: 模型参数</span></span><br><span class="line"><span class="string">    :param my_lambda: 正则化参数</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    regularization = (my_lambda/<span class="number">2</span>*m)*theta.T.dot(theta)</span><br><span class="line">    left = np.dot((-y).T,np.log(hypothesis_function(theta,X)))</span><br><span class="line">    right = np.dot((<span class="number">1</span>-y).T,np.log(<span class="number">1</span> - hypothesis_function(theta,X)))</span><br><span class="line">    cost = (left-right)/m+regularization</span><br><span class="line">    <span class="comment">#print(cost)</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta,X,y,my_lambda = <span class="number">0</span>)</span>:</span></span><br><span class="line">    x_0 = X[:, <span class="number">0</span>]</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    temp = hypothesis_function(theta,X).flatten()-y</span><br><span class="line"></span><br><span class="line">    partial_derivative_theta_0 = (<span class="number">1</span>/m)*np.dot(temp,x_0)</span><br><span class="line">    x_remain = X[:,<span class="number">1</span>:].T</span><br><span class="line">    tail = (my_lambda/m)*theta[<span class="number">1</span>:]</span><br><span class="line">    partial_derivative_theta_j = (<span class="number">1</span>/m)*np.dot(x_remain,temp)-tail.flatten() <span class="comment">#j&gt;=1 时的偏导数</span></span><br><span class="line">    res_partial_derivative = np.append(partial_derivative_theta_0,partial_derivative_theta_j)</span><br><span class="line">    <span class="keyword">return</span> res_partial_derivative</span><br></pre></td></tr></table></figure>

<p><strong>一对多分类</strong></p>
<p>现在我们进行一对多分类，对于本问题我们将对数字 1做一个分类器，数字 2做一个分类器……详情请看如下核心代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_vs_all</span><span class="params">(X,y,k,my_lambda = <span class="number">0</span>)</span>:</span></span><br><span class="line">    n = X.shape[<span class="number">1</span>]</span><br><span class="line">    theta_all = np.zeros((k,n))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">        theta_i = np.zeros(n)</span><br><span class="line">        res = op.minimize(cost_function,theta_i,args=(X,y==i,my_lambda),method=<span class="string">'TNC'</span>,jac=gradient)</span><br><span class="line">        <span class="comment"># print("res: ",i)</span></span><br><span class="line">        <span class="comment"># print(res)</span></span><br><span class="line">        theta_all[i<span class="number">-1</span>] = res.x</span><br><span class="line">    <span class="keyword">return</span> theta_all</span><br></pre></td></tr></table></figure>

<p><strong>结果展示</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">训练集上模型准确度为: 94.56%</span><br><span class="line">图片&#x3D;&#x3D;&gt; 0.png:识别该数字为:  0</span><br><span class="line">图片&#x3D;&#x3D;&gt; 1.png:识别该数字为:  5</span><br><span class="line">图片&#x3D;&#x3D;&gt; 2.png:识别该数字为:  2</span><br><span class="line">图片&#x3D;&#x3D;&gt; 3.png:识别该数字为:  3</span><br><span class="line">图片&#x3D;&#x3D;&gt; 4.png:识别该数字为:  4</span><br><span class="line">图片&#x3D;&#x3D;&gt; 5.png:识别该数字为:  5</span><br><span class="line">图片&#x3D;&#x3D;&gt; 6.png:识别该数字为:  6</span><br><span class="line">图片&#x3D;&#x3D;&gt; 7.png:识别该数字为:  1</span><br><span class="line">图片&#x3D;&#x3D;&gt; 8.png:识别该数字为:  5</span><br><span class="line">图片&#x3D;&#x3D;&gt; 9.png:识别该数字为:  3</span><br><span class="line">测试集上模型准确度为: 50.00%</span><br></pre></td></tr></table></figure>

<br>

<p><strong>问题描述</strong></p>
<p>在之前的练习中，我们实现了一个一对多的多元逻辑回归分类器用于手写数字识别。识别效果不甚理想，这是因为它仅仅只是一个线性分类器【你也可以使用多项式特征，但是会增加计算开销】，无法使用非线性假设函数来拟合数据集。</p>
<p>在本次练习中，你将在相同的数据集上实现一个神经网络，它可以表示更复杂的模型。在本次练习中，将使用官方已准备好的神经网络的模型参数。你的目标是使用已知权重矩阵实现一个<strong>前向传播模型</strong>，在下次练习中，你将自己实现<strong>反向传播算法来计算模型参数</strong>。</p>
<p>我们的神经网络架构如下图所示：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/0323/212933_076ee2fa_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>输入层为400个输入单元，排除一个恒为1的偏置单元。第二层拥有25个神经元，输出层为10个神经元，用于分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"></span><br><span class="line">THETA_FILE_PATH = <span class="string">r"ex3weights.mat"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_theta</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    data = io.loadmat(file_path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">"Theta1"</span>],data[<span class="string">"Theta2"</span>]</span><br><span class="line"></span><br><span class="line">theta_1,theta_2=get_theta(THETA_FILE_PATH)</span><br><span class="line">print(theta_1.shape)</span><br><span class="line">print(theta_2.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(25, 401)</span><br><span class="line">(10, 26)</span><br></pre></td></tr></table></figure>

<p><strong>前向传播算法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"></span><br><span class="line">THETA_FILE_PATH = <span class="string">r"ex3weights.mat"</span></span><br><span class="line">FILE_PATH = <span class="string">r"ex3data1.mat"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_theta</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    data = io.loadmat(file_path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">"Theta1"</span>],data[<span class="string">"Theta2"</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    data = io.loadmat(file_path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">"X"</span>], data[<span class="string">"y"</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X,theta1,theta2)</span>:</span></span><br><span class="line">    z = sigmoid(X @ theta1.T)</span><br><span class="line">    z = np.column_stack((np.ones((z.shape[<span class="number">0</span>], <span class="number">1</span>)), z))</span><br><span class="line">    final = sigmoid(z @ theta2.T)</span><br><span class="line">    <span class="keyword">return</span> final</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    theta_1,theta_2=get_theta(THETA_FILE_PATH)</span><br><span class="line">    X,y = get_data(FILE_PATH)</span><br><span class="line">    X = np.column_stack((np.ones((X.shape[<span class="number">0</span>],<span class="number">1</span>)),X))</span><br><span class="line"></span><br><span class="line">    final_theta = forward_propagation(X,theta_1,theta_2)</span><br><span class="line">    y_prefi=np.argmax(final_theta,axis=<span class="number">1</span>)+<span class="number">1</span></span><br><span class="line">    y=y.flatten()</span><br><span class="line">    acc=np.mean(y_prefi==y)</span><br><span class="line">    print(<span class="string">"训练集上模型准确度为: %.2f"</span>%(acc*<span class="number">100</span>) + <span class="string">"%"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">训练集上模型准确度为: 97.52%</span><br></pre></td></tr></table></figure>

<br>

<h3 id="Programming-Exercise-4-Neural-Networks-Learning"><a href="#Programming-Exercise-4-Neural-Networks-Learning" class="headerlink" title="Programming Exercise 4: Neural Networks Learning"></a>Programming Exercise 4: Neural Networks Learning</h3><p>在Ex3的基础上，我们要进行反向传播算法的实现，下图为这次神经网络的结构以及参数的维度，值得一提的是，我们把神经网络的<strong>偏置全看做1</strong>.</p>
<p><img src="https://images.gitee.com/uploads/images/2021/1011/151247_8b6a0f37_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>实现代价函数与标签值的转化</strong></p>
<p>因为标签值还是10和1-9，我们要把标签值翻译成神经网络输出层中的那个10维向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DATA_SET_PATH = <span class="string">r"dataset/ex4data1.mat"</span></span><br><span class="line">WEIGHT_PATH = <span class="string">r"dataset/ex4weights.mat"</span></span><br><span class="line"></span><br><span class="line">data_mat_dict = io.loadmat(DATA_SET_PATH)</span><br><span class="line">weight_mat_dict = io.loadmat(WEIGHT_PATH)</span><br><span class="line"></span><br><span class="line">X = data_mat_dict[<span class="string">"X"</span>]  <span class="comment"># 这是数据集中的特征向量,5000*400,每一行代表一个数字,数字图片是20*20的</span></span><br><span class="line">y = data_mat_dict[<span class="string">"y"</span>]  <span class="comment"># 这是数据集中的标签,5000*1,1-10对应1-0这十个数字</span></span><br><span class="line"></span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones((<span class="number">1</span>, X.shape[<span class="number">0</span>])), axis=<span class="number">1</span>, )</span><br><span class="line"></span><br><span class="line">theta_1 = weight_mat_dict[<span class="string">"Theta1"</span>]  <span class="comment"># 25*401</span></span><br><span class="line">theta_2 = weight_mat_dict[<span class="string">"Theta2"</span>]  <span class="comment"># 10*26</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"X: "</span>, X.shape)</span><br><span class="line">print(<span class="string">"y: "</span>, y.shape)</span><br><span class="line">print(<span class="string">"Theta1: "</span>, theta_1.shape)</span><br><span class="line">print(<span class="string">"Theta2: "</span>, theta_2.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">设神经网络结构为</span></span><br><span class="line"><span class="string">Input:5000*401</span></span><br><span class="line"><span class="string">H_1: W^[1]:35*401,b^[1]:5000*1 = 1</span></span><br><span class="line"><span class="string">Output:W^[2]:10*35,b^[2]:5000*1 = 1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_z</span><span class="params">(theta_matrix, feature_x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(feature_x, theta_matrix.T)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function_gradient</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> activate_function(z)(<span class="number">1</span> - activate_function(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(predict_y, label_y, regularization=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    print(<span class="string">"predict_y: "</span>, predict_y.shape)</span><br><span class="line">    print(<span class="string">"label_y: "</span>, label_y.shape)</span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    part_a = label_y * np.log(predict_y)</span><br><span class="line">    part_b = (<span class="number">1</span> - label_y) * np.log((<span class="number">1</span> - predict_y))</span><br><span class="line">    sum_part = np.sum(part_a + part_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (sum_part / (-m)) + regularization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularize</span><span class="params">(m, theta_matrix_tup, my_lambda=<span class="number">1</span>)</span>:</span></span><br><span class="line">    n = len(theta_matrix_tup)  <span class="comment"># 矩阵个数</span></span><br><span class="line">    sum_result = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sub_i <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">        <span class="comment"># print(theta_matrix[sub_i].shape)</span></span><br><span class="line">        sum_result += np.sum(np.dot(theta_matrix_tup[sub_i], theta_matrix_tup[sub_i].T))</span><br><span class="line">    reg_result = my_lambda / (<span class="number">2</span> * m) * sum_result</span><br><span class="line">    print(<span class="string">"reg: "</span>, reg_result)</span><br><span class="line">    <span class="keyword">return</span> reg_result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recode_label</span><span class="params">(label_y)</span>:</span></span><br><span class="line">    <span class="comment"># 把标签0-9重编码:10 = [0,0,0,0,0,0,0,0,0,1](1*10)</span></span><br><span class="line">    matrix_y = np.zeros((label_y.shape[<span class="number">0</span>], <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> sub_i <span class="keyword">in</span> range(<span class="number">0</span>, label_y.shape[<span class="number">0</span>]):</span><br><span class="line">        matrix_y[sub_i, label_y[sub_i] - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># print(label_y[sub_i],matrix_y[sub_i,:])</span></span><br><span class="line">    <span class="keyword">return</span> matrix_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(theta_1, theta_2, feature_x)</span>:</span></span><br><span class="line">    z_1 = calc_z(theta_1, feature_x)</span><br><span class="line">    a_1 = activate_function(z_1)</span><br><span class="line">    a_1 = np.insert(a_1, <span class="number">0</span>, np.ones((<span class="number">1</span>, a_1.shape[<span class="number">0</span>])), axis=<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"a_1: "</span>, a_1.shape)</span><br><span class="line">    z_2 = calc_z(theta_2, a_1)</span><br><span class="line">    a_2 = activate_function(z_2)</span><br><span class="line">    print(<span class="string">"a_2: "</span>, a_2.shape)</span><br><span class="line">    <span class="keyword">return</span> a_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    y_predict = forward_propagation(theta_1, theta_2, feature_x=X)</span><br><span class="line">    y_true = recode_label(label_y=y)</span><br><span class="line">    reg = regularize(m=y_true.shape[<span class="number">0</span>], my_lambda=<span class="number">1</span>, theta_matrix_tup=(theta_1, theta_2))</span><br><span class="line">    print(cost_function(predict_y=y_predict, label_y=y_true, ))</span><br><span class="line">    print(cost_function(predict_y=y_predict, label_y=y_true, regularization=reg))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<p>然后对答案：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0.2876291651613189(不带正则项：0.287629)</span><br><span class="line">0.3527834696960166(带正则项：0.383700)</span><br></pre></td></tr></table></figure>

<p><strong>实现sigmoid的梯度</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function_gradient</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> activate_function(z)*(<span class="number">1</span> - activate_function(z))</span><br></pre></td></tr></table></figure>

<p>代数对答案：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z&#x3D;0,梯度&#x3D;0.25</span><br></pre></td></tr></table></figure>

<p><strong>随机初始化参数矩阵Θ</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init_theta</span><span class="params">(m_shape, init_epsilon=<span class="number">0.12</span>)</span>:</span></span><br><span class="line">    result = (<span class="number">2</span> * init_epsilon) * np.random.random(m_shape) - init_epsilon</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<p><strong>完整代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">DATA_SET_PATH = <span class="string">r"dataset/ex4data1.mat"</span></span><br><span class="line">WEIGHT_PATH = <span class="string">r"dataset/ex4weights.mat"</span></span><br><span class="line"></span><br><span class="line">data_mat_dict = io.loadmat(DATA_SET_PATH)</span><br><span class="line">weight_mat_dict = io.loadmat(WEIGHT_PATH)</span><br><span class="line"></span><br><span class="line">X = data_mat_dict[<span class="string">"X"</span>]  <span class="comment"># 这是数据集中的特征向量,5000*400,每一行代表一个数字,数字图片是20*20的</span></span><br><span class="line">y = data_mat_dict[<span class="string">"y"</span>]  <span class="comment"># 这是数据集中的标签,5000*1,1-10对应1-0这十个数字</span></span><br><span class="line"></span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones((<span class="number">1</span>, X.shape[<span class="number">0</span>])), axis=<span class="number">1</span>, )</span><br><span class="line"></span><br><span class="line">theta_1 = weight_mat_dict[<span class="string">"Theta1"</span>]  <span class="comment"># 25*401</span></span><br><span class="line">theta_2 = weight_mat_dict[<span class="string">"Theta2"</span>]  <span class="comment"># 10*26</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"X: "</span>, X.shape)</span><br><span class="line">print(<span class="string">"y: "</span>, y.shape)</span><br><span class="line">print(<span class="string">"Theta1: "</span>, theta_1.shape)</span><br><span class="line">print(<span class="string">"Theta2: "</span>, theta_2.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">设神经网络结构为</span></span><br><span class="line"><span class="string">Input:5000*401</span></span><br><span class="line"><span class="string">H_1: W^[1]:35*401,b^[1]:5000*1 = 1</span></span><br><span class="line"><span class="string">Output:W^[2]:10*35,b^[2]:5000*1 = 1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_z</span><span class="params">(theta_matrix, feature_x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(feature_x, theta_matrix.T)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate_function_gradient</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> activate_function(z) * (<span class="number">1</span> - activate_function(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(predict_y, label_y, regularization=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算代价函数</span></span><br><span class="line"><span class="string">    :param predict_y:</span></span><br><span class="line"><span class="string">    :param label_y:</span></span><br><span class="line"><span class="string">    :param regularization:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># print("predict_y: ", predict_y.shape)</span></span><br><span class="line">    <span class="comment"># print("label_y: ", label_y.shape)</span></span><br><span class="line">    m = y.shape[<span class="number">0</span>]</span><br><span class="line">    part_a = label_y * np.log(predict_y)</span><br><span class="line">    part_b = (<span class="number">1</span> - label_y) * np.log((<span class="number">1</span> - predict_y))</span><br><span class="line">    sum_part = np.sum(part_a + part_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (sum_part / (-m)) + regularization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularize</span><span class="params">(m, theta_matrix_tup, my_lambda=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    正则化</span></span><br><span class="line"><span class="string">    :param m:</span></span><br><span class="line"><span class="string">    :param theta_matrix_tup:</span></span><br><span class="line"><span class="string">    :param my_lambda:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(theta_matrix_tup)  <span class="comment"># 矩阵个数</span></span><br><span class="line">    sum_result = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sub_i <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">        <span class="comment"># print(theta_matrix[sub_i].shape)</span></span><br><span class="line">        sum_result += np.sum(np.dot(theta_matrix_tup[sub_i], theta_matrix_tup[sub_i].T))</span><br><span class="line">    reg_result = my_lambda / (<span class="number">2</span> * m) * sum_result</span><br><span class="line">    print(<span class="string">"reg: "</span>, reg_result)</span><br><span class="line">    <span class="keyword">return</span> reg_result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recode_label</span><span class="params">(label_y)</span>:</span></span><br><span class="line">    <span class="comment"># 把标签0-9重编码:10 = [0,0,0,0,0,0,0,0,0,1](1*10)</span></span><br><span class="line">    matrix_y = np.zeros((label_y.shape[<span class="number">0</span>], <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> sub_i <span class="keyword">in</span> range(<span class="number">0</span>, label_y.shape[<span class="number">0</span>]):</span><br><span class="line">        matrix_y[sub_i, label_y[sub_i] - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># print(label_y[sub_i],matrix_y[sub_i,:])</span></span><br><span class="line">    <span class="keyword">return</span> matrix_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(theta_1, theta_2, feature_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向传播</span></span><br><span class="line"><span class="string">    :param theta_1:</span></span><br><span class="line"><span class="string">    :param theta_2:</span></span><br><span class="line"><span class="string">    :param feature_x:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    z_1 = calc_z(theta_1, feature_x)</span><br><span class="line">    a_1 = activate_function(z_1)</span><br><span class="line">    a_1 = np.insert(a_1, <span class="number">0</span>, np.ones((<span class="number">1</span>, a_1.shape[<span class="number">0</span>])), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># print("a_1: ", a_1.shape)</span></span><br><span class="line">    z_2 = calc_z(theta_2, a_1)</span><br><span class="line">    a_2 = activate_function(z_2)</span><br><span class="line">    <span class="comment"># print("a_2: ", a_2.shape)</span></span><br><span class="line">    <span class="keyword">return</span> a_2, a_1, z_1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init_theta</span><span class="params">(m_shape, init_epsilon=<span class="number">0.12</span>)</span>:</span></span><br><span class="line">    result = (<span class="number">2</span> * init_epsilon) * np.random.random(m_shape) - init_epsilon</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">back_propagation</span><span class="params">(y_true, a_2, a_1, w_2, z_1, a_0)</span>:</span></span><br><span class="line">    m = y_true.shape[<span class="number">0</span>]</span><br><span class="line">    d_z_2 = a_2 - y_true</span><br><span class="line">    <span class="comment"># print("d_z_2: ", d_z_2.shape)</span></span><br><span class="line">    <span class="comment"># print("A_1.T: ", a_1.T.shape)</span></span><br><span class="line">    d_w_2 = np.dot(a_1.T, d_z_2) / m</span><br><span class="line">    <span class="comment"># print("w_2[ :,1:].T: ",w_2[:,1:].T.shape)</span></span><br><span class="line">    <span class="comment"># print("d_z_2.T: ",d_z_2.T.shape)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print("z_1: ",z_1.shape)</span></span><br><span class="line">    d_z_1 = np.dot(w_2[:, <span class="number">1</span>:].T, d_z_2.T) * activate_function_gradient(z_1).T</span><br><span class="line">    <span class="comment"># print("d_z_1: ",d_z_1.shape)</span></span><br><span class="line">    <span class="comment"># print("a_0: ",a_0.shape)</span></span><br><span class="line"></span><br><span class="line">    d_w_1 = np.dot(d_z_1, a_0)</span><br><span class="line">    <span class="keyword">return</span> d_w_1, d_w_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(feature_matrix, y_true, theta_1, theta_2, alpha=<span class="number">0.001</span>, it=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(it):</span><br><span class="line">        a_2, a_1, z_1 = forward_propagation(theta_1, theta_2, feature_matrix)</span><br><span class="line">        d_w_1, d_w_2 = back_propagation(y_true, a_2, a_1, theta_2, z_1, feature_matrix)</span><br><span class="line">        <span class="comment"># print("theta_1: ",theta_1.shape)</span></span><br><span class="line">        <span class="comment"># print("d_w_1: ", d_w_1.shape)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># print("theta_2: ",theta_2.shape)</span></span><br><span class="line">        <span class="comment"># print("d_w_2.T: ", d_w_2.T.shape)</span></span><br><span class="line">        theta_1 = theta_1 - alpha * d_w_1</span><br><span class="line">        theta_2 = theta_2 - alpha * d_w_2.T</span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"episode:%s,Cost:%s"</span> % (i, cost_function(forward_propagation(theta_1, theta_2, feature_matrix)[<span class="number">0</span>], y_true)))</span><br><span class="line">    <span class="keyword">return</span> theta_1, theta_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(feature_x, label_y)</span>:</span></span><br><span class="line">    my_theta_1 = random_init_theta((<span class="number">25</span>, <span class="number">401</span>))</span><br><span class="line">    my_theta_2 = random_init_theta((<span class="number">10</span>, <span class="number">26</span>))</span><br><span class="line">    my_theta_1, my_theta_2 = gradient_descent(feature_x, label_y, my_theta_1, my_theta_2, alpha=<span class="number">0.08</span>, it=<span class="number">1150</span>)</span><br><span class="line">    print(<span class="string">"Cost: "</span>, cost_function(forward_propagation(my_theta_1, my_theta_2, feature_x)[<span class="number">0</span>], label_y))</span><br><span class="line">    <span class="keyword">return</span> my_theta_1, my_theta_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(X, y, my_theta_1, my_theta_2, theta_1, theta_2)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000</span>):</span><br><span class="line">        a_2, a_1, z_1 = forward_propagation(theta_1, theta_2, X[i, :].reshape((X[i].shape[<span class="number">0</span>], <span class="number">1</span>)).T)</span><br><span class="line">        print(<span class="string">"predict_1: "</span>,print_digit(np.argmax(a_2, axis=<span class="number">1</span>)[<span class="number">0</span>]) )</span><br><span class="line">        my_a_2, my_a_1, my_z_1 = forward_propagation(my_theta_1, my_theta_2, X[i, :].reshape((X[i].shape[<span class="number">0</span>], <span class="number">1</span>)).T)</span><br><span class="line">        print(<span class="string">"predict_2:"</span>, print_digit(np.argmax(my_a_2, axis=<span class="number">1</span>)[<span class="number">0</span>]))</span><br><span class="line">        print(<span class="string">"true: "</span>, y[i]%<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_digit</span><span class="params">(num)</span>:</span></span><br><span class="line">    res = num + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> res == <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># y_predict = forward_propagation(theta_1, theta_2, feature_x=X)</span></span><br><span class="line">    y_true = recode_label(label_y=y)</span><br><span class="line">    <span class="comment"># reg = regularize(m=y_true.shape[0], my_lambda=1, theta_matrix_tup=(theta_1, theta_2))</span></span><br><span class="line">    <span class="comment"># print(cost_function(predict_y=y_predict, label_y=y_true, ))</span></span><br><span class="line">    <span class="comment"># print(cost_function(predict_y=y_predict, label_y=y_true, regularization=reg))</span></span><br><span class="line"></span><br><span class="line">    my_theta_1, my_theta_2 = train(X, y_true)</span><br><span class="line"></span><br><span class="line">    test(X, y, my_theta_1, my_theta_2, theta_1, theta_2)</span><br><span class="line">    <span class="comment"># res = forward_propagation(theta_1,theta_2,X[0,:].reshape(X[0].shape[0],1).T)[0]</span></span><br><span class="line">    <span class="comment"># print("predict_1: ", print_digit(np.argmax(res, axis=1)[0]))</span></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<br>

<h3 id="Programming-Exercise-5-Regularized-Linear-Regression-and-Bias-v-s"><a href="#Programming-Exercise-5-Regularized-Linear-Regression-and-Bias-v-s" class="headerlink" title="Programming Exercise 5: Regularized Linear Regression and Bias v.s."></a>Programming Exercise 5: Regularized Linear Regression and Bias v.s.</h3><p>Variance</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir.In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s. variance.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在前半部部分的练习中，你将实现一个带有正则化的线性回归模型，通过水位来预测大坝的排水流量。在下半部分，将去使用一些机器学习诊断法来调试和验证算法在偏差（欠拟合）和方差（过拟合）上的问题。</span><br></pre></td></tr></table></figure>

<p><strong>数据集说明</strong></p>
<p><img src="https://images.gitee.com/uploads/images/2021/1012/203043_f8f811a1_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>训练集可视化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">DATA_SET_PATH = <span class="string">r"dataset/ex5data1.mat"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    data_dict = io.loadmat(file_path)</span><br><span class="line">    training_set_x = data_dict[<span class="string">"X"</span>]</span><br><span class="line">    training_set_y = data_dict[<span class="string">"y"</span>]</span><br><span class="line">    test_set_x = data_dict[<span class="string">"Xtest"</span>]</span><br><span class="line">    test_set_y = data_dict[<span class="string">"ytest"</span>]</span><br><span class="line">    cross_validation_x = data_dict[<span class="string">"Xval"</span>]</span><br><span class="line">    cross_validation_y = data_dict[<span class="string">"yval"</span>]</span><br><span class="line">    <span class="keyword">return</span> training_set_x, training_set_y, cross_validation_x, cross_validation_y, test_set_x, test_set_y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_training_set</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    x = x.T</span><br><span class="line">    y = y.T</span><br><span class="line">    plt.scatter(x,y,marker=<span class="string">'x'</span>,c=<span class="string">"red"</span>,label=<span class="string">"Training set"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">"change in water level"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Water flowing of the dam"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    training_set_x, training_set_y, cross_validation_x, cross_validation_y, test_set_x, test_set_y = load_dataset(</span><br><span class="line">        DATA_SET_PATH)</span><br><span class="line"></span><br><span class="line">    draw_training_set(training_set_x,training_set_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p><img src="https://images.gitee.com/uploads/images/2021/1012/203650_d85b94d6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>带有正则项的代价函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost_function</span><span class="params">(theta,x,y,my_lambda = <span class="number">1</span>)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    predict_y = hypothesis_function(theta,x)</span><br><span class="line">    part_a = (np.dot((predict_y-y).T,(predict_y-y)))/(<span class="number">2</span>*m)</span><br><span class="line">    part_b = (my_lambda/(<span class="number">2</span>*m))*(np.dot(theta[<span class="number">1</span>,:].T,theta[<span class="number">1</span>,:]))</span><br><span class="line">    res = part_a+part_b</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">303.99319222</span><br><span class="line">答案：303.993</span><br></pre></td></tr></table></figure>

<p><strong>计算梯度</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_gradient</span><span class="params">(theta, x, y,lambd = <span class="number">1</span>)</span>:</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    X_0 = x[:,<span class="number">0</span>]</span><br><span class="line">    X_j = x[:,<span class="number">1</span>:]</span><br><span class="line">    theta_j = theta[<span class="number">1</span>:,<span class="number">0</span>]</span><br><span class="line">    term = np.dot(x,theta)-y</span><br><span class="line">    theta_0_grad = (<span class="number">1</span>/m)*(np.dot(X_0.T,term))</span><br><span class="line">    theta_j_grad = (<span class="number">1</span>/m)*(np.dot(X_j.T,term))+ (lambd/m)*theta_j</span><br><span class="line">    <span class="keyword">return</span> np.array([theta_0_grad.flatten(),theta_j_grad.flatten()])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果与答案:</span><br><span class="line">[[-15.30301567]</span><br><span class="line"> [598.25074417]]</span><br><span class="line">[-15.30; 598.250]</span><br></pre></td></tr></table></figure>

<p>之后训练模型，在lambda=0时进行训练，并得到相关结果,再进行可视化：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/1013/162112_2631c49f_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>参考答案：</p>
<p><img src="C:%5CUsers%5Cmicha%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20211013162136914.png" alt="image-20211013162136914"></p>
<p><strong>画出学习曲线</strong></p>
<p>我的：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/1013/185159_60ea31c6_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p>官方答案：</p>
<p><img src="https://images.gitee.com/uploads/images/2021/1013/185210_e2c58d8c_5550632.png" alt="输入图片说明" title="屏幕截图.png"></p>
<p><strong>多项式回归</strong></p>
<p>在多项式回归中，我们的假设函数形式为：<br>$$<br>h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+…+\theta_px^p<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_k_polynomial</span><span class="params">(ndarr,k = <span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param ndarr: ndarray类型，(m,1)</span></span><br><span class="line"><span class="string">    :param k: k阶</span></span><br><span class="line"><span class="string">    :return: (m,k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> my_pow <span class="keyword">in</span> range(<span class="number">2</span>,k+<span class="number">1</span>):</span><br><span class="line">        <span class="comment">#np.insert(x, 0, values=np.ones((1, m)), axis=1, )</span></span><br><span class="line">        ndarr = np.insert(ndarr,my_pow<span class="number">-1</span>,values=ndarr[:,<span class="number">0</span>]**my_pow,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ndarr</span><br></pre></td></tr></table></figure>

<p><strong>训练多项式回归</strong></p>
<p>在本小节中，我们会使用8阶的特征向量，那么此时如果我们直接在投影出的数据集上进行训练，效果是非常不好的。</p>
<p>假设x=40，我们的第8列会计算出$40^8=6.5*10^{12}$，这个量级和40比一比，是不是相差甚远？因此我们需要标准化。</p>
<p>对于每一列，我们都计算平均值和方差，然后对每个指标进行规约。当正则化系数为0时，进行模型训练，得出代价函数图像和学习曲线图像。</p>
<img src="https://images.gitee.com/uploads/images/2021/1213/123353_88e54fcd_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:50%;" />

<p>代价函数图像如上图所示，可以看出该函数极其复杂，但完全拟合了训练集中的数据，训练结果显示，此时的代价函数值为：$0.21$</p>
<p>然后，让我们看看学习曲线：</p>
<img src="https://images.gitee.com/uploads/images/2021/1213/123609_39fab514_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:50%;" />

<p>可以看出，在训练集上，我们基本不存在模型误差，但是在交叉验证集上确出现了较大的波动，两条曲线中存在一个很明显的gap，说明此时模型过拟合了。附上官方的实验结果<strong>（注：官方的画图范围和我的不同，我偷懒了只画了数据集上的那几个点，所以看起来特别的直，他的看起来更好看因为采样范围比我宽，但是实验想说明的问题是一样的）</strong>：</p>
<img src="https://images.gitee.com/uploads/images/2021/1213/123821_68026c41_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:50%;" />

<img src="https://images.gitee.com/uploads/images/2021/1213/123851_20b4d431_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:50%;" />

<p><strong>调整正则化系数减少过拟合现象</strong></p>
<p>在下面的两个实验中。我们使用正则化系数为1，100。这两个值来观察对方差问题的影响。</p>
<p><strong>λ=1</strong></p>
<img src="https://images.gitee.com/uploads/images/2021/1213/124434_10c8bff1_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:67%;" />

<p>此时，我们可以明显地看出，模型在交叉验证集上的误差在减小，同时训练集和交叉验证集的误差趋向于某一个数，此时模型效果是较好的。</p>
<p><strong>λ=100</strong></p>
<img src="https://images.gitee.com/uploads/images/2021/1213/125240_0171229b_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:67%;" />

<p>可以看出由于惩罚项过大，导致模型无法拟合数据集，出现了高偏差问题，从学习曲线中也可以看出，两个数据集上的误差都很大。</p>
<p>实验结果表明，通过更改正则化系数，确实可以影响模型的训练效果。它越大，越能解决过拟合的问题</p>
<h3 id="Programming-Exercise-6-Support-Vector-Machines"><a href="#Programming-Exercise-6-Support-Vector-Machines" class="headerlink" title="Programming Exercise 6: Support Vector Machines"></a>Programming Exercise 6: Support Vector Machines</h3><p>在本章练习中，我们将动手实现一个SVM分类器，用于垃圾邮件的分类。本章练习分为两大部分，前半段内容是在二维数据上使用带有Guassian核函数的SVM用于回忆起基本原理。后半段是创造SVM垃圾邮件分类器。由于SVM训练器实现的复杂性。本节并没有实现SVM的具体代码，而是调用了SVM训练库。</p>
<p><strong>热身</strong></p>
<p>我们的数据集如下图所示：</p>
<img src="https://images.gitee.com/uploads/images/2021/1215/083731_38de89d3_5550632.png" alt="输入图片说明" title="屏幕截图.png" style="zoom:50%;" />

<p>正样本为“+”，负样本为“O”。</p>
<p>其中，存在一条肉眼可见的线性分割界限，同时还有一个利群的正样本（0.1，4.1）</p>
<p>按照作业要求，我们需要尝试不同的C值，来观测超平面的绘制。但是，想画超平面我们需要做如下的编程工作：</p>
<p>1.可视化数据集</p>
<p>2.编写假设函数</p>
<p>3.编写代价函数</p>
<p>4.写SVM参数训练算法（不确定还用不用梯度下降）</p>
<p>5.画图</p>
<p>看着任务不多，其实光画图就要了命了。</p>

      


        <!-- 《添加版权声明 -->
        
            <!-- 《添加版权声明 -->
<!--添加版权声明https://github.com/JoeyBling/hexo-theme-yilia-plus/commit/c1215e132f6d5621c5fea83d3c4f7ccbcca074a3-->


<!-- #版权基础设定：0-关闭声明； 1-文章对应的md文件里有declare: true属性，才有版权声明； 2-所有文章均有版权声明 -->

  <div class="declare">
    <strong class="author">本文作者：</strong>
    
      甯宓
    
    <br>
    <strong class="create-time">发布时间：</strong>
    2020-04-10
    <br>
    <strong class="update-time">最后更新：</strong>
    2021-12-15
    <br>
    <strong class="article-titles">本文标题：</strong>
    <a href="https://ningmidaoren.github.io/2020/04/10/机器学习-基础版/" title="机器学习-基础版" target="_blank">机器学习-基础版</a>
    <br>
    <strong class="article-url">本文链接：</strong>
    <a href="https://ningmidaoren.github.io/2020/04/10/机器学习-基础版/" title="机器学习-基础版" target="_blank">https://ningmidaoren.github.io/2020/04/10/机器学习-基础版/</a>
    <br>
    <strong class="copyright">版权声明：</strong>
    本作品采用
    <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议">CC BY-NC-SA 4.0</a>
    许可协议进行许可。转载请注明出处！
    
      <br>
      <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"/></a>
    
  </div>


        
        <!-- 添加版权声明》 -->



      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>谢谢你请我喝咖啡<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    
                    <div class="reward-box-item">
                      <img class="reward-img" src="/assets/WechatPay.png">
                      <span class="reward-type">微信</span>
                    </div>
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>

    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">机器学习</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习//" class="article-tag-list-link color5">机器学习</a>
        		</li>
      		
		</ul>
	</div>


      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//pan.baidu.com/share/qrcode?url=https://ningmidaoren.github.io/2020/04/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E7%89%88/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2020/04/22/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          滑动窗口
        
      </div>
    </a>
  
  
    <a href="/2020/04/08/x86%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">x86汇编语言</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习-基础版"><span class="toc-number">1.</span> <span class="toc-text">机器学习-基础版</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-1-序"><span class="toc-number">1.1.</span> <span class="toc-text">Part 1 序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-本人学习机器学习的初衷"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 本人学习机器学习的初衷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-机器学习的应用场景"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 机器学习的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-什么是机器学习"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 什么是机器学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-2-机器学习的分类"><span class="toc-number">1.2.</span> <span class="toc-text">Part 2 机器学习的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-监督学习-Supervised-Learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 监督学习(Supervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-无监督学习-Unsupervised-Learning"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 无监督学习(Unsupervised Learning)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-线性回归-Linear-Regression-机器学习的第一步"><span class="toc-number">1.3.</span> <span class="toc-text">Part 3 线性回归(Linear Regression)-机器学习的第一步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-什么是线性回归"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 什么是线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-如何实现线性回归模型"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 如何实现线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-代价函数-Cost-Function"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 代价函数(Cost Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-代价函数和预测函数的关系"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 代价函数和预测函数的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-等高线图-Contour-plot"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3.2.3 等高线图(Contour plot)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-梯度下降-Gradient-decent-algorithm"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">3.2.4 梯度下降(Gradient decent algorithm)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-5-梯度下降总结"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">3.2.5 梯度下降总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-线性回归模型算法实现"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 线性回归模型算法实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-4-线性代数-优化梯度下降算法"><span class="toc-number">1.4.</span> <span class="toc-text">Part 4 线性代数-优化梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-矩阵与向量"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 矩阵与向量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-矩阵-Matrix"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 矩阵 (Matrix)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-向量-Vector"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 向量 (Vector)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-矩阵运算"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 矩阵运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-加法和标量乘法"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 加法和标量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-矩阵向量乘法"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 矩阵向量乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-矩阵间乘法"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 矩阵间乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-4-矩阵乘法性质"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">4.2.4 矩阵乘法性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-5-逆和转置"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">4.3.5 逆和转置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-5-多元线性回归"><span class="toc-number">1.5.</span> <span class="toc-text">Part 5 多元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-多元梯度下降法"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 多元梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-多元梯度下降-特征缩放"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 多元梯度下降-特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-均值归一化"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">5.2.1 均值归一化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-多元梯度下降-学习率"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 多元梯度下降-学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-特征和多项式回归"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.4 特征和多项式回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-正规方程-Normal-equation"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.5 正规方程 (Normal equation)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-1-矩阵不可逆时的正规方程方法"><span class="toc-number">1.5.5.1.</span> <span class="toc-text">5.5.1 矩阵不可逆时的正规方程方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-题外话-线性回归模型的数学原理"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.6 题外话-线性回归模型的数学原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-6-Octave编程"><span class="toc-number">1.6.</span> <span class="toc-text">Part 6 Octave编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-基本数学运算"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 基本数学运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-逻辑运算"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 逻辑运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-声明矩阵和向量"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 声明矩阵和向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-从文件中读写矩阵"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 从文件中读写矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-数据计算"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 数据计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-1-矩阵相关计算"><span class="toc-number">1.6.5.1.</span> <span class="toc-text">6.5.1 矩阵相关计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-2-常用函数"><span class="toc-number">1.6.5.2.</span> <span class="toc-text">6.5.2 常用函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-数据可视化"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 数据可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-控制语句-if-for-while"><span class="toc-number">1.6.7.</span> <span class="toc-text">6.7 控制语句-if-for-while</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-8-定义函数"><span class="toc-number">1.6.8.</span> <span class="toc-text">6.8 定义函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-9-向量化"><span class="toc-number">1.6.9.</span> <span class="toc-text">6.9 向量化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-7-Logistic-回归算法"><span class="toc-number">1.7.</span> <span class="toc-text">Part 7 Logistic 回归算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-分类问题的引出"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 分类问题的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-二元分类问题的假设函数"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 二元分类问题的假设函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-1-Sigmoid-function-Logistic-function"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">7.2.1 Sigmoid function &#x2F; Logistic function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-2-分类问题的代价函数"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">7.2.2 分类问题的代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-3-简化分类问题的代价函数"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">7.2.3 简化分类问题的代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-4-Logistic-Regression-Gradient-Descent"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">7.2.4 Logistic Regression Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-5-高级优化-知道个名称就行了"><span class="toc-number">1.7.2.5.</span> <span class="toc-text">7.2.5 高级优化-知道个名称就行了</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-多元分类问题"><span class="toc-number">1.7.3.</span> <span class="toc-text">7.3 多元分类问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-8-过拟合-Overfitting"><span class="toc-number">1.8.</span> <span class="toc-text">Part 8 过拟合 (Overfitting)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-什么是过拟合"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 什么是过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-解决过拟合的途径"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 解决过拟合的途径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-正则化-Regularization"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 正则化 (Regularization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-线性回归的正则化"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.4  线性回归的正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-Logistic-回归的正则化"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 Logistic 回归的正则化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-9-机器学习实战-房价预测"><span class="toc-number">1.9.</span> <span class="toc-text">Part 9 机器学习实战-房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-数据集来源以及数据集说明"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 数据集来源以及数据集说明</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-数据集的介绍"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">0x00 数据集的介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-数据集的类别和规模"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">0x01 数据集的类别和规模</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-实验设计"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 实验设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-概述"><span class="toc-number">1.9.2.1.</span> <span class="toc-text">0x00 概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-源代码"><span class="toc-number">1.9.2.2.</span> <span class="toc-text">0x01 源代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x02-实验结果及可视化展示"><span class="toc-number">1.9.2.3.</span> <span class="toc-text">0x02 实验结果及可视化展示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-踩到的坑"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 踩到的坑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0x00-多元梯度下降的向量化"><span class="toc-number">1.9.3.1.</span> <span class="toc-text">0x00 多元梯度下降的向量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0x01-均值归一化"><span class="toc-number">1.9.3.2.</span> <span class="toc-text">0x01 均值归一化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-10-神经网络-新的征程"><span class="toc-number">1.10.</span> <span class="toc-text">Part 10 神经网络-新的征程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-神经网络的引出"><span class="toc-number">1.10.1.</span> <span class="toc-text">10.1 神经网络的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-神经元与大脑"><span class="toc-number">1.10.2.</span> <span class="toc-text">10.2 神经元与大脑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-神经网络结构"><span class="toc-number">1.10.3.</span> <span class="toc-text">10.3 神经网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-神经网络向量化"><span class="toc-number">1.10.4.</span> <span class="toc-text">10.4 神经网络向量化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#通过例子进行深入理解"><span class="toc-number">1.10.4.0.1.</span> <span class="toc-text">通过例子进行深入理解</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#引入"><span class="toc-number">1.10.4.0.1.1.</span> <span class="toc-text">引入</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-多元分类问题"><span class="toc-number">1.10.5.</span> <span class="toc-text">10.5 多元分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-1-问题概述"><span class="toc-number">1.10.5.1.</span> <span class="toc-text">10.5.1 问题概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-2-代价函数"><span class="toc-number">1.10.5.2.</span> <span class="toc-text">10.5.2 代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-3-反向传播算法"><span class="toc-number">1.10.5.3.</span> <span class="toc-text">10.5.3 反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#算法描述"><span class="toc-number">1.10.5.3.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#深入理解反向传播算法"><span class="toc-number">1.10.5.3.2.</span> <span class="toc-text">深入理解反向传播算法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-4-梯度检验-Gradient-Checking"><span class="toc-number">1.10.5.4.</span> <span class="toc-text">10.5.4 梯度检验 (Gradient Checking)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-5-随机初始化"><span class="toc-number">1.10.5.5.</span> <span class="toc-text">10.5.5 随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-6-组合到一起"><span class="toc-number">1.10.5.6.</span> <span class="toc-text">10.5.6 组合到一起</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#直观理解梯度下降在神经网络中的作用"><span class="toc-number">1.10.5.6.1.</span> <span class="toc-text">直观理解梯度下降在神经网络中的作用</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-11-机器学习诊断法-Machine-Learning-diagnostic"><span class="toc-number">1.11.</span> <span class="toc-text">Part 11 机器学习诊断法 (Machine Learning diagnostic)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-评估假设函数"><span class="toc-number">1.11.1.</span> <span class="toc-text">11.1 评估假设函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-模型选择问题"><span class="toc-number">1.11.2.</span> <span class="toc-text">11.2 模型选择问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-诊断偏差与方差"><span class="toc-number">1.11.3.</span> <span class="toc-text">11.3 诊断偏差与方差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-1-偏差与方差"><span class="toc-number">1.11.3.1.</span> <span class="toc-text">11.3.1 偏差与方差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-2-正则化和偏差与方差的关系"><span class="toc-number">1.11.3.2.</span> <span class="toc-text">11.3.2 正则化和偏差与方差的关系</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-学习曲线-Learning-curves"><span class="toc-number">1.11.4.</span> <span class="toc-text">11.4 学习曲线 (Learning curves)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-12-机器学习系统设计"><span class="toc-number">1.12.</span> <span class="toc-text">Part 12 机器学习系统设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-情景引入"><span class="toc-number">1.12.1.</span> <span class="toc-text">12.1 情景引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-误差分析"><span class="toc-number">1.12.2.</span> <span class="toc-text">12.2 误差分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-1-偏斜类问题-Skewed-classes"><span class="toc-number">1.12.2.1.</span> <span class="toc-text">12.2.1 偏斜类问题(Skewed classes)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#查准率-（Precision）"><span class="toc-number">1.12.2.1.1.</span> <span class="toc-text">查准率 （Precision）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#召回率-（Recall）"><span class="toc-number">1.12.2.1.2.</span> <span class="toc-text">召回率 （Recall）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-2-2-查准率和召回率的权衡"><span class="toc-number">1.12.2.2.</span> <span class="toc-text">12.2.2 查准率和召回率的权衡</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-数据集的规模与机器学习算法"><span class="toc-number">1.12.3.</span> <span class="toc-text">12.3 数据集的规模与机器学习算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-13-支持向量机-Support-Vector-Machine"><span class="toc-number">1.13.</span> <span class="toc-text">Part 13 支持向量机 (Support Vector Machine)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-支持向量机的引出"><span class="toc-number">1.13.1.</span> <span class="toc-text">13.1 支持向量机的引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-对SVM的直观理解"><span class="toc-number">1.13.2.</span> <span class="toc-text">13.2 对SVM的直观理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-大间隔分类器的数学原理-初步"><span class="toc-number">1.13.3.</span> <span class="toc-text">13.3 大间隔分类器的数学原理-初步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-4-核函数"><span class="toc-number">1.13.4.</span> <span class="toc-text">13.4 核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-5-核函数与SVM"><span class="toc-number">1.13.5.</span> <span class="toc-text">13.5 核函数与SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-6-SVM的使用"><span class="toc-number">1.13.6.</span> <span class="toc-text">13.6 SVM的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-14-无监督学习-Unsupervised-Learning"><span class="toc-number">1.14.</span> <span class="toc-text">Part 14 无监督学习 (Unsupervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#14-1-启航"><span class="toc-number">1.14.1.</span> <span class="toc-text">14.1 启航</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-2-K-means算法"><span class="toc-number">1.14.2.</span> <span class="toc-text">14.2 K-means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-1-算法流程概述"><span class="toc-number">1.14.2.1.</span> <span class="toc-text">14.2.1 算法流程概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-2-K-means-的代价函数（优化目标）"><span class="toc-number">1.14.2.2.</span> <span class="toc-text">14.2.2 K-means 的代价函数（优化目标）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-3-簇的随机初始化"><span class="toc-number">1.14.2.3.</span> <span class="toc-text">14.2.3 簇的随机初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-2-4-分簇的数量"><span class="toc-number">1.14.2.4.</span> <span class="toc-text">14.2.4 分簇的数量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-3-第二类无监督学习-降维-Dimensionality-Reduction"><span class="toc-number">1.14.3.</span> <span class="toc-text">14.3 第二类无监督学习-降维(Dimensionality Reduction)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-3-1-降维的应用-数据可视化"><span class="toc-number">1.14.3.1.</span> <span class="toc-text">14.3.1 降维的应用-数据可视化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-3-2-PCA-Principal-Component-Analysis"><span class="toc-number">1.14.3.2.</span> <span class="toc-text">14.3.2 PCA (Principal Component Analysis)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#PCA的概述"><span class="toc-number">1.14.3.2.1.</span> <span class="toc-text">PCA的概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#如何选择主要成分数量"><span class="toc-number">1.14.3.2.2.</span> <span class="toc-text">如何选择主要成分数量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#压缩重现-（Reconstruction）"><span class="toc-number">1.14.3.2.3.</span> <span class="toc-text">压缩重现 （Reconstruction）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#PCA应用时的小建议"><span class="toc-number">1.14.3.2.4.</span> <span class="toc-text">PCA应用时的小建议</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-4-异常检测-Anomaly-Detection"><span class="toc-number">1.14.4.</span> <span class="toc-text">14.4 异常检测 (Anomaly Detection )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-1-问题背景"><span class="toc-number">1.14.4.1.</span> <span class="toc-text">14.4.1 问题背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-2-高斯分布（Gaussian-distribution）"><span class="toc-number">1.14.4.2.</span> <span class="toc-text">14.4.2 高斯分布（Gaussian distribution）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#参数估计"><span class="toc-number">1.14.4.2.1.</span> <span class="toc-text">参数估计</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-3-异常检测算法"><span class="toc-number">1.14.4.3.</span> <span class="toc-text">14.4.3 异常检测算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-4-使用异常检测解决实际问题"><span class="toc-number">1.14.4.4.</span> <span class="toc-text">14.4.4 使用异常检测解决实际问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-5-异常检测与监督学习的比较"><span class="toc-number">1.14.4.5.</span> <span class="toc-text">14.4.5 异常检测与监督学习的比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-6-异常检测的特征选择"><span class="toc-number">1.14.4.6.</span> <span class="toc-text">14.4.6 异常检测的特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-7-多元高斯分布"><span class="toc-number">1.14.4.7.</span> <span class="toc-text">14.4.7 多元高斯分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-4-8-使用多元高斯分布的异常检测"><span class="toc-number">1.14.4.8.</span> <span class="toc-text">14.4.8 使用多元高斯分布的异常检测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-15-推荐系统-Recommender-System"><span class="toc-number">1.15.</span> <span class="toc-text">Part 15  推荐系统 (Recommender System)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#15-1-问题场景"><span class="toc-number">1.15.1.</span> <span class="toc-text">15 .1 问题场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-2-基于内容的推荐算法"><span class="toc-number">1.15.2.</span> <span class="toc-text">15.2 基于内容的推荐算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-3-协同过滤-Collaborative-Filtering"><span class="toc-number">1.15.3.</span> <span class="toc-text">15.3 协同过滤 (Collaborative Filtering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-4-组合到一起"><span class="toc-number">1.15.4.</span> <span class="toc-text">15.4 组合到一起</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-5-低秩矩阵分解-Low-rank-matrix-factorization"><span class="toc-number">1.15.5.</span> <span class="toc-text">15.5 低秩矩阵分解 (Low rank matrix factorization)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-16-学习大规模数据集"><span class="toc-number">1.16.</span> <span class="toc-text">Part 16 学习大规模数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#16-1-随机梯度下降-Stochastic-gradient-decent"><span class="toc-number">1.16.1.</span> <span class="toc-text">16.1 随机梯度下降(Stochastic gradient decent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-2-Mini-Batch-gradient-decent"><span class="toc-number">1.16.2.</span> <span class="toc-text">16.2 Mini-Batch gradient decent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-3-在线学习（Online-Learning）"><span class="toc-number">1.16.3.</span> <span class="toc-text">16.3 在线学习（Online Learning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-4-减少映射-（Map-reduce）与数据并行（Data-parallel）"><span class="toc-number">1.16.4.</span> <span class="toc-text">16.4 减少映射 （Map-reduce）与数据并行（Data parallel）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-17-机器学习实例：Photo-OCR"><span class="toc-number">1.17.</span> <span class="toc-text">Part 17 机器学习实例：Photo OCR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#17-1-Machine-Learning-Pipeline"><span class="toc-number">1.17.1.</span> <span class="toc-text">17.1 Machine Learning Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#0x00-滑动窗口"><span class="toc-number">1.17.1.0.1.</span> <span class="toc-text">0x00 滑动窗口</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-2-数据获取和人工数据合成"><span class="toc-number">1.17.2.</span> <span class="toc-text">17.2 数据获取和人工数据合成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-3-上限分析（Ceiling-Analysis）"><span class="toc-number">1.17.3.</span> <span class="toc-text">17.3 上限分析（Ceiling Analysis）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-18-总结与感谢"><span class="toc-number">1.18.</span> <span class="toc-text">Part 18 总结与感谢</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Appendix-作业及解析"><span class="toc-number">1.19.</span> <span class="toc-text">Appendix 作业及解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-1-Linear-Regression"><span class="toc-number">1.19.1.</span> <span class="toc-text">Programming Exercise 1 Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-2-Logistics-Regression"><span class="toc-number">1.19.2.</span> <span class="toc-text">Programming Exercise 2 Logistics Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-带有的正则化逻辑回归"><span class="toc-number">1.19.2.1.</span> <span class="toc-text">2.2 带有的正则化逻辑回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-3-Multi-class-Classification-and-Neural-Networks"><span class="toc-number">1.19.3.</span> <span class="toc-text">Programming Exercise 3 Multi-class Classification and Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-4-Neural-Networks-Learning"><span class="toc-number">1.19.4.</span> <span class="toc-text">Programming Exercise 4: Neural Networks Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-5-Regularized-Linear-Regression-and-Bias-v-s"><span class="toc-number">1.19.5.</span> <span class="toc-text">Programming Exercise 5: Regularized Linear Regression and Bias v.s.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Exercise-6-Support-Vector-Machines"><span class="toc-number">1.19.6.</span> <span class="toc-text">Programming Exercise 6: Support Vector Machines</span></a></li></ol></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  

  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">

    	<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
      <script>
        var now = new Date();
        function createtime() {
          var grt= new Date("11/23/2019 20:00:00");//此处修改你的建站时间或者网站上线时间
          now.setTime(now.getTime()+250);
          days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
          hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
          if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
          mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
          seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
          snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
          document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
          document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        }
        setInterval("createtime()",250);
      </script>

    		&copy; 2026 甯宓
    	</div>
      	<div class="footer-right">

      	</div>
    </div>
  </div>
  <span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</footer>
<!--页面点击小红心-->
<script type="text/javascript" src="/love.js"></script>

</span>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&lt; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &gt;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 50%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
        
      
      <li style="width: 50%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">一只来自软工的小渣渣</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">杂项</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">动态规划</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">贪心</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">数组</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">汇编语言</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">编程技巧</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据挖掘</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">贪心算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">竞赛</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">UE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">游戏开发</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">前端</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">数据结构与算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">深度优先</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">广度优先</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">回溯</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">大数据</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">排序</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据结构</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">队列</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">顺序表</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">链表</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">机器学习</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数值分析</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">线代</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">计算机基础</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">深度学习</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">大数运算</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Android</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">操作系统</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">计算机网络</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧，只做了一点微小的工作，谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>